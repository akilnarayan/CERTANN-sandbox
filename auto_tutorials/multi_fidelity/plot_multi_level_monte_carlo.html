<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Multi-level Monte Carlo &mdash; PyApprox 1.0.3 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/plot_directive.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script>window.MathJax = {"tex": {"macros": {"V": ["{\\boldsymbol{#1}}", 1], "mean": ["{\\mathbb{E}\\left[#1\\right]}", 1], "var": ["{\\mathbb{V}\\left[#1\\right]}", 1], "rv": "{z}", "rvset": "{\\mathcal{Z}}", "reals": "\\mathbb{R}", "pdf": "\\rho", "rvdom": "\\Gamma", "coloneqq": "\\colon=", "norm": ["{\\lVert #1 \\rVert}", 1], "argmax": ["\\operatorname{argmax}"], "argmin": ["\\operatorname{argmin}"], "covar": ["\\mathbb{C}\\text{ov}\\left[#1,#2\\right]", 2], "corr": ["\\mathbb{C}\\text{or}\\left[#1,#2\\right]", 2], "ai": "\\alpha", "bi": "\\beta", "dx": ["\\;\\text{d}#1", 1], "mat": ["{\\boldsymbol{\\mathrm{#1}}}", 1]}}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Multi-fidelity Monte Carlo" href="plot_multi_fidelity_monte_carlo.html" />
    <link rel="prev" title="Approximate Control Variate Allocation Matrices" href="plot_allocation_matrices.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            PyApprox
              <img src="../../_static/pyapprox-logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">Software Tutorial</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Theoretical Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#model-analysis">Model Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#experimental-design">Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#surrogates">Surrogates</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#multi-fidelity-methods">Multi-Fidelity Methods</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_monte_carlo.html">Monte Carlo Quadrature</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_multioutput_monte_carlo.html">Monte Carlo Quadrature: Beyond Mean Estimation</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_control_variate_monte_carlo.html">Two Model Control Variate Monte Carlo</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_approximate_control_variates.html">Two model Approximate Control Variate Monte Carlo</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_many_model_acv.html">Approximate Control Variate Monte Carlo</a></li>
<li class="toctree-l3"><a class="reference internal" href="acv_covariances.html">Delta-Based Covariance Formulas For Approximate Control Variates</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_allocation_matrices.html">Approximate Control Variate Allocation Matrices</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Multi-level Monte Carlo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#two-model-mlmc">Two Model MLMC</a></li>
<li class="toctree-l4"><a class="reference internal" href="#many-model-mlmc">Many Model MLMC</a></li>
<li class="toctree-l4"><a class="reference internal" href="#optimal-sample-allocation">Optimal Sample Allocation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-index-monte-carlo">Multi-index Monte Carlo</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-relationship-between-mlmc-and-acv">The Relationship between MLMC and ACV</a></li>
<li class="toctree-l4"><a class="reference internal" href="#video">Video</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_multi_fidelity_monte_carlo.html">Multi-fidelity Monte Carlo</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_pacv.html">Parametrically Defined Approximate Control Variates</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_multioutput_acv.html">Multioutput Approximate Control Variates</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_pilot_studies.html">Pilot Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_ensemble_selection.html">Model Ensemble Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_multilevel_blue.html">Multilevel Best Linear Unbiased estimators (MLBLUE)</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_multiindex_collocation.html">Multi-level and Multi-index Collocation</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_multifidelity_gp.html">Multifidelity Gaussian processes</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_gaussian_mfnets.html">MFNets: Multi-fidelity networks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmarks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Reference Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_reference_guide.html">User Reference Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PyApprox</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Theoretical Tutorials</a></li>
      <li class="breadcrumb-item active">Multi-level Monte Carlo</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/auto_tutorials/multi_fidelity/plot_multi_level_monte_carlo.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-tutorials-multi-fidelity-plot-multi-level-monte-carlo-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="multi-level-monte-carlo">
<span id="sphx-glr-auto-tutorials-multi-fidelity-plot-multi-level-monte-carlo-py"></span><h1>Multi-level Monte Carlo<a class="headerlink" href="#multi-level-monte-carlo" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>This tutorial builds upon <a class="reference internal" href="plot_approximate_control_variates.html#sphx-glr-auto-tutorials-multi-fidelity-plot-approximate-control-variates-py"><span class="std std-ref">Two model Approximate Control Variate Monte Carlo</span></a> and describes how the pioneering work of Multi-level Monte Carlo (MLMC) <a class="reference internal" href="#cgstcvs2011" id="id1"><span>[CGSTCVS2011]</span></a>, <a class="reference internal" href="#gor2008" id="id2"><span>[GOR2008]</span></a> can be used to estimate the mean of a high-fidelity model using multiple low fidelity models. MLMC is actually a special type of ACV estimator, but it was not originally derived this way. Consequently, this tutorial begins by presenting the typical formulation of MLMC and then concludes by discussing relationships with ACV.</p>
<section id="two-model-mlmc">
<h2>Two Model MLMC<a class="headerlink" href="#two-model-mlmc" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The two model MLMC estimator is based upon the observations that we can estimate</p>
<div class="math notranslate nohighlight">
\[\mean{f_0}=\mean{f_1}+\mean{f_0-f_1}\]</div>
<p>using the following unbiased estimator</p>
<div class="math notranslate nohighlight">
\[Q_{0}^\mathrm{ML}(\rvset_0,\rvset_1)=N_1^{-1}\sum_{i=1}^{N_1} f_{1}^{(i)}(\rvset_1)+N_0^{-1}\sum_{i=1}^{N_0} \left(f_{0}^{(i)}(\rvset_0)-f_{1}^{(i)}(\rvset_0)\right),\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{\alpha}^{(i)}(\rvset_\kappa)\)</span> denotes an evaluation of <span class="math notranslate nohighlight">\(f_\alpha\)</span> using a sample from the set <span class="math notranslate nohighlight">\(\rvset_\kappa\)</span>. To evaluate this estimator we must evaluate the low fidelity model at a set of samples  <span class="math notranslate nohighlight">\(\rvset_1\)</span> and then evaluate the difference between the two models at another independet set <span class="math notranslate nohighlight">\(\rvset_0\)</span>.</p>
<p>To simplify notation let</p>
<div class="math notranslate nohighlight">
\[Y_{\alpha}(\rvset_{\alpha})=-N_{\alpha}^{-1}\sum_{i=1}^{N_{\alpha}} \left(f_{\alpha+1}^{(i)}(\rvset_{\alpha})-f_{\alpha}^{(i)}(\rvset_{\alpha})\right)\]</div>
<p>so we can write</p>
<div class="math notranslate nohighlight">
\[Q_{0}^\mathrm{ML}(\rvset_0,\rvset_1)=Y_{0}(\rvset_0)+Y_{1}(\rvset_1)\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{2}=0\)</span>.</p>
<p>Now it is easy to see that the variance of this estimator is given by</p>
<div class="math notranslate nohighlight">
\[\var{Q_{0}^\mathrm{ML}(\rvset_0, \rvset_1)}=\var{Y_{0}(\rvset_0)+Y_{1}(\rvset_1)}=N_0^{-1}\var{Y_{0}(\rvset_0)}+N_1^{-1}\var{Y_{1}(\rvset_1)}\]</div>
<p>where <span class="math notranslate nohighlight">\(N_\alpha = |\rvset_\alpha|\)</span> and we used the fact</p>
<p><span class="math notranslate nohighlight">\(\rvset_0\bigcap\rvset_1=\emptyset\)</span> so <span class="math notranslate nohighlight">\(\covar{Y_{0}(\rvset_0)}{Y_{0}(\rvset_1)}=0\)</span></p>
<p>From the previous equation we can see that MLMC works well if the variance of the difference between models is smaller than the variance of either model. Although the variance of the low-fidelity model is likely high, we can set <span class="math notranslate nohighlight">\(N_0\)</span> to be large because the cost of evaluating the low-fidelity model is low. If the variance of the discrepancy is smaller we only need a smaller number of samples so that the two sources of error (the two terms on the RHS of the previous equation) are balanced.</p>
<p>Note above and below we assume that <span class="math notranslate nohighlight">\(f_0\)</span> is the high-fidelity model, but typical multi-level literature assumes <span class="math notranslate nohighlight">\(f_M\)</span> is the high-fidelity model. We adopt the reverse ordering to be consistent with control variate estimator literature.</p>
</section>
<section id="many-model-mlmc">
<h2>Many Model MLMC<a class="headerlink" href="#many-model-mlmc" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>MLMC can easily be extended to estimator based on <span class="math notranslate nohighlight">\(M+1\)</span> models. Letting <span class="math notranslate nohighlight">\(f_0,\ldots,f_M\)</span> be an ensemble of <span class="math notranslate nohighlight">\(M+1\)</span> models ordered by decreasing fidelity and cost (note typically MLMC literature reverses this order),  we simply introduce estimates of the differences between the remaining models. That is</p>
<div class="math notranslate nohighlight" id="equation-eq-ml-estimator">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-ml-estimator" title="Permalink to this equation">ÔÉÅ</a></span>\[Q_{0}^\mathrm{ML}(\rvset) = \sum_{\alpha=0}^M Y_{\alpha}(\rvset_\alpha), \quad f_{M+1}=0\]</div>
<p>To compute this estimator we use the following algorithm, starting with <span class="math notranslate nohighlight">\(\alpha=M\)</span></p>
<ol class="arabic simple">
<li><p>Draw  <span class="math notranslate nohighlight">\(N_\alpha\)</span> samples randomly from the PDF  <span class="math notranslate nohighlight">\(\pdf\)</span> of the random variables.</p></li>
<li><p>Estimate <span class="math notranslate nohighlight">\(f_{\alpha}\)</span> and <span class="math notranslate nohighlight">\(f_{\alpha}\)</span> at the samples <span class="math notranslate nohighlight">\(\rvset_\alpha\)</span></p></li>
<li><p>Compute the discrepancies <span class="math notranslate nohighlight">\(Y_{\alpha,\rvset_\alpha}\)</span> at each sample.</p></li>
<li><p>Decrease <span class="math notranslate nohighlight">\(\alpha\)</span> and repeat steps 1-3. until <span class="math notranslate nohighlight">\(\alpha=0\)</span>.</p></li>
<li><p>Compute the ML estimator using <a class="reference internal" href="#equation-eq-ml-estimator">(1)</a></p></li>
</ol>
<p>In the above algorithm, we evaluate only the lowest fidelity model with <span class="math notranslate nohighlight">\(\rvset_M\)</span> (this follows from the assumption that <span class="math notranslate nohighlight">\(f_{M+1}=0\)</span>) and evaluate each discrepancies between each pair of consecutive models at the sets <span class="math notranslate nohighlight">\(\rvset_\alpha\)</span>, such that <span class="math notranslate nohighlight">\(\rvset_\alpha\cap\rvset_\beta=\emptyset,\; \alpha\neq\beta\)</span> and the variance of the MLMC estimator is</p>
<div class="math notranslate nohighlight">
\[\var{Q_{0}^\mathrm{ML}(\rvset_0, \ldots, \rvset_M)} = \sum_{\alpha=0}^M N_\alpha\var{Y_{\alpha}(\rvset_\alpha)}\]</div>
</section>
<section id="optimal-sample-allocation">
<h2>Optimal Sample Allocation<a class="headerlink" href="#optimal-sample-allocation" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>When estimating the mean, the optimal allocation can be determined analytically. The following follows closely the exposition in <a class="reference internal" href="#gan2015" id="id3"><span>[GAN2015]</span></a> to derive the optimal allocation.</p>
<p>Let <span class="math notranslate nohighlight">\(C_\alpha\)</span> be the cost of evaluating the function <span class="math notranslate nohighlight">\(f_\alpha\)</span> at a single sample, then the total cost of the MLMC estimator is</p>
<div class="math notranslate nohighlight">
\[C_{\mathrm{tot}}=\sum_{\alpha=0}^M C_\alpha N_\alpha\]</div>
<p>Now recall that the variance of the estimator is</p>
<div class="math notranslate nohighlight">
\[\var{Q_0^\mathrm{ML}}=\sum_{\alpha=0}^M \var{Y_\alpha}N_\alpha,\]</div>
<p>where <span class="math notranslate nohighlight">\(Y_\alpha\)</span> is the disrepancy between two consecutive models, e.g. <span class="math notranslate nohighlight">\(f_{\alpha-1}-f_\alpha\)</span> and <span class="math notranslate nohighlight">\(N_\alpha\)</span> be the number of samples allocated to resolving the discrepancy, i.e. <span class="math notranslate nohighlight">\(N_\alpha=\lvert\hat{\rvset}_\alpha\rvert\)</span>. Then For a fixed variance <span class="math notranslate nohighlight">\(\epsilon^2\)</span> the cost of the MLMC estimator can be minimized, by solving</p>
<div class="math notranslate nohighlight">
\[\begin{split}\min_{N_0,\ldots,N_M} &amp; \sum_{\alpha=0}^M\left(N_\alpha C_\alpha\right)\\
\mathrm{subject}\; \mathrm{to} &amp;\sum_{\alpha=0}^M\left(N_\alpha^{-1}\var{Y_\alpha}\right)=\epsilon^2\end{split}\]</div>
<p>or alternatively by introducing the lagrange multiplier <span class="math notranslate nohighlight">\(\lambda^2\)</span> we can minimize</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathcal{J}(N_0,\ldots,N_M,\lambda)&amp;=\sum_{\alpha=0}^M\left(N_\alpha C_\alpha\right)+\lambda^2\left(\sum_{\alpha=0}^M\left(N_\alpha^{-1}\var{Y_\alpha}\right)-\epsilon^2\right)\\
&amp;=\sum_{\alpha=0}^M\left(N_\alpha C_\alpha+\lambda^2N_\alpha^{-1}\var{Y_\alpha}\right)-\lambda^2\epsilon^2\end{split}\]</div>
<p>To find the minimum we set the gradient of this expression to zero:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial \mathcal{J}^\mathrm{ML}}{N_\alpha}&amp;=C_\alpha-\lambda^2N_\alpha^{-2}\var{Y_\alpha}=0\\
\implies C_\alpha&amp;=\lambda^2N_\alpha^{-2}\var{Y_\alpha}\\
\implies N_\alpha&amp;=\lambda\sqrt{\var{Y_\alpha}C_\alpha^{-1}}\end{split}\]</div>
<p>The constraint is satisifed by noting</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{J}}{\lambda^2}=\sum_{\alpha=0}^M N_\alpha^{-1}\var{Y_\alpha}-\epsilon^2=0\]</div>
<p>Recalling that we can write the total variance as</p>
<div class="math notranslate nohighlight">
\[\begin{split}\var{Q_{0,\rvset}^\mathrm{ML}}&amp;=\sum_{\alpha=0}^M N_\alpha^{-1} \var{Y_\alpha}\\
&amp;=\sum_{\alpha=0}^M \lambda^{-1}\var{Y_\alpha}^{-\frac{1}{2}}C_\alpha^{\frac{1}{2}}\var{Y_\alpha}\\
&amp;=\lambda^{-1}\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}=\epsilon^2\\
\implies \lambda &amp;= \epsilon^{-2}\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}\end{split}\]</div>
<p>Then substituting <span class="math notranslate nohighlight">\(\lambda\)</span> into the following</p>
<div class="math notranslate nohighlight">
\[\begin{split}N_\alpha C_\alpha&amp;=\lambda\sqrt{\var{Y_\alpha}C_\alpha^{-1}}C_\alpha\\
&amp;=\lambda\sqrt{\var{Y_\alpha}C_\alpha}\\
&amp;=\epsilon^{-2}\left(\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}\right)\sqrt{\var{Y_\alpha}C_\alpha}\end{split}\]</div>
<p>allows us to determine the smallest total cost that generates and estimator with the desired variance.</p>
<div class="math notranslate nohighlight">
\[\begin{split}C_\mathrm{tot}&amp;=\sum_{\alpha=0}^M N_\alpha C_\alpha\\
&amp;=\sum_{\alpha=0}^M \epsilon^{-2}\left(\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}\right)\sqrt{\var{Y_\alpha}C_\alpha}\\
&amp;=\epsilon^{-2}\left(\sum_{\alpha=0}^M\sqrt{\var{Y_\alpha}C_\alpha}\right)^2\end{split}\]</div>
<p>Lets setup a problem to compute an MLMC estimate of <span class="math notranslate nohighlight">\(\mean{f_0}\)</span>
using the following ensemble of models</p>
<div class="math notranslate nohighlight">
\[f_\alpha(\rv)=\rv^{5-\alpha}, \quad \alpha=0,\ldots,4\]</div>
<p>where <span class="math notranslate nohighlight">\(z\sim\mathcal{U}[0, 1]\)</span></p>
<p>First load the necessary modules</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">pyapprox.util.visualization</span> <span class="kn">import</span> <span class="n">mathrm_labels</span>
<span class="kn">from</span> <span class="nn">pyapprox.benchmarks</span> <span class="kn">import</span> <span class="n">setup_benchmark</span>
<span class="kn">from</span> <span class="nn">pyapprox.multifidelity.factory</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_estimator</span><span class="p">,</span> <span class="n">compare_estimator_variances</span><span class="p">,</span> <span class="n">multioutput_stats</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">pyapprox.multifidelity.visualize</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">plot_estimator_variance_reductions</span><span class="p">,</span> <span class="n">plot_estimator_variances</span><span class="p">,</span>
    <span class="n">plot_estimator_sample_allocation_comparison</span><span class="p">)</span>
</pre></div>
</div>
<p>The following code computes the variance of the MLMC estimator for different target costs using the optimal sample allocation using an exact estimate of the covariance between models and an approximation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">benchmark</span> <span class="o">=</span> <span class="n">setup_benchmark</span><span class="p">(</span><span class="s2">&quot;polynomial_ensemble&quot;</span><span class="p">)</span>
<span class="n">poly_model</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">fun</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">covariance</span>
<span class="n">target_costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e1</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">,</span> <span class="mf">1e3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">10</span><span class="o">**-</span><span class="n">ii</span> <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cov</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
<span class="n">model_labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s1">&#39;$f_0$&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$f_1$&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$f_2$&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$f_3$&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$f_4$&#39;</span><span class="p">]</span>
<span class="n">stat</span> <span class="o">=</span> <span class="n">multioutput_stats</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">](</span><span class="n">benchmark</span><span class="o">.</span><span class="n">nqoi</span><span class="p">)</span>
<span class="n">stat</span><span class="o">.</span><span class="n">set_pilot_quantities</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
<span class="n">estimators</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">get_estimator</span><span class="p">(</span><span class="s2">&quot;mc&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="p">,</span> <span class="n">costs</span><span class="p">),</span>
    <span class="n">get_estimator</span><span class="p">(</span><span class="s2">&quot;mlmc&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="p">,</span> <span class="n">costs</span><span class="p">)]</span>
<span class="n">est_labels</span> <span class="o">=</span> <span class="n">mathrm_labels</span><span class="p">([</span><span class="s2">&quot;MC&quot;</span><span class="p">,</span> <span class="s2">&quot;MLMC&quot;</span><span class="p">])</span>
<span class="n">optimized_estimators</span> <span class="o">=</span> <span class="n">compare_estimator_variances</span><span class="p">(</span>
    <span class="n">target_costs</span><span class="p">,</span> <span class="n">estimators</span><span class="p">)</span>
</pre></div>
</div>
<p>The following compares the estimator variance reduction ratio of MLMC relative to MLMC for a fixed target cost</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">axs</span> <span class="o">=</span> <span class="p">[</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))[</span><span class="mi">1</span><span class="p">]]</span>
<span class="c1"># get estimators for target cost = 100</span>
<span class="n">mlmc_est_100</span> <span class="o">=</span> <span class="n">optimized_estimators</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plot_estimator_variance_reductions</span><span class="p">(</span>
    <span class="n">mlmc_est_100</span><span class="p">,</span> <span class="n">est_labels</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_multi_level_monte_carlo_001.png" srcset="../../_images/sphx_glr_plot_multi_level_monte_carlo_001.png" alt="plot multi level monte carlo" class = "sphx-glr-single-img"/><p>The following compares the estimator variance of MC with MLMC for a set of
target costs and plot the number of samples allocated to each model by MLMC</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># get MLMC estimators for all target costs</span>
<span class="n">mlmc_ests</span> <span class="o">=</span> <span class="n">optimized_estimators</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">target_costs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">target_costs</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_estimator_sample_allocation_comparison</span><span class="p">(</span>
    <span class="n">mlmc_ests</span><span class="p">,</span> <span class="n">model_labels</span><span class="p">,</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plot_estimator_variances</span><span class="p">(</span>
    <span class="n">optimized_estimators</span><span class="p">,</span> <span class="n">est_labels</span><span class="p">,</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">relative_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cost_normalization</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_multi_level_monte_carlo_002.png" srcset="../../_images/sphx_glr_plot_multi_level_monte_carlo_002.png" alt="plot multi level monte carlo" class = "sphx-glr-single-img"/><p>The left plot shows that the variance of the MLMC estimator is over and order of magnitude smaller than the variance of the single fidelity MC estimator for a fixed cost. The impact of using the approximate covariance is more significant for small samples sizes.</p>
<p>The right plot depicts the percentage of the computational cost due to evaluating each model. The numbers in the bars represent the number of samples allocated to each model. Relative to the low fidelity models only a small number of samples are allocated to the high-fidelity model, however evaluating these samples represents approximately 50% of the total cost.</p>
</section>
<section id="multi-index-monte-carlo">
<h2>Multi-index Monte Carlo<a class="headerlink" href="#multi-index-monte-carlo" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Multi-Level Monte Carlo utilizes a sequence of models controlled by a single hyper-parameter, specifying the level of discretization for example, in a manner that balances computational cost with increasing accuracy. In many applications, however, multiple hyper-parameters may control the model discretization, such as the mesh and time step sizes. In these situations, it may not be clear how to construct a one-dimensional hierarchy represented by a scalar hyper-parameter. To overcome this limitation, a generalization of multi-level Monte Carlo, referred to as multi-index stochastic collocation (MIMC), was developed to deal with multivariate hierarchies with multiple refinement hyper-parameters <a class="reference internal" href="#hntnm2016" id="id4"><span>[HNTNM2016]</span></a>.  PyApprox does not implement MIMC but a surrogate based version called Multi-index stochastic collocation (MISC) is presented in this tutorial <a class="reference internal" href="plot_multiindex_collocation.html#sphx-glr-auto-tutorials-multi-fidelity-plot-multiindex-collocation-py"><span class="std std-ref">Multi-level and Multi-index Collocation</span></a>.</p>
</section>
<section id="the-relationship-between-mlmc-and-acv">
<h2>The Relationship between MLMC and ACV<a class="headerlink" href="#the-relationship-between-mlmc-and-acv" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>MLMC estimators can be thought of a specific case of an ACV estimator. When using two models this can be seen by</p>
<div class="math notranslate nohighlight">
\[\begin{split}Q_{0}^\mathrm{ML}(\rvset_0,\rvset_1)&amp;=Y_{0}(\rvset_0)+Y_{1}(\rvset_1)\\
&amp;=Q_{0}(\rvset_0)-Q_{1}(\rvset_0)+Q_{1}(\rvset_1)\\
&amp;=Q_{0}(\rvset_0)-(Q_{1}(\rvset_0)-Q_{1}(\rvset_1))\end{split}\]</div>
<p>which has the same form as the two model ACV estimator presented in <a class="reference internal" href="plot_approximate_control_variates.html#sphx-glr-auto-tutorials-multi-fidelity-plot-approximate-control-variates-py"><span class="std std-ref">Two model Approximate Control Variate Monte Carlo</span></a> where the control variate weight has been set to <span class="math notranslate nohighlight">\(-1\)</span>.</p>
<p>For three models the allocation matrix of MLMC is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mat{A}=\begin{bmatrix}
0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}\end{split}\]</div>
<p>The following code plots the allocation matrix of one of the 5-model estimator we have already optimized. The numbers inside the boxes represent the sizes <span class="math notranslate nohighlight">\(p_m\)</span> of the independent partitions (different colors).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;xtick.labelsize&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
          <span class="s1">&#39;ytick.labelsize&#39;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
          <span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">18</span><span class="p">}</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">mlmc_ests</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot_allocation</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_multi_level_monte_carlo_003.png" srcset="../../_images/sphx_glr_plot_multi_level_monte_carlo_003.png" alt="plot multi level monte carlo" class = "sphx-glr-single-img"/><p>MLMC was designed when it is possible to always able to create new models with smaller bias than those already being used. Such a situation may occur, when refining the mesh of a finite element discretization. However, ACV methods were designed where there is one trusted high-fidelity model and unbiased statistics of that model are required. In the setting targeted by MLMC, the properties of MLMC can be used to bound the work needed to achieve a certain MSE if theoretical estimates of bias convergence rates are available. Such results are not possible with ACV.</p>
<p>The following code builds an MLMC estimator with the optimal control variate weights and compares them to traditional MLMC when estimating a single mean.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">estimators</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">get_estimator</span><span class="p">(</span><span class="s2">&quot;mc&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="p">,</span> <span class="n">costs</span><span class="p">),</span>
    <span class="n">get_estimator</span><span class="p">(</span><span class="s2">&quot;mlmc&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="p">,</span> <span class="n">costs</span><span class="p">),</span>
    <span class="n">get_estimator</span><span class="p">(</span><span class="s2">&quot;grd&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="p">,</span> <span class="n">costs</span><span class="p">,</span>
                  <span class="n">recursion_index</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">))]</span>
<span class="n">est_labels</span> <span class="o">=</span> <span class="n">mathrm_labels</span><span class="p">([</span><span class="s2">&quot;MC&quot;</span><span class="p">,</span> <span class="s2">&quot;MLMC&quot;</span><span class="p">,</span> <span class="s2">&quot;MLMC-OPT&quot;</span><span class="p">])</span>
<span class="n">optimized_estimators</span> <span class="o">=</span> <span class="n">compare_estimator_variances</span><span class="p">(</span>
    <span class="n">target_costs</span><span class="p">,</span> <span class="n">estimators</span><span class="p">)</span>

<span class="n">axs</span> <span class="o">=</span> <span class="p">[</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))[</span><span class="mi">1</span><span class="p">]]</span>

<span class="c1"># get estimators for target cost = 100</span>
<span class="n">mlmc_est_100</span> <span class="o">=</span> <span class="p">[</span><span class="n">optimized_estimators</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">optimized_estimators</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plot_estimator_variance_reductions</span><span class="p">(</span>
    <span class="n">mlmc_est_100</span><span class="p">,</span> <span class="n">est_labels</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_multi_level_monte_carlo_004.png" srcset="../../_images/sphx_glr_plot_multi_level_monte_carlo_004.png" alt="plot multi level monte carlo" class = "sphx-glr-single-img"/><p>For this problem there is a substantial difference between the two types of MLMC estimators.</p>
<section id="remark">
<h3>Remark<a class="headerlink" href="#remark" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>MLMC was originally developed to estimate the mean of a function, but adaptations MMLC have since ben developed to estimate other statistics, e.g. <a class="reference internal" href="#md2019" id="id5"><span>[MD2019]</span></a>. PyApprox, however, does not implement these specific methods, because it implements a more flexible way to compute multiple statistics which we will describe in later tutorials.</p>
</section>
</section>
<section id="video">
<h2>Video<a class="headerlink" href="#video" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Click on the image below to view a video tutorial on multi-level Monte Carlo quadrature</p>
<a class="reference external image-reference" href="https://youtu.be/ilvqZfa2Vt0?si=4ErVOsViRbn856wU"><img alt="../../_images/mlmc-thumbnail.png" src="../../_images/mlmc-thumbnail.png" /></a>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this heading">ÔÉÅ</a></h3>
<div role="list" class="citation-list">
<div class="citation" id="cgstcvs2011" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">CGSTCVS2011</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://doi.org/10.1007/s00791-011-0160-x">K.A. Cliffe, M.B. Giles, R. Scheichl, A.L. Teckentrup, Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Comput. Vis. Sci., 14, 3-15, 2011.</a></p>
</div>
<div class="citation" id="gor2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">GOR2008</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://doi.org/10.1287/opre.1070.0496">M.B. Giles, Multilevel Monte Carlo path simulation, Oper. Res., 56(3), 607-617, 2008.</a></p>
</div>
<div class="citation" id="gan2015" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">GAN2015</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://doi.org/10.1017/S096249291500001X">M. Giles, Multilevel Monte Carlo methods, Acta Numerica, 24, 259-328, 2015.</a></p>
</div>
<div class="citation" id="hntnm2016" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">HNTNM2016</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://doi.org/10.1007/s00211-015-0734-5">A. Haji-Ali, F. Nobile and R. Tempone. Multi-index Monte Carlo: when sparsity meets sampling. Numerische Mathematik, 132(4), 767-806, 2016.</a></p>
</div>
<div class="citation" id="md2019" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">MD2019</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://doi.org/10.1137/18M1216389">P. Mycek, M. De Lozzo. Multilevel Monte Carlo Covariance Estimation for the Computation of Sobol‚Äô Indices. 2019.</a></p>
</div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  2.645 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-tutorials-multi-fidelity-plot-multi-level-monte-carlo-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/bb1275901dfaafa79dd0bbce6a0f1ae4/plot_multi_level_monte_carlo.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_multi_level_monte_carlo.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/40dea2eaf8a95e3cf6ebe9d969e08ab7/plot_multi_level_monte_carlo.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_multi_level_monte_carlo.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="plot_allocation_matrices.html" class="btn btn-neutral float-left" title="Approximate Control Variate Allocation Matrices" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="plot_multi_fidelity_monte_carlo.html" class="btn btn-neutral float-right" title="Multi-fidelity Monte Carlo" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019 National Technology &amp; Engineering Solutions of Sandia, LLC (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S. Government retains certain rights in this software..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>