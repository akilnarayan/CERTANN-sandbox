<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Bayesian Inference &mdash; PyApprox 1.0.0 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/plot_directive.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script>window.MathJax = {"tex": {"macros": {"V": ["{\\boldsymbol{#1}}", 1], "mean": ["{\\mathbb{E}\\left[#1\\right]}", 1], "var": ["{\\mathbb{V}\\left[#1\\right]}", 1], "rv": "{z}", "reals": "\\mathbb{R}", "pdf": "\\rho", "rvdom": "\\Gamma", "coloneqq": "\\colon=", "norm": ["{\\lVert #1 \\rVert}", 1], "argmax": ["\\operatorname{argmax}"], "argmin": ["\\operatorname{argmin}"], "covar": ["\\mathbb{C}\\text{ov}\\left[#1,#2\\right]", 2], "corr": ["\\mathbb{C}\\text{or}\\left[#1,#2\\right]", 2], "ai": "\\alpha", "bi": "\\beta", "dx": ["\\;\\mathrm{d}#1", 1]}}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Gaussian Networks" href="plot_bayesian_networks.html" />
    <link rel="prev" title="Push Forward Based Inference" href="plot_push_forward_based_inference.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> PyApprox
            <img src="../../_static/pyapprox-logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../auto_examples/index.html">Software Tutorials</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Theoretical Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#model-analysis">Model Analysis</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#inference">Inference</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_push_forward_based_inference.html">Push Forward Based Inference</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Bayesian Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exact-linear-gaussian-inference">Exact Linear-Gaussian Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inexact-inference-using-markov-chain-monte-carlo">Inexact Inference using Markov Chain Monte Carlo</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_bayesian_networks.html">Gaussian Networks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#multi-fidelity-methods">Multi-Fidelity Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#surrogates">Surrogates</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmarks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Reference Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_reference_guide.html">User Reference Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PyApprox</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">Theoretical Tutorials</a> &raquo;</li>
      <li>Bayesian Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/auto_tutorials/inference/plot_bayesian_inference.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-auto-tutorials-inference-plot-bayesian-inference-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="bayesian-inference">
<span id="sphx-glr-auto-tutorials-inference-plot-bayesian-inference-py"></span><h1>Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Permalink to this headline"></a></h1>
<p>This tutorial describes how to use Bayesian inference condition estimates of uncertainty on observational data.</p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline"></a></h2>
<p>When observational data are available, that data should be used to inform prior assumptions of model uncertainties. This so-called inverse problem that seeks to estimate uncertain parameters from measurements or observations is usually ill-posed. Many different realizations of parameter values may be consistent with the data. The lack of a unique solution can be due to the non-convexity of the parameter-to-QoI map, lack of data, and model structure and measurement errors.</p>
<p>Deterministic model calibration is an inverse problem that seeks to find a single parameter set that minimizes the misfit between the measurements and model predictions. A unique solution is found by simultaneously minimising the misfit and a regularization term which penalises certain characteristics of the model parameters.</p>
<p>In the presence of uncertainty we typically do not want a single optimal solution, but rather a probabilistic description of the extent to which different realizations of parameters are consistent with the observations.
Bayesian inference <a class="reference internal" href="#kaipo2005" id="id1"><span>[KAIPO2005]</span></a> can be used to define a posterior density for the model parameters <span class="math notranslate nohighlight">\(\rv\)</span> given
observational data <span class="math notranslate nohighlight">\(\V{y}=(y_1,\ldots,y_{n_y})\)</span>:</p>
<section id="bayes-rule">
<h3>Bayes Rule<a class="headerlink" href="#bayes-rule" title="Permalink to this headline"></a></h3>
<p>Given a model <span class="math notranslate nohighlight">\(\mathcal{M}(\rv)\)</span> parameterized by a set of parameters <span class="math notranslate nohighlight">\(\rv\)</span>, our goal is to infer the parameter <span class="math notranslate nohighlight">\(\rv\)</span> from data <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>Bayes Theorem describes the probability of the parameters <span class="math notranslate nohighlight">\(\rv\)</span> conditioned on the data <span class="math notranslate nohighlight">\(d\)</span> is proportional to the conditional probability of observing the data given the parameters multiplied by the probability of observing the data, that is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\pi (\rv\mid d)&amp;=\frac{\pi (d\mid \rv)\,\pi (\rv)}{\pi(d)}\\
&amp;=\frac{\pi (d\mid \rv)\,\pi (\rv)}{\int_{\mathbb{R}^d} \pi (d\mid \rv)\,\pi (\rv)\,d\rv}\end{split}\]</div>
<p>The density <span class="math notranslate nohighlight">\(\pi (\rv\mid d)\)</span> is referred to as the posterior density.</p>
<div class="math notranslate nohighlight">
\[\pi_{\text{post}}(\rv)=\pi_\text(\rv\mid\V{y})=\frac{\pi(\V{y}|\rv)\pi(\rv)}{\int_{\rvdom}
\pi(\V{y}|\rv)\pi(\rv)d\rv}\]</div>
</section>
<section id="prior">
<h3>Prior<a class="headerlink" href="#prior" title="Permalink to this headline"></a></h3>
<p>To find the posterior density we must first quantify our prior belief of the possible values
of the parameter that can give the data. We do this by specifying the probability of
observing the parameter independently of observing the data.</p>
<p>Here we specify the prior distribution to be Normally distributed, e.g</p>
<div class="math notranslate nohighlight">
\[\pi\sim N(m_\text{prior},\Sigma_\text{prior})\]</div>
</section>
<section id="likelihood">
<h3>Likelihood<a class="headerlink" href="#likelihood" title="Permalink to this headline"></a></h3>
<p>Next we must specify the likelihood <span class="math notranslate nohighlight">\(\pi(d\mid \rv)\)</span> of observing the data given a realizations of the parameter <span class="math notranslate nohighlight">\(\rv\)</span>
The likelihood answers the question: what is the distribution of the data assuming that <span class="math notranslate nohighlight">\(\rv\)</span> are the exact parameters?</p>
<p>The form of the likelihood is derived from an assumed relationship between the model and the
data.</p>
<p>It is often assumed that</p>
<div class="math notranslate nohighlight">
\[d=\mathcal{M}(\rv)+\eta\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta\sim N(0,\Sigma_\text{noise})\)</span> is normally distributed noise with zero mean and covariance <span class="math notranslate nohighlight">\(\Sigma_\text{noise}\)</span>.</p>
<p>In this case the likelihood is</p>
<div class="math notranslate nohighlight">
\[\pi(d|\rv)=\frac{1}{\sqrt {(2\pi )^{k}|{\boldsymbol {\Sigma_\text{noise} }}}|}\exp \left(-{\frac {1}{2}}(\mathcal{M}(\rv)-d)^{\mathrm {T} }{\boldsymbol {\Sigma_\text{noise} }}^{-1}(\mathcal{M}(\rv)-d)\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(|\Sigma_\text{noise}|=\det \Sigma_\text{noise}\)</span> is the determinant of <span class="math notranslate nohighlight">\(\Sigma_\text{noise}\)</span></p>
</section>
</section>
<section id="exact-linear-gaussian-inference">
<h2>Exact Linear-Gaussian Inference<a class="headerlink" href="#exact-linear-gaussian-inference" title="Permalink to this headline"></a></h2>
<p>In the following we will generate data at a truth parameter <span class="math notranslate nohighlight">\(\rv_\text{truth}\)</span> and use Bayesian inference
to estimate the probability of any model parameter <span class="math notranslate nohighlight">\(\rv\)</span> conditioned on the observations we generated.
Firstly assume  <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> is a linear model, i.e.</p>
<div class="math notranslate nohighlight">
\[\mathcal{M}(\rv)=A\rv+b,\]</div>
<p>and as above assume that</p>
<div class="math notranslate nohighlight">
\[d=\mathcal{M}(\rv)+\eta\]</div>
<p>Now define the prior probability of the parameters to be</p>
<div class="math notranslate nohighlight">
\[\pi(\rv)\sim N(m_\text{prior},\Sigma_\text{prior})\]</div>
<p>Under these assumptions, the marginal density (integrating over the prior of <span class="math notranslate nohighlight">\(\rv\)</span>) of the data and parameters is</p>
<div class="math notranslate nohighlight">
\[\pi(d)\sim N(m_\text{noise}+Am_\text{prior},\Sigma_\text{noise}+ A\Sigma_\text{prior} A^T)=N(m_\text{data},\Sigma_\text{data})\]</div>
<p>and the joint density of the parameters and data is</p>
<div class="math notranslate nohighlight">
\[\pi(\rv,d)\sim N(m_\text{joint},\Sigma_\text{joint})\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol m_\text{joint}=\begin{bmatrix} \boldsymbol m_\text{prior} \\ \boldsymbol m_\text{data}\end{bmatrix},\quad
\boldsymbol \Sigma_\text{joint}=\begin{bmatrix} \boldsymbol\Sigma_\text{prior} &amp; \boldsymbol\Sigma_\text{prior,data} \\ \boldsymbol\Sigma_\text{prior,data} &amp; \boldsymbol\Sigma_\text{data}\end{bmatrix}\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\Sigma_\text{prior,data}=A\Sigma_\text{prior}\]</div>
<p>is the covariance between the parameters and data.</p>
<p>Now let us setup this problem in Python</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyapprox.bayes.markov_chain_monte_carlo</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">run_bayesian_inference_gaussian_error_model</span><span class="p">,</span> <span class="n">PYMC3LogLikeWrapper</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">pyapprox.bayes.tests.test_markov_chain_monte_carlo</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ExponentialQuarticLogLikelihoodModel</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">pyapprox.variables.density</span> <span class="kn">import</span> <span class="n">NormalDensity</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">.5</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="c1"># define the prior</span>
<span class="n">prior_covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">NormalDensity</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">prior_covariance</span><span class="p">)</span>
<span class="c1"># define the noise</span>
<span class="n">noise_covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">NormalDensity</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_covariance</span><span class="p">)</span>
<span class="c1"># compute the covariance between the prior and data</span>
<span class="n">C_12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">prior_covariance</span><span class="p">)</span>
<span class="c1"># define the data marginal distribution</span>
<span class="n">data_covariance</span> <span class="o">=</span> <span class="n">noise_covariance</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">C_12</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">NormalDensity</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">prior</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="p">,</span>
                         <span class="n">data_covariance</span><span class="p">)</span>
<span class="c1"># define the covariance of the joint distribution of the prior and data</span>


<span class="k">def</span> <span class="nf">form_normal_joint_covariance</span><span class="p">(</span><span class="n">C_11</span><span class="p">,</span> <span class="n">C_22</span><span class="p">,</span> <span class="n">C_12</span><span class="p">):</span>
    <span class="n">num_vars1</span> <span class="o">=</span> <span class="n">C_11</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_vars2</span> <span class="o">=</span> <span class="n">C_22</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">joint_covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">num_vars1</span><span class="o">+</span><span class="n">num_vars2</span><span class="p">,</span> <span class="n">num_vars1</span><span class="o">+</span><span class="n">num_vars2</span><span class="p">))</span>
    <span class="n">joint_covariance</span><span class="p">[:</span><span class="n">num_vars1</span><span class="p">,</span> <span class="p">:</span><span class="n">num_vars1</span><span class="p">]</span> <span class="o">=</span> <span class="n">C_11</span>
    <span class="n">joint_covariance</span><span class="p">[</span><span class="n">num_vars1</span><span class="p">:,</span> <span class="n">num_vars1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">C_22</span>
    <span class="n">joint_covariance</span><span class="p">[:</span><span class="n">num_vars1</span><span class="p">,</span> <span class="n">num_vars1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">C_12</span>
    <span class="n">joint_covariance</span><span class="p">[</span><span class="n">num_vars1</span><span class="p">:,</span> <span class="p">:</span><span class="n">num_vars1</span><span class="p">]</span> <span class="o">=</span> <span class="n">C_12</span>
    <span class="k">return</span> <span class="n">joint_covariance</span>


<span class="n">joint_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">prior</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">))</span>
<span class="n">joint_covariance</span> <span class="o">=</span> <span class="n">form_normal_joint_covariance</span><span class="p">(</span>
    <span class="n">prior_covariance</span><span class="p">,</span> <span class="n">data_covariance</span><span class="p">,</span> <span class="n">C_12</span><span class="p">)</span>
<span class="n">joint</span> <span class="o">=</span> <span class="n">NormalDensity</span><span class="p">(</span><span class="n">joint_mean</span><span class="p">,</span> <span class="n">joint_covariance</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can plot the joint distribution and some samples from that distribution
and print the sample covariance of the joint distribution</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">theta_samples</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">generate_samples</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span>
<span class="n">noise_samples</span> <span class="o">=</span> <span class="n">noise</span><span class="o">.</span><span class="n">generate_samples</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span>
<span class="n">data_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">theta_samples</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span> <span class="o">+</span> <span class="n">noise_samples</span>
<span class="n">plot_limits</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta_samples</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">theta_samples</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span>
               <span class="n">data_samples</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">data_samples</span><span class="o">.</span><span class="n">max</span><span class="p">()]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">joint</span><span class="o">.</span><span class="n">plot_density</span><span class="p">(</span><span class="n">plot_limits</span><span class="o">=</span><span class="n">plot_limits</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">100</span><span class="p">],</span> <span class="n">data_samples</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">100</span><span class="p">],</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_bayesian_inference_001.png" srcset="../../_images/sphx_glr_plot_bayesian_inference_001.png" alt="plot bayesian inference" class = "sphx-glr-single-img"/><section id="conditional-probability-of-multivariate-gaussians">
<h3>Conditional probability of multivariate Gaussians<a class="headerlink" href="#conditional-probability-of-multivariate-gaussians" title="Permalink to this headline"></a></h3>
<p>For multivariate Gaussians the dsitribution of <span class="math notranslate nohighlight">\(x\)</span> conditional on observing the data <span class="math notranslate nohighlight">\(d^\star=d_\text{truth}+\eta^\star\)</span>, <span class="math notranslate nohighlight">\(\pi(x\mid d=d^\star)\sim N(m_\text{post},\Sigma_\text{post})\)</span> is a multivariate Gaussian with mean and covariance</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{m}_\text{post}&amp;=\boldsymbol m_\text{prior} + \boldsymbol\Sigma_\text{prior,data} \boldsymbol\Sigma_{data}^{-1}\left( \mathbf{d}^\star - \boldsymbol m_\text{data}\right),\\
\boldsymbol \Sigma_\text{post}&amp;=\boldsymbol \Sigma_\text{prior}-\boldsymbol \Sigma_\text{prior,data}\boldsymbol\Sigma_\text{data}^{-1}\boldsymbol\Sigma_\text{data,prior}^T.\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta^\star\)</span> is a random sample from the noise variable <span class="math notranslate nohighlight">\(\eta\)</span>. In the case of one parameter and one QoI we have</p>
<div class="math notranslate nohighlight">
\[\pi(x\mid d=d^\star) \sim\ N\left(m_\text{prior}+\frac{\sigma_\text{prior}}{\sigma_\text{data}}\rho( d^\star - m_\text{data}),\, (1-\rho^2)\sigma_\text{prior}^2\right).\]</div>
<p>where the correlation coefficient between the parameter and data is</p>
<p>Lets use this formula to update the prior when one data <span class="math notranslate nohighlight">\(d^\star\)</span> becomes available.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">condition_normal_on_data</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">covariance</span><span class="p">,</span> <span class="n">fixed_indices</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mean</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">ind_remain</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">fixed_indices</span><span class="p">)))</span>
    <span class="n">new_mean</span> <span class="o">=</span> <span class="n">mean</span><span class="p">[</span><span class="n">ind_remain</span><span class="p">]</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">values</span> <span class="o">-</span> <span class="n">mean</span><span class="p">[</span><span class="n">fixed_indices</span><span class="p">]</span>
    <span class="n">sigma_11</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">covariance</span><span class="p">[</span><span class="n">ind_remain</span><span class="p">,</span> <span class="n">ind_remain</span><span class="p">],</span> <span class="n">ndmin</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma_12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">covariance</span><span class="p">[</span><span class="n">ind_remain</span><span class="p">,</span> <span class="n">fixed_indices</span><span class="p">],</span> <span class="n">ndmin</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma_22</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">covariance</span><span class="p">[</span><span class="n">fixed_indices</span><span class="p">,</span> <span class="n">fixed_indices</span><span class="p">],</span> <span class="n">ndmin</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">sigma_12</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">sigma_22</span><span class="p">,</span> <span class="n">diff</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">new_mean</span> <span class="o">=</span> <span class="n">new_mean</span> <span class="o">+</span> <span class="n">update</span>
    <span class="n">new_cov</span> <span class="o">=</span> <span class="n">sigma_11</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">sigma_12</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">sigma_22</span><span class="p">,</span> <span class="n">sigma_12</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">new_mean</span><span class="p">,</span> <span class="n">new_cov</span>


<span class="n">x_truth</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">data_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x_truth</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">noise</span><span class="o">.</span><span class="n">generate_samples</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">new_mean</span><span class="p">,</span> <span class="n">new_cov</span> <span class="o">=</span> <span class="n">condition_normal_on_data</span><span class="p">(</span>
    <span class="n">joint_mean</span><span class="p">,</span> <span class="n">joint_covariance</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">data_obs</span><span class="p">)</span>
<span class="n">posterior</span> <span class="o">=</span> <span class="n">NormalDensity</span><span class="p">(</span><span class="n">new_mean</span><span class="p">,</span> <span class="n">new_cov</span><span class="p">)</span>
</pre></div>
</div>
<p>Now lets plot the prior and posterior of the parameters as well as the joint distribution and the data.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">prior</span><span class="o">.</span><span class="n">plot_density</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;prior&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_truth</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x_\text</span><span class="si">{truth}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">posterior</span><span class="o">.</span><span class="n">plot_density</span><span class="p">(</span><span class="n">plot_limits</span><span class="o">=</span><span class="n">prior</span><span class="o">.</span><span class="n">plot_limits</span><span class="p">,</span>
                       <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">colorbar_lims</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># [0,1.5]</span>
<span class="n">joint</span><span class="o">.</span><span class="n">plot_density</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">colorbar_lims</span><span class="o">=</span><span class="n">colorbar_lims</span><span class="p">)</span>
<span class="n">axhline</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">data_obs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">axplot</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_truth</span><span class="p">,</span> <span class="n">data_obs</span><span class="p">,</span> <span class="s1">&#39;ok&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_bayesian_inference_002.png" srcset="../../_images/sphx_glr_plot_bayesian_inference_002.png" alt="plot bayesian inference" class = "sphx-glr-single-img"/><p>Lets also plot the joint distribution and marginals in a 3d plot</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">data_obs_limit_state</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">vals</span><span class="p">,</span> <span class="n">data_obs</span><span class="p">):</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="nb">float</span><span class="p">)</span>
    <span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;=</span> <span class="n">data_obs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">I</span><span class="p">,</span> <span class="mf">0.</span>


<span class="kn">from</span> <span class="nn">pyapprox.util.visualization</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_meshgrid_function_data</span><span class="p">,</span> <span class="n">create_3d_axis</span><span class="p">,</span> <span class="n">plot_surface</span><span class="p">,</span> <span class="n">plot_contours</span><span class="p">)</span>
<span class="n">num_pts_1d</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">limit_state</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">data_obs_limit_state</span><span class="p">,</span> <span class="n">data_obs</span><span class="o">=</span><span class="n">data_obs</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">get_meshgrid_function_data</span><span class="p">(</span>
    <span class="n">joint</span><span class="o">.</span><span class="n">pdf</span><span class="p">,</span> <span class="n">joint</span><span class="o">.</span><span class="n">plot_limits</span><span class="p">,</span> <span class="n">num_pts_1d</span><span class="p">,</span> <span class="n">qoi</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">offset</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">x_truth</span><span class="p">,</span> <span class="n">data_obs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">offset</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">create_3d_axis</span><span class="p">()</span>
<span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">axis_labels</span><span class="o">=</span><span class="p">[</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$d$&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">],</span>
             <span class="n">limit_state</span><span class="o">=</span><span class="n">limit_state</span><span class="p">,</span>
             <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">plot_axes</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">num_contour_levels</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">cset</span> <span class="o">=</span> <span class="n">plot_contours</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">num_contour_levels</span><span class="o">=</span><span class="n">num_contour_levels</span><span class="p">,</span>
                     <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Z_prior</span> <span class="o">=</span> <span class="n">prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z_prior</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">Y</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">)</span>
<span class="n">Z_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z_data</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">+</span><span class="n">offset</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">Z_prior</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">Z_data</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">num_pts_1d</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data_obs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_pts_1d</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">offset</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_pts_1d</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x_truth</span><span class="p">],</span> <span class="p">[</span><span class="n">data_obs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">offset</span><span class="p">],</span>
            <span class="n">zorder</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_bayesian_inference_003.png" srcset="../../_images/sphx_glr_plot_bayesian_inference_003.png" alt="plot bayesian inference" class = "sphx-glr-single-img"/><p>Now lets assume another piece of observational data becomes available we can use the posterior as a new prior.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_obs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">posteriors</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="n">num_obs</span>
<span class="n">posteriors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">posterior</span>
<span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_obs</span><span class="p">):</span>
    <span class="n">new_prior</span> <span class="o">=</span> <span class="n">posteriors</span><span class="p">[</span><span class="n">ii</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">data_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x_truth</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">noise</span><span class="o">.</span><span class="n">generate_samples</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">C_12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">new_prior</span><span class="o">.</span><span class="n">covariance</span><span class="p">)</span>
    <span class="n">new_joint_covariance</span> <span class="o">=</span> <span class="n">form_normal_joint_covariance</span><span class="p">(</span>
        <span class="n">new_prior</span><span class="o">.</span><span class="n">covariance</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">covariance</span><span class="p">,</span> <span class="n">C_12</span><span class="p">)</span>
    <span class="n">new_joint</span> <span class="o">=</span> <span class="n">NormalDensity</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">new_prior</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">)),</span>
                                  <span class="n">new_joint_covariance</span><span class="p">)</span>
    <span class="n">new_mean</span><span class="p">,</span> <span class="n">new_cov</span> <span class="o">=</span> <span class="n">condition_normal_on_data</span><span class="p">(</span>
        <span class="n">new_joint</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">new_joint</span><span class="o">.</span><span class="n">covariance</span><span class="p">,</span>
        <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">new_prior</span><span class="o">.</span><span class="n">num_vars</span><span class="p">(),</span> <span class="n">new_prior</span><span class="o">.</span><span class="n">num_vars</span><span class="p">()</span><span class="o">+</span><span class="n">data</span><span class="o">.</span><span class="n">num_vars</span><span class="p">()),</span>
        <span class="n">data_obs</span><span class="p">)</span>
    <span class="n">posteriors</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="o">=</span> <span class="n">NormalDensity</span><span class="p">(</span><span class="n">new_mean</span><span class="p">,</span> <span class="n">new_cov</span><span class="p">)</span>
</pre></div>
</div>
<p>And now lets again plot the joint density before the last data was added and final posterior and the intermediate priors.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">prior</span><span class="o">.</span><span class="n">plot_density</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;prior&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_truth</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$x_\text</span><span class="si">{truth}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_obs</span><span class="p">):</span>
    <span class="n">posteriors</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span><span class="o">.</span><span class="n">plot_density</span><span class="p">(</span>
        <span class="n">plot_limits</span><span class="o">=</span><span class="n">prior</span><span class="o">.</span><span class="n">plot_limits</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;posterior&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">colorbar_lims</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">new_joint</span><span class="o">.</span><span class="n">plot_density</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">colorbar_lims</span><span class="o">=</span><span class="n">colorbar_lims</span><span class="p">,</span>
                       <span class="n">plot_limits</span><span class="o">=</span><span class="n">joint</span><span class="o">.</span><span class="n">plot_limits</span><span class="p">)</span>
<span class="n">axhline</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">data_obs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">axplot</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_truth</span><span class="p">,</span> <span class="n">data_obs</span><span class="p">,</span> <span class="s1">&#39;ok&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_bayesian_inference_004.png" srcset="../../_images/sphx_glr_plot_bayesian_inference_004.png" alt="plot bayesian inference" class = "sphx-glr-single-img"/><p>As you can see the variance of the joint density decreases as more data is added. The posterior variance also decreases and the posterior will converge to a Dirac-delta function as the number of observations tends to infinity. Currently the mean of the posterior is not near the true parameter value (the horizontal line). Try increasing <code class="docutils literal notranslate"><span class="pre">num_obs1</span></code> to see what happens.</p>
</section>
</section>
<section id="inexact-inference-using-markov-chain-monte-carlo">
<h2>Inexact Inference using Markov Chain Monte Carlo<a class="headerlink" href="#inexact-inference-using-markov-chain-monte-carlo" title="Permalink to this headline"></a></h2>
<p>When using non-linear or non-Gaussian priors, a functional representation of the posterior distribution <span class="math notranslate nohighlight">\(\pi_\text{post}\)</span> cannot be computed analytically. Instead the the posterior is characterized by samples drawn from the posterior using Markov-chain Monte Carlo (MCMC) sampling methods.</p>
<p>Lets consider non-linear model with two uncertain parameters with independent uniform priors on [-2,2] and the negative log likelihood function</p>
<div class="math notranslate nohighlight">
\[-\log\left(\pi(d\mid\rv)\right)=\frac{1}{10}\rv_1^4 + \frac{1}{2}(2\rv_2-\rv_1^2)^2\]</div>
<p>We can sample the posterior using Sequential Markov Chain Monte Carlo using the following code.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">pyapprox.variables.joint</span> <span class="kn">import</span> <span class="n">IndependentMarginalsVariable</span>
<span class="n">univariate_variables</span> <span class="o">=</span> <span class="p">[</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)]</span>
<span class="n">plot_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="mi">2</span>
<span class="n">variables</span> <span class="o">=</span> <span class="n">IndependentMarginalsVariable</span><span class="p">(</span><span class="n">univariate_variables</span><span class="p">)</span>

<span class="n">loglike</span> <span class="o">=</span> <span class="n">ExponentialQuarticLogLikelihoodModel</span><span class="p">()</span>
<span class="n">loglike</span> <span class="o">=</span> <span class="n">PYMC3LogLikeWrapper</span><span class="p">(</span><span class="n">loglike</span><span class="p">)</span>

<span class="c1"># number of draws from the distribution</span>
<span class="n">ndraws</span> <span class="o">=</span> <span class="mi">500</span>
<span class="c1"># number of &quot;burn-in points&quot; (which we&#39;ll discard)</span>
<span class="n">nburn</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">ndraws</span><span class="o">*</span><span class="mf">0.1</span><span class="p">))</span>
<span class="c1"># number of parallel chains</span>
<span class="n">njobs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">samples</span><span class="p">,</span> <span class="n">effective_sample_size</span><span class="p">,</span> <span class="n">map_sample</span> <span class="o">=</span> \
    <span class="n">run_bayesian_inference_gaussian_error_model</span><span class="p">(</span>
        <span class="n">loglike</span><span class="p">,</span> <span class="n">variables</span><span class="p">,</span> <span class="n">ndraws</span><span class="p">,</span> <span class="n">nburn</span><span class="p">,</span> <span class="n">njobs</span><span class="p">,</span>
        <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;smc&#39;</span><span class="p">,</span> <span class="n">get_map</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">print_summary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAP sample&#39;</span><span class="p">,</span> <span class="n">map_sample</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Warning: gradient not available.(E.g. vars contains discrete variables). MAP estimates may not be accurate for the default parameters. Defaulting to non-gradient minimization &#39;Powell&#39;.
Sample initial stage: ...
Stage:   0 Beta: 0.586 Steps:  25 Acce: 1.000
Stage:   1 Beta: 1.000 Steps:  25 Acce: 0.443
arviz - WARNING - Shape validation failed: input_shape: (1, 500), minimum_shape: (chains=2, draws=4)
      mean     sd  hdi_3%  hdi_97%  ...  ess_sd  ess_bulk  ess_tail  r_hat
z_0 -0.060  0.967  -1.606    1.726  ...   388.0     398.0     460.0    NaN
z_1  0.424  0.648  -0.736    1.561  ...   589.0     603.0     515.0    NaN

[2 rows x 11 columns]
MAP sample [ 1.23767663e-11 -7.43454187e-11]
</pre></div>
</div>
<p>The NUTS sampler offerred by PyMC3 can also be used by specifying <cite>algorithm=’nuts’</cite>. This sampler requires gradients of the likelihood function which if not provided will be computed using finite difference.</p>
<p>Lets plot the posterior distribution and the MCMC samples. First we must compute the evidence</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyapprox.surrogates.orthopoly.quadrature</span> <span class="kn">import</span> <span class="n">gauss_jacobi_pts_wts_1D</span>
<span class="kn">from</span> <span class="nn">pyapprox.surrogates.interp.tensorprod</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_tensor_product_quadrature_rule</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">unnormalized_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">loglike</span><span class="o">.</span><span class="n">loglike</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">rvs</span> <span class="o">=</span> <span class="n">variables</span><span class="o">.</span><span class="n">marginals</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">variables</span><span class="o">.</span><span class="n">num_vars</span><span class="p">()):</span>
        <span class="n">vals</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="n">rvs</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">ii</span><span class="p">,</span> <span class="p">:])</span>
    <span class="k">return</span> <span class="n">vals</span>


<span class="k">def</span> <span class="nf">univariate_quadrature_rule</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">gauss_jacobi_pts_wts_1D</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">*=</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span>


<span class="n">x</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">get_tensor_product_quadrature_rule</span><span class="p">(</span>
    <span class="mi">100</span><span class="p">,</span> <span class="n">variables</span><span class="o">.</span><span class="n">num_vars</span><span class="p">(),</span> <span class="n">univariate_quadrature_rule</span><span class="p">)</span>
<span class="n">evidence</span> <span class="o">=</span> <span class="n">unnormalized_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;evidence&#39;</span><span class="p">,</span> <span class="n">evidence</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">get_meshgrid_function_data</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">unnormalized_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">evidence</span><span class="p">,</span> <span class="n">plot_range</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Z</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">Z</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">30</span><span class="p">),</span>
    <span class="n">cmap</span><span class="o">=</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">samples</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="s1">&#39;ko&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_bayesian_inference_005.png" srcset="../../_images/sphx_glr_plot_bayesian_inference_005.png" alt="plot bayesian inference" class = "sphx-glr-single-img"/><p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>evidence 0.014953532968320103
</pre></div>
</div>
<p>Now lets compute the mean of the posterior using a highly accurate quadrature rule and compars this to the mean estimated using MCMC samples.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">exact_mean</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="o">*</span><span class="n">unnormalized_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">/</span><span class="n">evidence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;mcmc mean&#39;</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;exact mean&#39;</span><span class="p">,</span> <span class="n">exact_mean</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>mcmc mean [-0.04713958  0.42756304]
exact mean [-3.05426260e-16  4.33326051e-01]
</pre></div>
</div>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h3>
<dl class="citation">
<dt class="label" id="kaipo2005"><span class="brackets"><a class="fn-backref" href="#id1">KAIPO2005</a></span></dt>
<dd><p><a class="reference external" href="https://link.springer.com/book/10.1007/b138659">J. Kaipio and E. Somersalo. Statistical and Computational Inverse Problems. 2005</a></p>
</dd>
</dl>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  4.206 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-tutorials-inference-plot-bayesian-inference-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/43434288c7446fa54e82fe444db0cec7/plot_bayesian_inference.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_bayesian_inference.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/1126141c04021d5845d61b810ec2c767/plot_bayesian_inference.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_bayesian_inference.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="plot_push_forward_based_inference.html" class="btn btn-neutral float-left" title="Push Forward Based Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="plot_bayesian_networks.html" class="btn btn-neutral float-right" title="Gaussian Networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019 National Technology &amp; Engineering Solutions of Sandia, LLC (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the U.S. Government retains certain rights in this software..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>