============================= test session starts ==============================
platform darwin -- Python 3.8.11, pytest-6.2.5, py-1.11.0, pluggy-0.13.1
rootdir: /Users/jdjakem/software/pyapprox, configfile: pytest.ini
plugins: cov-3.0.0
collected 559 items

pyapprox/analysis/tests/test_active_subspace.py ...-1.0000000000403002 1.0000000000402627
.s....
pyapprox/analysis/tests/test_parameter_sweeps.py ..
pyapprox/analysis/tests/test_sensitivity_analysis.py [0.01692934]
Worst case relative interpolation error 0.00045443179125478623
Median relative interpolation error 0.00021702363801820757
3.5807683879189236
std of realizations error 0.018893609763428736
var of realizations error 0.026562622614238578
mean interpolation error 3.083836121106713e-06
F....10
.ncombinations 91390
[0. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 1.]
           mu*      sigma
Z_1    0.03681    0.01374
Z_2    0.21880    0.23331
Z_3    1.56133    1.59957
Z_4    0.84808    0.89703
Z_5    0.02853    0.02280
Z_6    0.07976    0.03173
..[9.62687902e-07 1.49563380e-07 7.32465571e-07] [0.99999045 0.99999979 0.99999999]
[0.00119213 0.00090398 0.00065314] [0.99856363 0.99874378 0.99863104]
Rel. Error 0.03112573882776962
....error [0.02998466]
Worst case relative interpolation error 0.0002884685068173573
Median relative interpolation error 0.00020554999062980566
0.004889068368345573
[ 0.00206074 -0.00444433 -0.00034484]
[ 0.00394277 -0.00607838  0.00187719]
[ 0.00206074 -0.00444433 -0.00034484 -0.0002812   0.00340871 -0.00034853]
..
pyapprox/bayes/tests/test_gaussian_network.py ....Prior Covariance
 [[ 1.00000000e+00  5.97676633e-18  5.00000000e-01 -1.62579881e-01
   1.84979886e-01  2.28371411e-01]
 [ 1.28748180e-17  1.00000000e+00 -1.88308603e-01 -3.30277245e-01
   1.36904661e-01 -1.76875810e-02]
 [ 5.00000000e-01 -1.88308603e-01  2.00000000e+00  4.71874619e-17
   3.96791286e-01  8.00000000e-01]
 [-1.62579881e-01 -3.30277245e-01  0.00000000e+00  2.00000000e+00
  -1.05526051e+00 -3.49015025e-01]
 [ 1.84979886e-01  1.36904661e-01  3.96791286e-01 -1.05526051e+00
   3.00000000e+00 -6.24500451e-17]
 [ 2.28371411e-01 -1.76875810e-02  8.00000000e-01 -3.49015025e-01
  -6.83203723e-17  3.00000000e+00]]
Prior Mean
 [-1. -1. -2. -2. -3. -3.]
[-1 -1 -2 -2 -3 -3]
.['Node_0', 'Node_1', 'Node_2']
...
pyapprox/bayes/tests/test_laplace.py .TODO replace by opearator hess_vec_prod = model.hess.apply(map_point,vectors). first arg says where to evaluate hessian opearator
TODO replace by opearator hess_vec_prod = model.hess.apply(map_point,vectors). first arg says where to evaluate hessian opearator
.......sTODO replace by opearator hess_vec_prod = model.hess.apply(map_point,vectors). first arg says where to evaluate hessian opearator
TODO replace by opearator hess_vec_prod = model.hess.apply(map_point,vectors). first arg says where to evaluate hessian opearator
..s...
pyapprox/benchmarks/tests/test_benchmarks.py Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.166761475250739         0.8369280177668985        0.2826914193519791       
0.1          1.166761475250739         1.1337781295023586        0.02826914193519475      
0.01         1.166761475250739         1.16346314067588          0.002826914193537402     
0.001        1.166761475250739         1.1664316417929754        0.0002826914195916639    
0.0001       1.166761475250739         1.166728491908131         2.8269139243502413e-05   
1e-05        1.166761475250739         1.1667581769181368        2.8269125027839445e-06   
1e-06        1.166761475250739         1.166761145476869         2.8264034850684116e-07   
1e-07        1.166761475250739         1.1667614430166395        2.7626982955325865e-08   
1e-08        1.166761475250739         1.1667614430166395        2.7626982955325865e-08   
1e-09        1.166761475250739         1.1667609101095877        4.843673391669951e-07    
1e-10        1.166761475250739         1.1667555810390695        5.051770901283687e-06    
1e-11        1.166761475250739         1.1667111721180845        4.311346725225612e-05    
1e-12        1.166761475250739         1.1661782650662644        0.0004998538234639253    
1e-13        1.166761475250739         1.163513729807164         0.0027835556045222713    
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          6.958093575000038         27.08975181818299         2.9061789981083463       
0.1          6.958093575000038         7.649555062779245         0.09988500647885927      
0.01         6.958093575000038         7.022447338775441         0.00929563953873515      
0.001        6.958093575000038         6.964484310569401         0.0009231094901276859    
0.0001       6.958093575000038         6.958732205225657         9.22468488605021e-05     
1e-05        6.958093575000038         6.958157433698364         9.224060079048341e-06    
1e-06        6.958093575000038         6.958099962068786         9.225817715171251e-07    
1e-07        6.958093575000038         6.958094216798936         9.277729157238452e-08    
1e-08        6.958093575000038         6.958093677778097         1.6383448669233806e-08   
1e-09        6.958093575000038         6.958093484988951         3.8670010528196416e-08   
1e-10        6.958093575000038         6.958106201732627         1.8151411885996984e-06   
1e-11        6.958093575000038         6.958182593087883         1.3892622023185074e-05   
1e-12        6.958093575000038         6.959383728063811         0.0002311786621200542    
1e-13        6.958093575000038         6.960119103602342         0.0008703593293654989    
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          18.682534379041986        30.50212686740181         0.924035256772789        
0.1          18.682534379041986        37.99611788753263         2.6029622154253795       
0.01         18.682534379041986        397.95351040616976        21.954051174017923       
0.001        18.682534379041986        4011.486883880985         215.3781068255669        
0.0001       18.682534379041986        40147.90946104776         2149.614291287166        
1e-05        18.682534379041986        401512.24126447045        21491.97575101486        
1e-06        18.682534379041986        4015155.569873636         214915.59031033728       
1e-07        18.682534379041986        40151588.85702248         2149151.73589977         
1e-08        18.682534379041986        401515921.7286167         21491513.19179372        
1e-09        18.682534379041986        4015159250.444568         214915127.75073314       
1e-10        18.682534379041986        40151592537.6041          2149151273.3401284       
1e-11        18.682534379041986        401515925409.1994         21491512729.234077       
1e-12        18.682534379041986        4015159254125.153         214915127288.17358       
1e-13        18.682534379041986        40151592541284.69         2149151272877.5688       
F/var/folders/2m/27s1738j5wg9z16rldx9t320002b2c/T/tmp5bxyzmep
...[[ 8.97922828e-04 -1.00061864e+01  5.83690948e+01  1.44096120e-06
  -4.44986270e-07  3.14893907e-04 -2.66182675e-04]]
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          33.649893208999586        0.6723500286708922        1.0199807477692402       
0.1          33.649893208999586        nan                       nan                      
0.01         33.649893208999586        nan                       nan                      
0.001        33.649893208999586        38.07373449173576         0.13146672577115434      
0.0001       33.649893208999586        34.0295644095942          0.01128298381919003      
1e-05        33.649893208999586        33.6873602564125          0.0011134373348587606    
1e-06        33.649893208999586        33.65363501053875         0.00011119802122174244   
1e-07        33.649893208999586        33.650267340645804        1.111836058125096e-05    
1e-08        33.649893208999586        33.649930605839096        1.1113509120896293e-06   
1e-09        33.649893208999586        33.64989717424827         1.1783837339058155e-07   
1e-10        33.649893208999586        33.64989564769161         7.247250404359307e-08    
1e-11        33.649893208999586        33.64991618681756         6.8284965525762e-07      
1e-12        33.649893208999586        33.6498051645151          2.6164862972506333e-06   
1e-13        33.649893208999586        33.648639430339244        3.725951379858729e-05    
...Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          4.33536307654206          nan                       nan                      
0.1          4.33536307654206          1.4720221173433856        0.6604616288522047       
0.01         4.33536307654206          4.0664783376769265        0.06202127344766687      
0.001        4.33536307654206          4.308636696833901         0.006164738508931635     
0.0001       4.33536307654206          4.332692048478748         0.000616102507714863     
1e-05        4.33536307654206          4.335095994179028         6.160553529578788e-05    
1e-06        4.33536307654206          4.335336512895083         6.127202383787658e-06    
1e-07        4.33536307654206          4.33536058608297          5.744522536294082e-07    
1e-08        4.33536307654206          4.335367975727422         1.1300519185561231e-06   
1e-09        4.33536307654206          4.335475978223258         2.6042035973575426e-05   
1e-10        4.33536307654206          4.335731773608131         8.504410347230536e-05    
1e-11        4.33536307654206          4.3371528590796515        0.0004128333673541383    
1e-12        4.33536307654206          4.433786671143025         0.02270250331131878      
1e-13        4.33536307654206          4.547473508864641         0.048925644421865415     
.
pyapprox/expdesign/tests/test_bayesian_oed.py [0, 0]
-0.32732683535398854
0.3273268353539885 -0.32732683535398854
[0, 0]
-0.45819524713356574
[0.45819525] -0.45819524713356574
[0, 0]
-0.38107035535480355
0.38050464880103124 -0.38107035535480355
[0, 0]
-0.5865351396541667
0.585664416524284 -0.5865351396541667
[0, 10]
1.3540251005511055
1.3513351836159044 1.3540251005511055
[0, 0]
0.9729550745276567
0.9723770982732802 0.9729550745276567
[0, 0]
0.9729550745276567
0.97237709827328 0.9729550745276567
.Running 31 model evaluations
[0.34657359 0.24734812 0.15374235 0.07421    0.01961036 0.01961036
 0.07421    0.15374235 0.24734812 0.34657359]
[0.35136651 0.2501308  0.1515702  0.07286172 0.01934518 0.01989419
 0.07481736 0.15472562 0.23954818 0.34997783]
0.031534273102622135
[0.48538946 0.42933081 0.38505411 0.3564749  0.3564749  0.38505411
 0.42933081 0.48538946 0.54930614]
[0.492184   0.42956331 0.38916977 0.36163999 0.36022608 0.39178499
 0.4288238  0.48591864 0.55327984]
0.017480355368971377
[0.64599184 0.60597049 0.57528601 0.55592876 0.55592876 0.57528601
 0.60597049 0.64599184]
[0.65285381 0.60907574 0.58052287 0.5608762  0.55919905 0.58072671
 0.60432519 0.64498572]
0.010622369962891863
.nouter_loop_samples * ninner_loop_samples:  137842
Running 137842 model evaluations
..Running 3000000 model evaluations
...Running 906010 model evaluations
Running 906010 model evaluations
Running 102010 model evaluations
.[[-0.00056912  0.00056912 -0.00056912]
 [-0.00056912  0.00056912 -0.00056912]
 [-0.00056912  0.00056912 -0.00056912]]
[[-0.00018273  0.00018273 -0.00018273]
 [ 0.00018273 -0.00018273  0.00018273]
 [-0.00018273  0.00018273 -0.00018273]]
.nouter_loop_samples * ninner_loop_samples:  1800
Running 900 model evaluations
[0.14002801 0.0995086  0.14142136] [0.01414037 0.009999   0.01414178]
[0.14107087 0.09950372 0.14107087] [0.01414143 0.009999   0.01414143]
[0.14142136 0.0995086  0.14002801] [0.01414178 0.009999   0.01414037]
F.[[1.07116305]
 [1.69958177]] [[1.07116305]
 [1.69958177]]
.nouter_loop_samples * ninner_loop_samples:  189
Running 189 model evaluations
nouter_loop_samples * ninner_loop_samples:  3969
Running 3969 model evaluations
nouter_loop_samples * ninner_loop_samples:  2709
Running 2709 model evaluations
nouter_loop_samples * ninner_loop_samples:  90000
Running 90000 model evaluations
nouter_loop_samples * ninner_loop_samples:  90000
Running 90000 model evaluations
.Running 51 model evaluations
1 [-8.32667268e-17 -1.38777878e-16  1.66533454e-16 ...  1.66533454e-16
 -1.66533454e-16  2.77555756e-17]
2 [-9.04304409e-13  1.47082346e-12 -1.27120536e-13 ...  1.19865229e-12
  1.60413349e-12 -1.03606013e-12]
3 [ 1.86753973e-10 -2.53276317e-10 -1.03529893e-10 ... -2.39364778e-10
 -2.14579299e-10  2.36829389e-10]
4 [ 7.72857028e-09  1.78106357e-09 -3.14854499e-09 ...  1.45038925e-10
  6.71233835e-09  1.60240924e-08]
nouter_loop_samples * ninner_loop_samples:  510000
Running 51 model evaluations
1 [ 4.16333634e-17  4.33680869e-18 -1.38777878e-16 ...  6.93889390e-17
 -5.55111512e-17 -2.22044605e-16]
2 [-2.95583003e-13 -2.25930386e-12 -4.44033699e-13 ... -1.81445137e-13
  1.79384285e-13 -1.23434596e-12]
3 [ 1.28882433e-10  3.75456555e-10  1.77803855e-10 ...  9.28842836e-11
  3.59006158e-11 -3.11873277e-10]
4 [-1.59151644e-08  9.55334586e-09 -2.39043819e-11 ...  8.59974705e-09
  1.28390309e-09 -1.58830657e-09]
.Running 310 model evaluations
Running 31 model evaluations
[0.42126092 0.23823041 0.09065694 0.02152239 0.0670063  0.31795678] [0.42126092 0.23823041 0.09065694 0.02152239 0.0670063  0.31795678]
[ 0.16875446  0.17819015  0.05871689 -0.02065554  0.03396466  0.31334358] [ 0.16875446  0.17819015  0.05871689 -0.02065554  0.03396466  0.31334358]
[ 0.21150456  0.02278688  0.00978856 -0.01070029  0.02671226  0.15661335] [ 0.21150456  0.02278688  0.00978856 -0.01070029  0.02671226  0.15661335]
[ 0.19316263 -0.00836801 -0.00034855 -0.01018648  0.01974748  0.09922177] [ 0.19316263 -0.00836801 -0.00034855 -0.01018648  0.01974748  0.09922177]
Running 310 model evaluations
Running 31 model evaluations
[0.18741246 0.29030835 0.06681197 0.09622277 0.1638003  0.39342023] [0.18741246 0.29030835 0.06681197 0.09622277 0.1638003  0.39342023]
[0.232578   0.21180944 0.01934096 0.05542908 0.09952819 0.25562474] [0.232578   0.21180944 0.01934096 0.05542908 0.09952819 0.25562474]
[0.18076462 0.19439923 0.02063939 0.03201992 0.09890375 0.19819639] [0.18076462 0.19439923 0.02063939 0.03201992 0.09890375 0.19819639]
[0.16778061 0.13973267 0.02545133 0.02675451 0.08065897 0.12382014] [0.16778061 0.13973267 0.02545133 0.02675451 0.08065897 0.12382014]
.
pyapprox/expdesign/tests/test_linear_oed.py .Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          19499.547700902138        17239.72248162958         0.11589116085846492      
0.1          19499.547700902138        19252.776649257867        0.01265521926095006      
0.01         19499.547700902138        19474.635640037013        0.0012775712158683615    
0.001        19499.547700902138        19497.115971695166        0.00012470695445204055   
0.0001       19499.547700902138        19499.51891670935         1.4761466896991251e-06   
1e-05        19499.547700902138        19518.953119404614        0.0009951727496519476    
1e-06        19499.547700902138        19567.030714824796        0.003460747652087119     
1e-07        19499.547700902138        20721.75731882453         0.06267887012916955      
1e-08        19499.547700902138        31845.33561579883         0.6331320143556725       
1e-09        19499.547700902138        106213.90538290143        4.446993284771804        
1e-10        19499.547700902138        1890653.5115092993        95.95883927717098        
1e-11        19499.547700902138        8285016.519948841         423.88249712405064       
1e-12        19499.547700902138        12366101.145744324        633.1737426644112        
1e-13        19499.547700902138        795475789.3458009         40793.57644594488        
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1830.248801470794         70.44829875294818         1.0384911049778236       
0.1          1830.248801470794         1652.3550935153617        0.09719646193046341      
0.01         1830.248801470794         1812.5472845102195        0.009671645159034941     
0.001        1830.248801470794         1828.4397008246742        0.0009884452019124177    
0.0001       1830.248801470794         1828.8485839730129        0.0007650421607466123    
1e-05        1830.248801470794         1827.9448093380777        0.001258840946031385     
1e-06        1830.248801470794         1816.2591150030494        0.007643598212714275     
1e-07        1830.248801470794         1266.2244262173772        0.3081681434787254       
1e-08        1830.248801470794         3061.7869924753904        0.6728802062402272       
1e-09        1830.248801470794         103828.8464769721         57.72936181875104        
1e-10        1830.248801470794         340634.7241252661         187.11389000857747       
1e-11        1830.248801470794         3059301.5253543854        1672.5222121145061       
1e-12        1830.248801470794         4215457.011014223         2304.2152828766602       
1e-13        1830.248801470794         190029386.43097878        103828.07874375906       
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.0112665269477734        1.2292824679626495        0.2155870239993966       
0.1          1.0112665269477734        1.0298616790403603        0.01838798338229505      
0.01         1.0112665269477734        1.0130994245823643        0.0018124773101340819    
0.001        1.0112665269477734        1.0114496032347375        0.00018103663286145563   
0.0001       1.0112665269477734        1.0112848653509587        1.8134094916277372e-05   
1e-05        1.0112665269477734        1.0112547144558448        1.1680888879276577e-05   
1e-06        1.0112665269477734        1.0111927286615696        7.297609901767072e-05    
1e-07        1.0112665269477734        1.0103027570096401        0.0009530325709901304    
1e-08        1.0112665269477734        1.0020207774630308        0.00914274252965574      
1e-09        1.0112665269477734        0.9407887802126423        0.06969255370080185      
1e-10        1.0112665269477734        0.5560085725164754        1.5498140773972144       
1e-11        1.0112665269477734        0.49071857688431925       1.48525147803064         
1e-12        1.0112665269477734        25.04574325712383         23.76670846875298        
1e-13        1.0112665269477734        1023.4302294520603        1013.0281866156489       
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.09878588101021146       0.10480085972713526       0.060889052721026356     
0.1          0.09878588101021146       0.0993574834159272        0.005786276336966179     
0.01         0.09878588101021146       0.09884277202596436       0.0005759022966755901    
0.001        0.09878588101021146       0.09879161202164255       5.801447911867796e-05    
0.0001       0.09878588101021146       0.0987867735435799        9.035029695618425e-06    
1e-05        0.09878588101021146       0.09878608584479308       2.0735208262669082e-06   
1e-06        0.09878588101021146       0.09881443618908747       0.0002890613373489987    
1e-07        0.09878588101021146       0.09920464449919564       0.004239102639990503     
1e-08        0.09878588101021146       0.09891105712256376       0.0012671457810793764    
1e-09        0.09878588101021146       0.18144508118211888       0.8367511564062781       
1e-10        0.09878588101021146       0.09245049170658604       0.06413253836315476      
1e-11        0.09878588101021146       0.021804780203638074      1.220727698944995        
1e-12        0.09878588101021146       22.443824576612315        226.1966838489025        
1e-13        0.09878588101021146       91.90426197847046         929.3380304820113        
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.38203257879748137       0.05701535447043682       1.1492421265482207       
0.1          0.38203257879748137       0.33712476432732785       0.11754969854013293      
0.01         0.38203257879748137       0.3774732803918823        0.011934318324238953     
0.001        0.38203257879748137       0.3815759314278466        0.0011953100206064271    
0.0001       0.38203257879748137       0.3819846618569045        0.0001254263202570389    
1e-05        0.38203257879748137       0.3820152997491277        4.5229253505238e-05      
1e-06        0.38203257879748137       0.3817463465338733        0.0007492352210092827    
1e-07        0.38203257879748137       0.3812436943917419        0.0020649663131417947    
1e-08        0.38203257879748137       0.3800870729264716        0.005092512992304531     
1e-09        0.38203257879748137       0.7242988431244157        0.8959085777560676       
1e-10        0.38203257879748137       3.4015101846307516        7.903717571254362        
1e-11        0.38203257879748137       15.970158528944014        40.80313254752529        
1e-12        0.38203257879748137       469.83927859400865        1230.84086873668         
1e-13        0.38203257879748137       2308.5533484845655        6043.8180124092205       
Eps          norm(jv)                  norm(jv_fd)               Abs. Errors              
1.0          0.20938772544090464       0.136576291693288         0.07281143374761664      
0.1          0.20938772544090464       0.20204523924583384       0.007342486195070802     
0.01         0.20938772544090464       0.208651892692302         0.0007358327486026439    
0.001        0.20938772544090464       0.20931380433353297       7.392110737167168e-05    
0.0001       0.20938772544090464       0.20937898810302613       8.737337878511608e-06    
1e-05        0.20938772544090464       0.20944857865856645       6.085321766180485e-05    
1e-06        0.20938772544090464       0.20976545300754879       0.00037772756664414264   
1e-07        0.20938772544090464       0.2188405545666683        0.009452829125763645     
1e-08        0.20938772544090464       0.27946072123086196       0.07007299578995732      
1e-09        0.20938772544090464       0.22748025685359605       0.018092531412691404     
1e-10        0.20938772544090464       0.8591172218075371        0.6497294963666325       
1e-11        0.20938772544090464       0.3780087354243733        0.587396460865278        
1e-12        0.20938772544090464       301.5685479113017         301.35916018586084       
1e-13        0.20938772544090464       4714.308943221113         4714.099555495672        
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.0156365157318152        1.27058057475866          0.26530816794129475      
0.1          1.0156365157318152        1.0358015399890306        0.02103600597816124      
0.01         1.0156365157318152        1.0176121396823088        0.0020617058749382655    
0.001        1.0156365157318152        1.0158336030655213        0.00020568415707556214   
0.0001       1.0156365157318152        1.0156561094570198        2.031535692405947e-05    
1e-05        1.0156365157318152        1.015641074090865         1.2360165481593003e-05   
1e-06        1.0156365157318152        1.0155641444235475        0.00020207474351800718   
1e-07        1.0156365157318152        1.015702401726644         0.000614493248050921     
1e-08        1.0156365157318152        1.0162507482394054        0.007509873929912011     
1e-09        1.0156365157318152        1.008080191480413         0.06484696973090262      
1e-10        1.0156365157318152        2.1894216149026264        1.8138056229154442       
1e-11        1.0156365157318152        5.943986474864438         5.703307462950092        
1e-12        1.0156365157318152        124.28820989041166        122.74968334644056       
1e-13        1.0156365157318152        1321.0046358282568        1300.9665933542453       
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.3601767544622638        1.7712708916973214        0.30368066428555146      
0.1          1.3601767544622638        1.392370805912743         0.023797113267101863     
0.01         1.3601767544622638        1.3633278475163788        0.002329356193876325     
0.001        1.3601767544622638        1.3604912011640211        0.0002324445560414897    
0.0001       1.3601767544622638        1.3602117644472034        2.5883418501745312e-05   
1e-05        1.3601767544622638        1.3602074736311838        2.3247310222578745e-05   
1e-06        1.3601767544622638        1.360139396884167         5.3157453642964284e-05   
1e-07        1.3601767544622638        1.358797490700305         0.00111752513253302      
1e-08        1.3601767544622638        1.357247912882296         0.0069425008453321965    
1e-09        1.3601767544622638        1.2478101643685717        0.09664613082928074      
1e-10        1.3601767544622638        1.711232340614673         0.4778071555826667       
1e-11        1.3601767544622638        15.365897903340315        10.35979132389378        
1e-12        1.3601767544622638        201.5098170452852         147.1939999068713        
1e-13        1.3601767544622638        2661.3761906283257        1955.7107257868777       
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.031004815496706074      0.04345099868270719       0.40142742301831746      
0.1          0.031004815496706074      0.03202976267915325       0.03305767720359654      
0.01         0.031004815496706074      0.031105637492068183      0.0032518172982780977    
0.001        0.031004815496706074      0.03101486122369934       0.0003240053789171915    
0.0001       0.031004815496706074      0.03100575612702272       3.033820074650623e-05    
1e-05        0.031004815496706074      0.031005501144321098      2.211422980721233e-05    
1e-06        0.031004815496706074      0.030985514154480143      0.0006225272402598728    
1e-07        0.031004815496706074      0.03096991552098416       0.0011256308145301546    
1e-08        0.031004815496706074      0.031079050444304812      0.0023943038011829203    
1e-09        0.031004815496706074      0.029260593947810772      0.056256472452822885     
1e-10        0.031004815496706074      0.06377565142656749       1.0569595530521048       
1e-11        0.031004815496706074      0.3369748924342275        9.868469527581206        
1e-12        0.031004815496706074      11.12887559884257         359.940229785431         
1e-13        0.031004815496706074      126.2101534393878         4071.662941142038        
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.04490498007915709       0.0562745177791939        0.2531910197932378       
0.1          0.04490498007915709       0.04582396234924069       0.020465041259647525     
0.01         0.04490498007915709       0.04499519319343115       0.0020089779377484704    
0.001        0.04490498007915709       0.04491398862005447       0.00020061340371380453   
0.0001       0.04490498007915709       0.04490599629702885       2.2630404689423274e-05   
1e-05        0.04490498007915709       0.044906489882201577      3.362217379514131e-05    
1e-06        0.04490498007915709       0.04490501370746358       7.488769939177595e-07    
1e-07        0.04490498007915709       0.044861838688348143      0.0009607261985841205    
1e-08        0.04490498007915709       0.04528859509633776       0.008542816776768321     
1e-09        0.04490498007915709       0.037650993434112934      0.1615408053239764       
1e-10        0.04490498007915709       0.09671374812114664       1.1537421450953254       
1e-11        0.04490498007915709       0.5662581514798148        11.610141469423498       
1e-12        0.04490498007915709       9.72244507124742          215.5115106188445        
1e-13        0.04490498007915709       93.30314298949816         2076.7905440560558       
..Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.0666410086627589        0.09130735962453396       0.37013771935236905      
0.1          0.0666410086627589        0.06864207208358453       0.030027508001148832     
0.01         0.0666410086627589        0.06683778789091654       0.002952824876247801     
0.001        0.0666410086627589        0.06666062161375663       0.00029430753512420244   
0.0001       0.0666410086627589        0.06664287627300425       2.8024939640306903e-05   
1e-05        0.0666410086627589        0.06664210991935704       1.652520903019281e-05    
1e-06        0.0666410086627589        0.06660972928784759       0.0004693712706182264    
1e-07        0.0666410086627589        0.06660596740815095       0.0005258211919523573    
1e-08        0.0666410086627589        0.0668615607324341        0.0033095547936753617    
1e-09        0.0666410086627589        0.0629400975782346        0.0555350400419808       
1e-10        0.0666410086627589        0.12420953154901326       0.8638603172647574       
1e-11        0.0666410086627589        0.613487038947369         8.205848639716116        
1e-12        0.0666410086627589        17.985168909717686        270.8814029171255        
1e-13        0.0666410086627589        194.57768729580494        2920.7890488194407       
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.19221353712948208       0.21713337951972678       0.1296466563302343       
0.1          0.19221353712948208       0.1944687703691672        0.011732957383568257     
0.01         0.19221353712948208       0.19244549061196992       0.001206748941577315     
0.001        0.19221353712948208       0.19223683164470984       0.00012119081504683173   
0.0001       0.19221353712948208       0.1922152971745028        9.156717299994616e-06    
1e-05        0.19221353712948208       0.19221420368698003       3.467796846680762e-06    
1e-06        0.19221353712948208       0.1921588619335779        0.00028445028753284205   
1e-07        0.19221353712948208       0.19188760580490793       0.0016956731010812603    
1e-08        0.19221353712948208       0.1942584981762252        0.01063900637427821      
1e-09        0.19221353712948208       0.18164336701431694       0.05499180896944158      
1e-10        0.19221353712948208       0.21547652551134888       0.12102679514292482      
1e-11        0.19221353712948208       0.5903055821931957        4.071092655641336        
1e-12        0.19221353712948208       4.46931380793103          24.251816051438336       
1e-13        0.19221353712948208       232.44739466576902        1210.3185429972289       
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          9494.207859489146         8577.123539047956         0.09659408494249418      
0.1          9494.207859489146         9393.213940631249         0.010637424454211553     
0.01         9494.207859489146         9484.001189604169         0.001075041755566408     
0.001        9494.207859489146         9493.18948938162          0.0001072622511111204    
0.0001       9494.207859489146         9494.379659299739         1.8095223228195794e-05   
1e-05        9494.207859489146         9496.453707106411         0.0002365492361767126    
1e-06        9494.207859489146         9495.039761532098         8.762205918214e-05       
1e-07        9494.207859489146         9756.454674061388         0.027621768814565652     
1e-08        9494.207859489146         10423.501953482628        0.09788010835097558      
1e-09        9494.207859489146         24127.773940563202        1.5413151152413722       
1e-10        9494.207859489146         372808.42661857605        38.26693328564177        
1e-11        9494.207859489146         1733010.8676105738        181.5334870753316        
1e-12        9494.207859489146         7545779.226347804         793.7771249610938        
1e-13        9494.207859489146         229753786.69798374        24198.363453829636       
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.25487639254476224       0.3104161841163986        0.21790873221764653      
0.1          0.25487639254476224       0.2596198036711428        0.018610633487946494     
0.01         0.25487639254476224       0.25534400889313247       0.0018346789347628626    
0.001        0.25487639254476224       0.2549231035260391        0.00018326915572865142   
0.0001       0.25487639254476224       0.25488093676528933       1.7829115053461575e-05   
1e-05        0.25487639254476224       0.25487620973763114       7.172383808253679e-07    
1e-06        0.25487639254476224       0.25488314037147575       2.647489885639183e-05    
1e-07        0.25487639254476224       0.2547026412003106        0.0006817082693177198    
1e-08        0.25487639254476224       0.2544503274748422        0.0016716537207156926    
1e-09        0.25487639254476224       0.2378257590862631        0.06689765689266278      
1e-10        0.25487639254476224       0.07524647571699461       0.7047726744493232       
1e-11        0.25487639254476224       0.7133849067031406        3.7989446161744995       
1e-12        0.25487639254476224       8.519407401763601         34.42564337443451        
1e-13        0.25487639254476224       201.3678113144124         791.060661577543         
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.2681674620602684        0.11355949011010535       1.4234648351356802       
0.1          0.2681674620602684        0.23845572175929863       0.11079547113099174      
0.01         0.2681674620602684        0.26522872298611055       0.01095859673496635      
0.001        0.2681674620602684        0.2678738808277181        0.0010947682850662903    
0.0001       0.2681674620602684        0.26813810690029527       0.00010946577838946468   
1e-05        0.2681674620602684        0.2681645268509669        1.0945434166217368e-05   
1e-06        0.2681674620602684        0.26816717513256094       1.069957202406285e-06    
1e-07        0.2681674620602684        0.26816747711322364       5.613266837071075e-08    
1e-08        0.2681674620602684        0.26816762144221684       5.943373860214808e-07    
1e-09        0.2681674620602684        0.2681737054643918        2.3281736253146248e-05   
1e-10        0.2681674620602684        0.26822766230338857       0.00022448749992874182   
1e-11        0.2681674620602684        0.2687183808802729        0.0020543835399413598    
1e-12        0.2681674620602684        0.27422508708241367       0.022588963536463048     
1e-13        0.2681674620602684        0.3552713678800501        0.32481161267881853      
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          51.39093771453286         39.1648004765995          1.6073696741924017       
0.1          51.39093771453286         30.63499750716944         0.8256885739152816       
0.01         51.39093771453286         22.767409802116486        0.8589639766445781       
0.001        51.39093771453286         22.235266033642723        0.8621724668364689       
0.0001       51.39093771453286         22.183728267244096        0.8624889027453781       
1e-05        51.39093771453286         22.178590689757975        0.8625205013547975       
1e-06        51.39093771453286         22.178077098517047        0.8625236606752418       
1e-07        51.39093771453286         22.17802580959829         0.8625239764619865       
1e-08        51.39093771453286         22.17802043820224         0.8625240001750727       
1e-09        51.39093771453286         22.178024362811204        0.8625239696925944       
1e-10        51.39093771453286         22.17810541803869         0.8625248010704849       
1e-11        51.39093771453286         22.178166122917332        0.86251859957873         
1e-12        51.39093771453286         22.179800905201155        0.8624683816166605       
1e-13        51.39093771453286         22.19475990927598         0.862307394086916        
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          37.0705690442081          9.047213551664264         1.1906036801164945       
0.1          37.0705690442081          74.94518094570188         1.0590670270251406       
0.01         37.0705690442081          38.85534016846567         0.05324601123924959      
0.001        37.0705690442081          37.23921383525257         0.005076564914514263     
0.0001       37.0705690442081          37.08733995710092         0.0005053087758232101    
1e-05        37.0705690442081          37.07224520460121         5.05075316571005e-05     
1e-06        37.0705690442081          37.07073665912246         5.050573648022185e-06    
1e-07        37.0705690442081          37.07058586627175         5.056892559577501e-07    
1e-08        37.0705690442081          37.07057064352134         6.711649636587518e-08    
1e-09        37.0705690442081          37.07057459567819         2.8203160094344507e-07   
1e-10        37.0705690442081          37.07054572140724         3.7519237754658368e-06   
1e-11        37.0705690442081          37.070474588652225        4.1655018311031355e-05   
1e-12        37.0705690442081          37.07655145343824         0.0003789593313415342    
1e-13        37.0705690442081          37.07157376953406         0.0024426681657526523    
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.011360987566300681      0.00954007544339247       0.16027762659554856      
0.1          0.011360987566300681      0.011174129810349043      0.016447316297211856     
0.01         0.011360987566300681      0.011342221344518011      0.0016518125447417295    
0.001        0.011360987566300681      0.011359111152353663      0.00016516292585201973   
0.0001       0.011360987566300681      0.011360844592023245      1.2584669827511344e-05   
1e-05        0.011360987566300681      0.01136127152912003       2.4994554187444184e-05   
1e-06        0.011360987566300681      0.011361002305587675      1.297359662443387e-06    
1e-07        0.011360987566300681      0.011400324018140395      0.0034624148305905343    
1e-08        0.011360987566300681      0.011474554639789858      0.0099962325305278       
1e-09        0.011360987566300681      0.013629763984113197      0.19969887340975812      
1e-10        0.011360987566300681      0.06484313086474458       4.707525907086134        
1e-11        0.011360987566300681      0.3304606588372394        28.087318061808485       
1e-12        0.011360987566300681      0.864114335641375         75.05979062986901        
1e-13        0.011360987566300681      34.90263633665336         3071.14809742268         
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.0113609875622313        0.00954007543964508       0.16027762662461648      
0.1          0.0113609875622313        0.01117412976441301       0.016447319988227353     
0.01         0.0113609875622313        0.011342220841470407      0.0016518564656544767    
0.001        0.0113609875622313        0.011359114182596386      0.00016489584419061802   
0.0001       0.0113609875622313        0.011360793905623723      1.7045754738801106e-05   
1e-05        0.0113609875622313        0.011361032647982936      3.9684711729795684e-06   
1e-06        0.0113609875622313        0.011356291906849947      0.00041331401479240175   
1e-07        0.0113609875622313        0.011400516086723655      0.00347932116603716      
1e-08        0.0113609875622313        0.011220890883123502      0.012331382139131755     
1e-09        0.0113609875622313        0.007617018127348273      0.3295461256668879       
1e-10        0.0113609875622313        0.05115907697472721       3.5030484097000074       
1e-11        0.0113609875622313        0.07958078640513122       8.004741970644456        
1e-12        0.0113609875622313        5.229594535194337         461.31161521377845       
1e-13        0.0113609875622313        7.958078640513122         701.4741970644455        
.[-1.        -0.4472136  0.4472136  1.       ]
[-8.04079026e-13 -1.37667655e-14  2.24542607e-13  5.89334137e-13]
.F.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.014918116333281996      0.022725627870304133      0.5233577324775076       
0.1          0.014918116333281996      0.015544843994655944      0.04201117938567965      
0.01         0.014918116333281996      0.014979785868340922      0.004133868759378346     
0.001        0.014918116333281996      0.014924273071259542      0.0004127021025979037    
0.0001       0.014918116333281996      0.014918749933201525      4.2471844660127805e-05   
1e-05        0.014918116333281996      0.014918571328292883      3.0499494756688998e-05   
1e-06        0.014918116333281996      0.014920686353114831      0.0001722750899255101    
1e-07        0.014918116333281996      0.01495759682779152       0.0026464798656545665    
1e-08        0.014918116333281996      0.01514920411338494       0.015490412793429678     
1e-09        0.014918116333281996      0.01503297486493693       0.007699265047202198     
1e-10        0.014918116333281996      0.09539369294486733       5.3944864628817815       
1e-11        0.014918116333281996      0.532018873400375         34.66260387804138        
1e-12        0.014918116333281996      0.13566925360919413       8.094261673407047        
1e-13        0.014918116333281996      28.561042419994465        1913.5206929560802       
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.13641442708740936       0.14987808166708383       0.0986967058187141       
0.1          0.13641442708740936       0.13966286893466573       0.023813037349597083     
0.01         0.13641442708740936       0.13683117887641205       0.003055041888902632     
0.001        0.13641442708740936       0.13645725504779493       0.00031395477223336      
0.0001       0.13641442708740936       0.13641866603464337       3.1074039047842326e-05   
1e-05        0.13641442708740936       0.13641510082651465       4.938913864673997e-06    
1e-06        0.13641442708740936       0.13640666046699224       5.693401044854412e-05    
1e-07        0.13641442708740936       0.13643950691477613       0.00018385025618074385   
1e-08        0.13641442708740936       0.13729243297788685       0.006436312560363506     
1e-09        0.13641442708740936       0.13573342450001746       0.004992159567957849     
1e-10        0.13641442708740936       0.2011824040693           0.47478832235530105      
1e-11        0.13641442708740936       0.4698852418272282        2.4445421342871816       
1e-12        0.13641442708740936       2.7448598949320058        19.12147801033684        
1e-13        0.13641442708740936       7.4112938008852325        55.329252111555235       
..Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.07954632027512501       0.07954632027512343       1.988863601256459e-14    
0.1          0.07954632027512501       0.07954632027511899       7.571638622327221e-14    
0.01         0.07954632027512501       0.07954632027500352       1.5272378917016704e-12   
0.001        0.07954632027512501       0.07954632027384889       1.604245294648565e-11    
0.0001       0.07954632027512501       0.07954632025075625       3.063467540421653e-10    
1e-05        0.07954632027512501       0.07954632001982986       3.2093897649989614e-09   
1e-06        0.07954632027512501       0.0795463179770195        2.8890154861924467e-08   
1e-07        0.07954632027512501       0.07954629843709426       2.74532255789038e-07     
1e-08        0.07954632027512501       0.07954605862892095       3.2892307671672497e-06   
1e-09        0.07954632027512501       0.07954437108992352       2.4503775847236146e-05   
1e-10        0.07954632027512501       0.07952749569994921       0.0002366492266479251    
1e-11        0.07954632027512501       0.07931433287922118       0.002916381236761891     
1e-12        0.07954632027512501       0.0772715225139109        0.028597146333687398     
1e-13        0.07954632027512501       0.062172489379008766      0.2184114970500933       
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          296.21865441452894        78.7241271385859          0.9752176207353341       
0.1          296.21865441452894        845.6166900841102         2.8039120798159045       
0.01         296.21865441452894        8440.31399884349          28.284787373157197       
0.001        296.21865441452894        84363.12362957816         284.5763093544474        
0.0001       296.21865441452894        843588.9645572165         2847.633648082442        
1e-05        296.21865441452894        8435847.148307571         28478.221141980357       
1e-06        296.21865441452894        84358428.9632866          284784.09749063104       
1e-07        296.21865441452894        843584247.1112691         2847842.8611195954       
1e-08        296.21865441452894        8435842428.603635         28478430.497466426       
1e-09        296.21865441452894        84358424243.37025         284784306.86040616       
1e-10        296.21865441452894        843584242391.0609         2847843070.489886        
1e-11        296.21865441452894        8435842423892.908         28478430706.86888        
1e-12        296.21865441452894        84358424238652.45         284784307069.7847        
1e-13        296.21865441452894        843584242389781.1         2847843070710.87         
F..........
pyapprox/expdesign/tests/test_low_discrepancy_sequences.py .....
pyapprox/interface/tests/test_async_model.py .....
pyapprox/interface/tests/test_datafunction_model.py ...
pyapprox/interface/tests/test_wrappers.py .Evaluating all 3 samples took 1.6261940002441406 seconds
Evaluating all 3 samples took 1.4352679252624512 seconds
Evaluating all 3 samples took 1.4166581630706787 seconds
Evaluating all 3 samples took 1.4310977458953857 seconds
Evaluating all 3 samples took 1.4150190353393555 seconds
Evaluating all 3 samples took 1.4512147903442383 seconds
Evaluating all 3 samples took 1.4236512184143066 seconds
Evaluating all 3 samples took 1.4854729175567627 seconds
Evaluating all 3 samples took 1.502338171005249 seconds
Evaluating all 3 samples took 1.4450650215148926 seconds
Evaluating all 3 samples took 1.448354959487915 seconds
Evaluating all 3 samples took 1.6255989074707031 seconds
Evaluating all 3 samples took 1.448180913925171 seconds
Evaluating all 3 samples took 1.4645819664001465 seconds
Evaluating all 3 samples took 1.4285471439361572 seconds
Evaluating all 3 samples took 1.4481260776519775 seconds
Evaluating all 3 samples took 1.4253082275390625 seconds
Evaluating all 3 samples took 1.4648208618164062 seconds
Evaluating all 3 samples took 1.4247798919677734 seconds
Evaluating all 3 samples took 1.4202659130096436 seconds
Evaluating all 3 samples took 1.4638910293579102 seconds
Evaluating all 3 samples took 1.5048022270202637 seconds
Evaluating all 3 samples took 1.398648738861084 seconds
Evaluating all 3 samples took 1.4356200695037842 seconds
Evaluating all 3 samples took 1.452420949935913 seconds
Evaluating all 3 samples took 1.4494047164916992 seconds
Evaluating all 3 samples took 1.4023079872131348 seconds
Evaluating all 3 samples took 1.3838677406311035 seconds
Evaluating all 3 samples took 1.3765978813171387 seconds
Evaluating all 3 samples took 1.3831090927124023 seconds
Evaluating all 3 samples took 1.3897528648376465 seconds
Evaluating all 3 samples took 1.3889870643615723 seconds
Evaluating all 3 samples took 1.3784408569335938 seconds
Evaluating all 3 samples took 1.3856401443481445 seconds
[115 178 145 179 142   6  72  26   1 189  33  56  83  34  40  36 108 144
  47  86 147  10 157  11  43  80   4 143 155 168  74 173 164 184 193 119
  68   9  70  16 154 105  31 175 102 141 192 195  20  19 182 121  49   8
  27 162  22 200 146 183  67  45  51 138  58  88  98 198 111  69  55 117
  12 170 180  65 148 136  50 104 149  35  54 202  97  81  95   0  64  84
 107   3 131 109  66 126  75  46 160   7  63  41]
.Evaluating all 102 samples took 2.197813034057617 seconds
.
pyapprox/multifidelity/tests/test_control_variate_monte_carlo.py [3.82842712 7.72740661]
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.039840429941132974      0.04171372654056313       0.04701998954825743      
0.1          0.039840429941132974      0.04001820426855651       0.004462158859385109     
0.01         0.039840429941132974      0.03985811728371147       0.00044395461104787217   
0.001        0.039840429941132974      0.039842197779638155      4.437297759571875e-05    
0.0001       0.039840429941132974      0.039840606715635385      4.437063120863796e-06    
1e-05        0.039840429941132974      0.03984044761040639       4.435010727922304e-07    
1e-06        0.039840429941132974      0.03984043156490813       4.075696869738445e-08    
1e-07        0.039840429941132974      0.0398404301216182        4.530202717567831e-09    
1e-08        0.039840429941132974      0.03984041707649766       3.229040282538516e-07    
1e-09        0.039840429941132974      0.039840408749824974      5.319046012143322e-07    
1e-10        0.039840429941132974      0.039839520571405274      2.2825299050332253e-05   
1e-11        0.039840429941132974      0.03984035323867374       1.9252417542842023e-06   
1e-12        0.039840429941132974      0.03980149543281186       0.0009772612489031933    
1e-13        0.039840429941132974      0.03941291737419306       0.010730621320392283     
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.003039154569822294      0.0010866487175717476     0.6424503286664731       
0.1          0.003039154569822294      0.0028724053512818593     0.054866975242455325     
0.01         0.003039154569822294      0.0030227250055747668     0.005405965333473625     
0.001        0.003039154569822294      0.003037514032744326      0.0005398004742035098    
0.0001       0.003039154569822294      0.0030389905397720796     5.397226315608496e-05    
1e-05        0.003039154569822294      0.003039138157800991      5.400193022804361e-06    
1e-06        0.003039154569822294      0.0030391528182960315     5.763202306198875e-07    
1e-07        0.003039154569822294      0.003039153928519056      2.1101369581929862e-07   
1e-08        0.003039154569822294      0.003039152263184519      7.589734980201819e-07    
1e-09        0.003039154569822294      0.0030391800187601348     8.37368987199454e-06     
1e-10        0.003039154569822294      0.003039235529911366      2.6639016612023982e-05   
1e-11        0.003039154569822294      0.003036459972349803      0.0008866273203894482    
1e-12        0.003039154569822294      0.0029976021664879227     0.013672356038410058     
1e-13        0.003039154569822294      0.003885780586188048      0.27857287180206103      
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          32.07593619101767         29.93754357992036         0.06666656893076532      
0.1          32.07593619101767         31.84844763665673         0.007092187520457875     
0.01         32.07593619101767         32.05304119755361         0.0007137747539997954    
0.001        32.07593619101767         32.073645219952596        7.142335772926884e-05    
0.0001       32.07593619101767         32.075707079251           7.142792818267348e-06    
1e-05        32.07593619101767         32.07591327978321         7.142810835953302e-07    
1e-06        32.07593619101767         32.07593390186503         7.136666647099825e-08    
1e-07        32.07593619101767         32.07593593401725         8.012249871212251e-09    
1e-08        32.07593619101767         32.07593621823435         8.485076951913865e-10    
1e-09        32.07593619101767         32.075934797148875        4.34552801368268e-08     
1e-10        32.07593619101767         32.07595966614463         7.318610069234915e-07    
1e-11        32.07593619101767         32.07567544905032         8.128896559480146e-06    
1e-12        32.07593619101767         32.07745180588972         4.725083823054259e-05    
1e-13        32.07593619101767         32.08100451956852         0.00015801030781058807   
0.06666656893076532 8.485076951913865e-10
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          13.92383788592834         13.89722068570353         0.0019116281331965845    
0.1          13.92383788592834         13.92117157861648         0.00019149226913606505   
0.01         13.92383788592834         13.923571209237195        1.9152527724657975e-05   
0.001        13.92383788592834         13.923811217800619        1.9152857092147722e-06   
0.0001       13.92383788592834         13.923835219102898        1.9152948082378675e-07   
1e-05        13.92383788592834         13.923837619245203        1.915299059092436e-08    
1e-06        13.92383788592834         13.923837855145393        2.210808989749964e-09    
1e-07        13.92383788592834         13.923837798301975        6.293262356881946e-09    
1e-08        13.92383788592834         13.923838793061805        6.514967156792775e-08    
1e-09        13.92383788592834         13.923838082519069        1.4119004478777967e-08   
1e-10        13.92383788592834         13.9238665042285          2.0553456880447693e-06   
1e-11        13.92383788592834         13.923795449954923        3.047721020870209e-06    
1e-12        13.92383788592834         13.912426766182762        0.0008195383944472667    
1e-13        13.92383788592834         13.784529073745944        0.010005058470494228     
0.010005058470494228 2.210808989749964e-09
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          2.023469100049793         1.8885713578120509        0.06666656893076472      
0.1          2.023469100049793         2.00911827775041          0.007092187520446707     
0.01         2.023469100049793         2.0220247988909534        0.0007137747538639863    
0.001        2.023469100049793         2.023324577095309         7.142335629454619e-05    
0.0001       2.023469100049793         2.0234546468600456        7.142777592644059e-06    
1e-05        2.023469100049793         2.023467654765909         7.142604172446127e-07    
1e-06        2.023469100049793         2.0234689550591156        7.165450524477733e-08    
1e-07        2.023469100049793         2.023469107825804         3.842910645913715e-09    
1e-08        2.023469100049793         2.023469747314266         3.198786050720623e-07    
1e-09        2.023469100049793         2.023472944756577         1.9000570772028052e-06   
1e-10        2.023469100049793         2.0234480757608253        1.0390219928258528e-05   
1e-11        2.023469100049793         2.0236257114447653        7.739747296789384e-05    
1e-12        2.023469100049793         2.021494083237485         0.0009760548417859346    
1e-13        2.023469100049793         2.0250467969162855        0.0007796990161371129    
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.9807194459686688        1.8486716764914912        0.06666656893076532      
0.1          1.9807194459686688        1.9666718122324411        0.007092187520458082     
0.01         1.9807194459686688        1.9793056584333169        0.000713774754031618     
0.001        1.9807194459686688        1.9805779763353293        7.142335762261957e-05    
0.0001       1.9807194459686688        1.9807052981013484        7.142792155214876e-06    
1e-05        1.9807194459686688        1.980718031191486         7.142743943971695e-07    
1e-06        1.9807194459686688        1.9807193045728866        7.13860726119667e-08     
1e-07        1.9807194459686688        1.9807194329146682        6.590534855279167e-09    
1e-08        1.9807194459686688        1.9807193663012868        4.0221436805116986e-08   
1e-09        1.9807194459686688        1.9807187001674718        3.765304564155982e-07    
1e-10        1.9807194459686688        1.9807266937732493        3.6591777776770434e-06   
1e-11        1.9807194459686688        1.9807711026942345        2.607977907768103e-05    
1e-12        1.9807194459686688        1.9806378759312793        4.118202482210672e-05    
1e-13        1.9807194459686688        1.9895196601282805        0.00444293823515627      
0.06666656893076532 6.590534855279167e-09
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          2.107542822939598         2.1035139847873476        0.0019116281331976797    
0.1          2.107542822939598         2.10713924478215          0.00019149226912728434   
0.01         2.107542822939598         2.107502458167243         1.9152527728241103e-05   
0.001        2.107542822939598         2.107538786392915         1.9152857245643014e-06   
0.0001       2.107542822939598         2.1075424192851244        1.9152847999503712e-07   
1e-05        2.107542822939598         2.107542782603389         1.913897482912196e-08    
1e-06        2.107542822939598         2.107542818485797         2.1132670877075667e-09   
1e-07        2.107542822939598         2.1075428069394775        7.591836410439921e-09    
1e-08        2.107542822939598         2.1075427625305565        2.8663256882487442e-08   
1e-09        2.107542822939598         2.107541696716453         5.343773482116279e-07    
1e-10        2.107542822939598         2.107540808538033         9.558057576525784e-07    
1e-11        2.107542822939598         2.107469754264457         3.4670078512928606e-05   
1e-12        2.107542822939598         2.107647389948397         4.961560337526147e-05    
1e-13        2.107542822939598         2.1227464230832993        0.007213898563871418     
0.007213898563871418 2.1132670877075667e-09
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.4786704531777173        1.4546681451731471        0.01623235789488341      
0.1          1.4786704531777173        1.4762346372448576        0.0016473014170432824    
0.01         1.4786704531777173        1.4784265099210714        0.00016497472856217041   
0.001        1.4786704531777173        1.4786460552276282        1.64999239936767e-05     
0.0001       1.4786704531777173        1.4786680133482832        1.6500156805046804e-06   
1e-05        1.4786704531777173        1.4786702092095536        1.6499157276520266e-07   
1e-06        1.4786704531777173        1.4786704287672592        1.6508382950948942e-08   
1e-07        1.4786704531777173        1.4786704483071844        3.2938595989669386e-09   
1e-08        1.4786704531777173        1.478670341725774         7.537307788250514e-08    
1e-09        1.4786704531777173        1.4786696311830383        5.559011997727598e-07    
1e-10        1.4786704531777173        1.4786927238219505        1.5061262761660515e-05   
1e-11        1.4786704531777173        1.4788170688007085        9.915368409245508e-05    
1e-12        1.4786704531777173        1.4797052472204086        0.0006998138364552734    
1e-13        1.4786704531777173        1.4743761767022079        0.0029041470777216366    
0.01623235789488341 3.2938595989669386e-09
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.5175506736979139        1.5677050929383398        0.03304958451121204      
0.1          1.5175506736979139        1.5224212423778738        0.0032094932738499673    
0.01         1.5175506736979139        1.5180363277327302        0.00032002492123241557   
0.001        1.5175506736979139        1.5175992251172943        3.199327720775128e-05    
0.0001       1.5175506736979139        1.5175555287072484        3.199240340837148e-06    
1e-05        1.5175506736979139        1.5175511592246946        3.199410663480924e-07    
1e-06        1.5175506736979139        1.5175507215303696        3.1519511409423374e-08   
1e-07        1.5175506736979139        1.517550707319515         2.2155175205365363e-08   
1e-08        1.5175506736979139        1.517550529683831         9.48990273453598e-08     
1e-09        1.5175506736979139        1.5175487533269916        1.2654410528526115e-06   
1e-10        1.5175506736979139        1.517577175036422         1.7463231355263417e-05   
1e-11        1.5175506736979139        1.517719283583574         0.00011110659339584355   
1e-12        1.5175506736979139        1.517008740847814         0.0003571102168070571    
1e-13        1.5175506736979139        1.5276668818842154        0.006666141936236453     
0.03304958451121204 2.2155175205365363e-08
.lagrange_mult 13.5
  NIT    FC           OBJFUN            GNORM
    1     6     7.253886E-06     8.315523E-04
    2    11     4.468893E-06     7.934723E-04
    3    16     5.653465E-07     6.100196E-04
    4    21     5.559580E-07     6.620073E-05
    5    26     5.509661E-07     5.947786E-05
    6    31     5.190347E-07     5.900874E-05
    7    36     4.091808E-07     6.033781E-05
    8    41     2.978966E-07     8.274041E-05
    9    46     1.714558E-07     9.198212E-05
   10    51     1.291042E-07     6.596546E-05
   11    56     1.235166E-07     2.350810E-05
   12    61     1.232941E-07     5.040653E-06
   13    66     1.232709E-07     4.236826E-06
   14    71     1.231919E-07     4.224015E-06
   15    76     1.230105E-07     4.221877E-06
   16    81     1.225125E-07     4.284232E-06
   17    86     1.212473E-07     4.555800E-06
   18    91     1.180043E-07     5.317631E-06
   19    96     1.101425E-07     7.014966E-06
   20   101     9.305291E-08     9.925601E-06
   21   106     6.425862E-08     1.352536E-05
   22   111     3.432409E-08     1.518772E-05
   23   116     2.055861E-08     1.115580E-05
   24   121     1.866255E-08     4.027934E-06
   25   126     1.857547E-08     1.384433E-06
   26   131     1.857209E-08     1.285123E-06
   27   136     1.856912E-08     1.280061E-06
   28   141     1.855750E-08     1.277320E-06
   29   146     1.853092E-08     1.271059E-06
   30   151     1.845772E-08     1.263485E-06
   31   156     1.827146E-08     1.253920E-06
   32   161     1.779107E-08     1.247760E-06
   33   166     1.660936E-08     1.260561E-06
   34   171     1.394588E-08     1.327822E-06
   35   176     9.113384E-09     1.476120E-06
   36   181     3.524983E-09     1.567478E-06
   37   186     5.751501E-10     1.217047E-06
   38   191     3.215125E-11     4.976872E-07
   39   196     4.852774E-13     8.834044E-08
   40   201     1.704802E-15     6.375143E-09
   41   206     1.357478E-17     7.283004E-10
   42   210     1.357478E-17     3.926402E-11
Optimization terminated successfully    (Exit mode 0)
            Current function value: 1.3574778685418848e-17
            Iterations: 42
            Function evaluations: 210
            Gradient evaluations: 42
.0 -0.0078073295765380335
[400 665 666] 998.9000000000001 [1.6625 1.665 ]
0.0017893371928012097 0.0017319466427486564 var
0.03207363613937308
.No. samples 10000
Mean 1.0090797893874486
Mean +/- 2 sigma [0.9891576255399198, 1.0290019532349775]
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.06845842287174012       0.048271212490688         0.29488278482362573      
0.1          0.06845842287174012       0.06947512962133473       0.014851448615745352     
0.01         0.06845842287174012       0.06856479533960957       0.0015538258611179575    
0.001        0.06845842287174012       0.06846910944546813       0.00015610312478336023   
0.0001       0.06845842287174012       0.06845949202527457       1.561756011321594e-05    
1e-05        0.06845842287174012       0.06845852980053024       1.5619522864000824e-06   
1e-06        0.06845842287174012       0.06845843358860293       1.565455697675159e-07    
1e-07        0.06845842287174012       0.06845842448477413       2.356224324562524e-08    
1e-08        0.06845842287174012       0.06845843891767345       2.3438946821935435e-07   
1e-09        0.06845842287174012       0.06845846112213394       5.587390451020144e-07    
1e-10        0.06845842287174012       0.06845746192141178       1.4036991914617693e-05   
1e-11        0.06845842287174012       0.06847855615887966       0.00029409510612390943   
1e-12        0.06845842287174012       0.06850076061937216       0.0006184446830065696    
1e-13        0.06845842287174012       0.06661338147750939       0.026951269352019542     
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.03853408608851641       0.11148191144996344       1.8930726732140224       
0.1          0.03853408608851641       0.043803920090565995      0.13675772639175302      
0.01         0.03853408608851641       0.03903956055695268       0.013117593272489854     
0.001        0.03853408608851641       0.038584443089462184      0.001306817056205702     
0.0001       0.03853408608851641       0.03853911990425729       0.0001306328046632788    
1e-05        0.03853408608851641       0.03853458943048338       1.306225261996167e-05    
1e-06        0.03853408608851641       0.03853413632626257       1.3037222692172422e-06   
1e-07        0.03853408608851641       0.03853408259146818       9.075207405441894e-08    
1e-08        0.03853408608851641       0.03853406482789978       5.517353280285218e-07    
1e-09        0.03853408608851641       0.03853362073868993       1.2076316677381093e-05   
1e-10        0.03853408608851641       0.03852917984659143       0.0001273221301709068    
1e-11        0.03853408608851641       0.03850253449400043       0.0008187970111320611    
1e-12        0.03853408608851641       0.038191672047105385      0.00888600395567886      
1e-13        0.03853408608851641       0.03552713678800501       0.07803349205179429      
.....[[0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 2. 2. 2. 2. 2. 2. 2.]
 [0. 2. 2. 2. 2. 2. 2. 2.]
 [0. 2. 2. 4. 2. 4. 2. 4.]
 [0. 2. 2. 2. 2. 2. 2. 2.]
 [0. 2. 2. 4. 2. 6. 2. 6.]
 [0. 2. 2. 2. 2. 2. 2. 2.]
 [0. 2. 2. 4. 2. 6. 2. 8.]]
[[0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 2. 2. 0. 0. 0. 0. 0.]
 [0. 2. 2. 0. 0. 0. 0. 0.]
 [0. 0. 0. 2. 2. 0. 0. 0.]
 [0. 0. 0. 2. 2. 0. 0. 0.]
 [0. 0. 0. 0. 0. 4. 4. 0.]
 [0. 0. 0. 0. 0. 4. 4. 0.]
 [0. 0. 0. 0. 0. 0. 0. 4.]]
..........F4 1 [0 0 0 0]
0.00515787450044217 892.0 [ 1  6  6  2 53]
F
pyapprox/multifidelity/tests/test_low_rank_multi_fidelilty.py QOI: 100, snapshots: 10000, interpolation nodes: 20
ID None
.QOI: 3, snapshots: 3, interpolation nodes: 3
QOI: 4, snapshots: 3, interpolation nodes: 3
.(3, 3)
(4, 3)
.QOI: 5, snapshots: 4, interpolation nodes: 4
.
pyapprox/multifidelity/tests/test_mfnets.py Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          125.97099317554773        126.13866659157793        0.0013310478214340596    
0.1          125.97099317554773        125.9877605168731         0.00013310477993926865   
0.01         125.97099317554773        125.97266991215292        1.3310497622688632e-05   
0.001        125.97099317554773        125.97116085817106        1.3311209120516846e-06   
0.0001       125.97099317554773        125.97100983839482        1.32275269630365e-07     
1e-05        125.97099317554773        125.97099412232636        7.515846332448939e-09    
1e-06        125.97099317554773        125.97101158462465        1.4613742776204338e-07   
1e-07        125.97099317554773        125.97069144248962        2.3952582297123286e-06   
1e-08        125.97099317554773        125.97010936588049        7.015977606938459e-06    
1e-09        125.97099317554773        125.96137821674346        7.632676826544323e-05    
1e-10        125.97099317554773        126.01958587765694        0.0003857451694572826    
1e-11        125.97099317554773        125.14647096395494        0.006545333896381801     
1e-12        125.97099317554773        116.41532182693481        0.07585612455477388      
1e-13        125.97099317554773        291.03830456733704        1.3103596886130653       
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          6805.641046886892         7109.571052510786         0.04465854186695922      
0.1          6805.641046886892         6837.1956978185335        0.004636543525326818     
0.01         6805.641046886892         6808.807766370592         0.0004653080381235489    
0.001        6805.641046886892         6805.957831020351         4.6547287944873674e-05   
0.0001       6805.641046886892         6805.672726040939         4.654837630863268e-06    
1e-05        6805.641046886892         6805.64421636518          4.657134084051679e-07    
1e-06        6805.641046886892         6805.6413874728605        5.004465655716335e-08    
1e-07        6805.641046886892         6805.64102367498          3.4106870618750895e-09   
1e-08        6805.641046886892         6805.639714002609         1.9584992409041347e-07   
1e-09        6805.641046886892         6805.625162087381         2.334063668851951e-06    
1e-10        6805.641046886892         6805.348675698042         4.296012481932117e-05    
1e-11        6805.641046886892         6801.5651777386665        0.000598895698457321     
1e-12        6805.641046886892         6810.296326875687         0.0006840325483996017    
1e-13        6805.641046886892         6839.40015733242          0.004960460037922677     
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          27.844424500705827        21.963216652433402        0.21121671407226758      
0.1          27.844424500705827        27.165103822435412        0.024397009112298033     
0.01         27.844424500705827        27.77563555209781         0.002470474784144773     
0.001        27.844424500705827        27.837537091727427        0.00024735325300854235   
0.0001       27.844424500705827        27.84373567919829         2.4738220304044524e-05   
1e-05        27.844424500705827        27.84435569083143         2.4712263093812997e-06   
1e-06        27.844424500705827        27.84441767289536         2.4521284202424976e-07   
1e-07        27.844424500705827        27.844421310874168        1.1455908019126615e-07   
1e-08        27.844424500705827        27.84440766845364         6.045106870649547e-07    
1e-09        27.844424500705827        27.84372554742731         2.5102091030749383e-05   
1e-10        27.844424500705827        27.848727768287063        0.0001545468314896031    
1e-11        27.844424500705827        27.87601260934025         0.0011344500452369801    
1e-12        27.844424500705827        28.194335754960775        0.012566654205623046     
1e-13        27.844424500705827        22.737367544323206        0.18341398854385238      
.[[ 1.62434536]
 [-0.61175641]
 [-0.52817175]
 [-1.07296862]
 [ 0.86540763]
 [-2.3015387 ]
 [ 1.74481176]
 [-0.7612069 ]
 [ 0.3190391 ]
 [-0.24937038]
 [ 1.46210794]
 [-2.06014071]
 [-0.3224172 ]
 [-0.38405435]
 [ 1.13376944]] pp
[array([[1.7527783 , 1.78921333, 0.17008842, 0.07810957]]), array([[0.33966084, 1.75628501, 0.19669367, 0.84221525]]), array([[1.91577906, 1.06633057, 1.38375423, 0.63103126]]), array([[1.37300186, 1.66925134, 0.03657655, 1.50028863]])] [array([[-1.07059405],
       [-1.16104532],
       [ 1.50501263],
       [ 1.5733389 ]]), array([[ 1.32075218],
       [-8.53531567],
       [ 1.59678116],
       [-0.69514221]]), array([[-1.06523998],
       [-0.70455506],
       [-0.79722355],
       [-0.65918265]]), array([[-1.11002879],
       [-1.01997366],
       [-0.13576348],
       [-1.08494292]])]
(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), (array([[0.]]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])))
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.6618735546418022        1.313471397423804         2.9844748112570207       
0.1          0.6618735546418022        0.3787716807680397        0.4277280333808982       
0.01         0.6618735546418022        0.632437178938261         0.04447431914615744      
0.001        0.6618735546418022        0.6589183855680858        0.00446485443177391      
0.0001       0.6618735546418022        0.6615779221874618        0.00044666001877107994   
1e-05        0.6618735546418022        0.6618439897465578        4.46684945139049e-05     
1e-06        0.6618735546418022        0.6618706009930975        4.4625573631315195e-06   
1e-07        0.6618735546418022        0.6618732584229292        4.475460167710456e-07    
1e-08        0.6618735546418022        0.6618741110742121        8.40692917890069e-07     
1e-09        0.6618735546418022        0.6618705583605333        4.5269693098645745e-06   
1e-10        0.6618735546418022        0.6619416126341093        0.0001028262752452283    
1e-11        0.6618735546418022        0.6622258297284134        0.0005322392534655999    
1e-12        0.6618735546418022        0.6608047442568932        0.0016148256376362577    
1e-13        0.6618735546418022        0.7105427357601002        0.07353244555092875      
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 57
         Function evaluations: 61
         Gradient evaluations: 61
...
pyapprox/optimization/tests/test_cvar_regression.py FEps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.11301899862653669       0.3412861723471946        2.0197239092070767       
0.1          0.11301899862653669       0.13439917026193515       0.1891732531275359       
0.01         0.11301899862653669       0.11497797871725668       0.017333192777555018     
0.001        0.11301899862653669       0.11321332820657393       0.0017194417080209844    
0.0001       0.11301899862653669       0.1130384161540654        0.00017180764088056385   
1e-05        0.11301899862653669       0.11302094021781171       1.7179335320747368e-05   
1e-06        0.11301899862653669       0.11301919278228212       1.7179036072248944e-06   
1e-07        0.11301899862653669       0.1130190163678435        1.5697632281418704e-07   
1e-08        0.11301899862653669       0.11301899416338301       3.9490295739709416e-08   
1e-09        0.11301899862653669       0.11301892755000152       6.288901515241903e-07    
1e-10        0.11301899862653669       0.11301737323776706       1.438155345017415e-05    
1e-11        0.11301899862653669       0.11300960167659468       8.314486994403791e-05    
1e-12        0.11301899862653669       0.11302070390684094       1.5088439332910318e-05   
1e-13        0.11301899862653669       0.11213252548714081       0.007843576302822947     
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.10950421725502703       0.16644477592689821       0.5199850754538607       
0.1          0.10950421725502703       0.11719363845792641       0.07022032023653756      
0.01         0.10950421725502703       0.11028539526187764       0.007133770976430125     
0.001        0.10950421725502703       0.10958243469882945       0.0007142870454044195    
0.0001       0.10950421725502703       0.1095120399741134        7.143760562353326e-05    
1e-05        0.10950421725502703       0.10950499954587299       7.1439334993030955e-06   
1e-06        0.10950421725502703       0.1095042955201464        7.147224219612055e-07    
1e-07        0.10950421725502703       0.10950422568711815       7.700243272748944e-08    
1e-08        0.10950421725502703       0.1095042279075642        9.727969788595417e-08    
1e-09        0.10950421725502703       0.10950418349864321       3.0826560528334066e-07   
1e-10        0.10950421725502703       0.10950351736482844       6.391445152822763e-06    
1e-11        0.10950421725502703       0.10951239914902544       7.47176154810362e-05     
1e-12        0.10950421725502703       0.10946799022804043       0.0003308276876882586    
1e-13        0.10950421725502703       0.11102230246251565       0.01386325792323706      
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.1361490224281866        0.09951687343745985       0.26905921421542933      
0.1          0.1361490224281866        0.13614902242818694       2.446340792224395e-15    
0.01         0.1361490224281866        0.13614902242820026       1.002999724812002e-13    
0.001        0.1361490224281866        0.13614902242831128       9.157469032226652e-13    
0.0001       0.1361490224281866        0.1361490224294215        9.070216210637316e-12    
1e-05        0.1361490224281866        0.13614902244496463       1.2323278651444243e-10   
1e-06        0.1361490224281866        0.13614902250047578       5.30956251885175e-10     
1e-07        0.1361490224281866        0.1361490231666096        5.423637836333965e-09    
1e-08        0.1361490224281866        0.13614902538705564       2.1732576451163266e-08   
1e-09        0.1361490224281866        0.13614909200043712       5.110007348960422e-07    
1e-10        0.1361490224281866        0.13614998017885682       7.034576180827763e-06    
1e-11        0.1361490224281866        0.1361577517400292        6.411586133273031e-05    
1e-12        0.1361490224281866        0.1362243651215067        0.0005533840197776093    
1e-13        0.1361490224281866        0.13655743202889425       0.0029997248120020043    
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.1399426901078535        0.26293276267123433       0.8788602853681936       
0.1          0.1399426901078535        0.13994269010785487       9.718429766625538e-15    
0.01         0.1399426901078535        0.13994269010786375       7.31857262017311e-14     
0.001        0.1399426901078535        0.13994269010786375       7.31857262017311e-14     
0.0001       0.1399426901078535        0.13994269010941807       1.1179962602345202e-11   
1e-05        0.1399426901078535        0.13994269011385896       4.291361081989798e-11    
1e-06        0.1399426901078535        0.13994269010275673       3.642050972398396e-11    
1e-07        0.1399426901078535        0.13994269076889054       4.7236267229089324e-09   
1e-08        0.1399426901078535        0.1399427018711208        8.405774726679087e-08    
1e-09        0.1399426901078535        0.13994272407558128       2.4272598835455477e-07   
1e-10        0.1399426901078535        0.13994361225400098       6.58945563186511e-06     
1e-11        0.1399426901078535        0.13995471448424723       8.592357617574706e-05    
1e-12        0.1399426901078535        0.13999912340523224       0.00040326005835127483   
1e-13        0.1399426901078535        0.13988810110276972       0.0003900811470875446    
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.5249791874789399        0.9355644418637913        0.7820981558460781       
0.1          0.5249791874789399        0.6429386650418597        0.2246936266738229       
0.01         0.5249791874789399        0.5374220930802073        0.023701712178383325     
0.001        0.5249791874789399        0.5262258548851956        0.0023746987232816466    
0.0001       0.5249791874789399        0.5251038734173374        0.0002375064409625601    
1e-05        0.5249791874789399        0.524991656260243         2.375100118349208e-05    
1e-06        0.5249791874789399        0.5249804343465669        2.3750801113160207e-06   
1e-07        0.5249791874789399        0.5249793121053781        2.373931027015242e-07    
1e-08        0.5249791874789399        0.5249792003891862        2.459192025962408e-08    
1e-09        0.5249791874789399        0.5249791712458318        3.092143168174117e-08    
1e-10        0.5249791874789399        0.524979087979105         1.8953100865707046e-07   
1e-11        0.5249791874789399        0.52497728386669          3.626071843122538e-06    
1e-12        0.5249791874789399        0.5249689571940053        1.9487029540655466e-05   
1e-13        0.5249791874789399        0.5248579348915428        0.00023096646550776117   
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          2.49376040192892          0.5249290153142562        0.7895030272722975       
0.1          2.49376040192892          2.359286901039439         0.05392398595529298      
0.01         2.49376040192892          2.49791874789399          0.0016675001984367782    
0.001        2.49376040192892          2.4943626871397617        0.00024151687161924517   
0.0001       2.49376040192892          2.4938224877690818        2.489647366036076e-05    
1e-05        2.49376040192892          2.493766629074745         2.497090666927692e-06    
1e-06        2.49376040192892          2.493761024680019         2.497237098846044e-07    
1e-07        2.49376040192892          2.4937604636843247        2.4763968800752777e-08   
1e-08        2.49376040192892          2.4937603981811662        1.5028523430177486e-09   
1e-09        2.49376040192892          2.4937603093633243        3.71188810125371e-08     
1e-10        2.49376040192892          2.4937596432295095        3.042390960339323e-07    
1e-11        2.49376040192892          2.493749651222288         4.31104232135486e-06     
1e-12        2.49376040192892          2.493671935610564         3.5475067407184294e-05   
1e-13        2.49376040192892          2.4924506902834764        0.0005251954616130755    
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.027999999999999997      9.499999999999999e-05     0.9966071428571428       
0.1          0.027999999999999997      0.0009499999999999999     0.9660714285714286       
0.01         0.027999999999999997      0.0095                    0.6607142857142856       
0.001        0.027999999999999997      0.025380499999999983      0.09355357142857194      
0.0001       0.027999999999999997      0.027730800499999896      0.009614267857146485     
1e-05        0.027999999999999997      0.027973008000499823      0.0009639999821490756    
1e-06        0.027999999999999997      0.027997300079980598      9.64257149785468e-05     
1e-07        0.027999999999999997      0.02799973000065228       9.642833847012455e-06    
1e-08        0.027999999999999997      0.027999973001258897      9.64240753586833e-07     
1e-09        0.027999999999999997      0.027999997286032304      9.692741760503848e-08    
1e-10        0.027999999999999997      0.027999999644172033      1.2708141582702076e-08   
1e-11        0.027999999999999997      0.028000003574404908      1.2765731824801056e-07   
1e-12        0.027999999999999997      0.02800000899541577       3.212648490489935e-07    
1e-13        0.027999999999999997      0.027999927680252834      2.5828481129657507e-06   
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          5.399999999999999         0.027999999999999997      0.9948148148148149       
0.1          5.399999999999999         0.27999999999999997       0.9481481481481481       
0.01         5.399999999999999         2.8                       0.4814814814814814       
0.001        5.399999999999999         5.157999999999996         0.04481481481481532      
0.0001       5.399999999999999         5.375979999999898         0.0044481481481667634    
1e-05        5.399999999999999         5.397599799999639         0.00044448148154814987   
1e-06        5.399999999999999         5.399759997994774         4.444481578226425e-05    
1e-07        5.399999999999999         5.399975999929863         4.4444574325368615e-06   
1e-08        5.399999999999999         5.399997600116779         4.444228183818088e-07    
1e-09        5.399999999999999         5.39999975637806          4.5115173818999223e-08   
1e-10        5.399999999999999         5.399999926380961         1.3633155180901604e-08   
1e-11        5.399999999999999         5.4000005855758815        1.0843997831376263e-07   
1e-12        5.399999999999999         5.400003361133443         6.224321193439278e-07    
1e-13        5.399999999999999         5.399986013898683         2.5900187620946045e-06   
.
pyapprox/optimization/tests/test_l1_minimization.py Primal Feasibility  Dual Feasibility    Duality Gap         Step             Path Parameter      Objective          
1.0                 1.0                 1.0                 -                1.0                 8.0                 
0.3053700406446     0.3053700406451     0.3053700406451     0.7094660279903  0.3053700409233     7.166482940989      
0.07012704563458    0.07012704563651    0.07012704563651    0.7841628879257  0.07012704565349    4.42804550582       
0.01467423522937    0.014674235227      0.014674235227      0.802245844349   0.01467423489296    4.084566028478      
0.001425387220461   0.001425387219776   0.001425387219774   0.9250917250667  0.001425386942393   3.909621074335      
2.840033525185e-07  2.840033396925e-07  2.840033405486e-07  0.9998023010839  2.83923309336e-07   3.895266328666      
1.430572506387e-11  1.433648351562e-11  1.433756816975e-11  0.9999495200373  1.432452174121e-11  3.89526341926       
Optimization terminated successfully.
         Current function value: 2.000000    
         Iterations: 6
.[0. 0. 1. 1. 0. 0. 0. 0.]
  i = 1  tdiff = 2.8284271247e+00  fdiff =         inf  xdiff = 1.2760748667e+00  r = 1.0000000000e+04  ttol = 1.00e+00  gtol = 1.00e-02  nfev = 33
  i = 2  tdiff = 0.0000000000e+00  fdiff = 9.4151523950e-02  xdiff = 5.5607627666e-01  r = 2.0000000000e+04  ttol = 1.00e-01  gtol = 1.00e-03  nfev = 119
  i = 3  tdiff = 0.0000000000e+00  fdiff = 7.1931117914e-04  xdiff = 3.0316208650e-03  r = 2.0000000000e+04  ttol = 1.00e-02  gtol = 1.00e-04  nfev = 209
  i = 4  tdiff = 0.0000000000e+00  fdiff = 1.8903204995e-03  xdiff = 4.6839456328e-03  r = 2.0000000000e+04  ttol = 1.00e-03  gtol = 1.00e-05  nfev = 309
  i = 5  tdiff = 6.6571173647e-01  fdiff = 7.7972960491e-04  xdiff = 6.6923173172e-03  r = 2.0000000000e+04  ttol = 1.00e-04  gtol = 1.00e-06  nfev = 452
  i = 6  tdiff = 1.0104430344e-01  fdiff = 1.1436486520e-05  xdiff = 1.6933895868e-04  r = 4.0000000000e+04  ttol = 1.00e-05  gtol = 1.00e-07  nfev = 622
  i = 7  tdiff = 3.8451416087e-02  fdiff = 2.5498561995e-07  xdiff = 3.8726647682e-06  r = 8.0000000000e+04  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 797
  i = 8  tdiff = 2.4730875371e-02  fdiff = 5.7856700320e-08  xdiff = 3.3305817211e-05  r = 1.6000000000e+05  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 1017
  i = 9  tdiff = 4.1724843086e-02  fdiff = 2.5037443230e-09  xdiff = 7.2973023905e-07  r = 3.2000000000e+05  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 1314
  i = 10  tdiff = 8.1504911920e-02  fdiff = 6.5221523737e-09  xdiff = 9.1987730035e-06  r = 6.4000000000e+05  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 1822
  i = 11  tdiff = 2.3836908942e+00  fdiff = 5.8355861339e-03  xdiff = 1.2500246222e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 2090
  i = 12  tdiff = 2.6541639468e+00  fdiff = 4.2138105863e-03  xdiff = 1.7589652153e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 2537
  i = 13  tdiff = 6.2912456424e-02  fdiff = 1.6076421613e-03  xdiff = 5.3147791396e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 2981
  i = 14  tdiff = 4.0554325631e-01  fdiff = 1.4211090085e-05  xdiff = 2.5509677805e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 3212
  i = 15  tdiff = 2.3978376146e+00  fdiff = 3.0896468724e-03  xdiff = 4.3055661108e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 3418
  i = 16  tdiff = 2.3659239531e+00  fdiff = 3.0924600921e-03  xdiff = 4.2882420253e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 3871
  i = 17  tdiff = 7.3586411950e-02  fdiff = 2.8860238808e-06  xdiff = 1.3213002371e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 4193
  i = 18  tdiff = 4.0230710846e-01  fdiff = 1.8022400224e-03  xdiff = 5.2082304365e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 4665
  i = 19  tdiff = 4.0917751249e-01  fdiff = 1.8023113952e-03  xdiff = 5.2014192266e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 5096
  i = 20  tdiff = 1.2164030729e-01  fdiff = 8.1158430865e-08  xdiff = 1.1083478093e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 5537
  i = 21  tdiff = 2.4254880030e-01  fdiff = 3.0338939583e-08  xdiff = 8.4184729366e-06  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 5995
  i = 22  tdiff = 4.6429947288e-01  fdiff = 4.4708632574e-08  xdiff = 1.5051926433e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 6315
  i = 23  tdiff = 8.8972974919e-01  fdiff = 5.3747005622e-06  xdiff = 2.6523754878e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 6603
  i = 24  tdiff = 5.0754057681e-01  fdiff = 1.0009039241e-03  xdiff = 1.7936520972e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 6863
  i = 25  tdiff = 1.4572785427e+00  fdiff = 9.1899390899e-04  xdiff = 1.6687446371e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 7149
  i = 26  tdiff = 1.1854900567e+00  fdiff = 8.8052037905e-05  xdiff = 1.4401841766e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 7532
  i = 27  tdiff = 1.6292690610e+00  fdiff = 1.6819636033e-03  xdiff = 5.5844829667e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 7889
  i = 28  tdiff = 1.6272681339e+00  fdiff = 1.6825843569e-03  xdiff = 5.6011100246e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 8276
  i = 29  tdiff = 1.3015403832e-01  fdiff = 7.3163320982e-06  xdiff = 1.2297753744e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 8721
  i = 30  tdiff = 1.7198815769e-01  fdiff = 6.0158682238e-06  xdiff = 1.5155023577e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 9068
  i = 31  tdiff = 1.6470280649e+00  fdiff = 2.1806532060e-03  xdiff = 1.1773676548e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 9611
  i = 32  tdiff = 1.6223875905e+00  fdiff = 2.1819729286e-03  xdiff = 1.1796818271e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 10091
  i = 33  tdiff = 4.5779147051e-01  fdiff = 1.7401074334e-03  xdiff = 4.3841127976e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 10474
  i = 34  tdiff = 2.1121460333e+00  fdiff = 2.7830099735e-05  xdiff = 1.3275345530e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 10689
  i = 35  tdiff = 2.1370004527e+00  fdiff = 7.4950007725e-04  xdiff = 8.5430758597e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 11031
  i = 36  tdiff = 6.2296660299e-01  fdiff = 1.0173220140e-03  xdiff = 3.1907360294e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 11423
  i = 37  tdiff = 4.3821054691e-01  fdiff = 2.4395162754e-04  xdiff = 1.0722488147e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 11962
  i = 38  tdiff = 1.6577086593e+00  fdiff = 1.0666265842e-03  xdiff = 1.0504937595e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 12308
  i = 39  tdiff = 2.0029326237e+00  fdiff = 9.6089086133e-04  xdiff = 4.6211808674e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 12592
  i = 40  tdiff = 4.9607565209e-01  fdiff = 2.2401762738e-03  xdiff = 4.8517463990e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 13189
  i = 41  tdiff = 3.7221938747e-01  fdiff = 3.1153438691e-05  xdiff = 4.3966525190e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 13639
  i = 42  tdiff = 6.7799328540e-01  fdiff = 1.8506638661e-03  xdiff = 6.6154701249e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 13905
  i = 43  tdiff = 6.7377383909e-01  fdiff = 1.8508265107e-03  xdiff = 6.6149531626e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 14458
  i = 44  tdiff = 2.6098703207e+00  fdiff = 8.0088986894e-04  xdiff = 5.4508505072e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 14932
  i = 45  tdiff = 2.5714843718e+00  fdiff = 8.0402783981e-04  xdiff = 5.4856832776e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 15229
  i = 46  tdiff = 1.6609460253e+00  fdiff = 1.2367727315e-05  xdiff = 2.2792587318e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 15600
  i = 47  tdiff = 1.6986607912e+00  fdiff = 1.0409583670e-05  xdiff = 2.3363976336e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 15951
  i = 48  tdiff = 1.3745289581e+00  fdiff = 5.5703415170e-04  xdiff = 4.3222975657e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 16605
  i = 49  tdiff = 1.3297049058e+00  fdiff = 5.5653438087e-04  xdiff = 4.3225774332e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 17052
  i = 50  tdiff = 6.6774959151e-02  fdiff = 9.1031721228e-07  xdiff = 2.2564276359e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 17471
  i = 51  tdiff = 6.4464964838e-01  fdiff = 2.6675151266e-07  xdiff = 1.3374646721e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 17835
  i = 52  tdiff = 7.3113343886e-01  fdiff = 6.5166165453e-05  xdiff = 1.3380397735e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 18169
  i = 53  tdiff = 5.3356906270e-01  fdiff = 6.5573227720e-05  xdiff = 1.3426052403e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 18675
  i = 54  tdiff = 8.7845978364e-01  fdiff = 9.5957960511e-07  xdiff = 6.6071827631e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 18968
  i = 55  tdiff = 1.0395207068e+00  fdiff = 3.2509667292e-07  xdiff = 1.1108796194e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 19367
  i = 56  tdiff = 3.7992320994e+00  fdiff = 2.2626948515e+00  xdiff = 1.3925019677e+00  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 19644
  i = 57  tdiff = 2.6999137599e+00  fdiff = 2.2622275763e+00  xdiff = 1.3926515201e+00  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 20036
  i = 58  tdiff = 2.8752290567e+00  fdiff = 4.7185708564e-04  xdiff = 3.7610212146e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 20451
  i = 59  tdiff = 9.3455377615e-02  fdiff = 4.0885922703e-06  xdiff = 1.5283431160e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 20892
  i = 60  tdiff = 9.5702616075e-02  fdiff = 3.4628950818e-09  xdiff = 8.3643892610e-06  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 21317
  i = 61  tdiff = 2.6078647705e-01  fdiff = 3.2621150003e-08  xdiff = 3.6824865063e-06  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 21820
  i = 62  tdiff = 3.0684935447e-01  fdiff = 6.9440097006e-08  xdiff = 4.2937389326e-06  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 22269
  i = 63  tdiff = 8.3272000655e-01  fdiff = 2.6818662537e-06  xdiff = 1.8225692104e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 22708
  i = 64  tdiff = 1.0489914297e+00  fdiff = 2.5881170900e-06  xdiff = 1.3725966699e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 23014
  i = 65  tdiff = 9.3958744787e-01  fdiff = 7.5472066185e-04  xdiff = 7.7603303276e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 23450
  i = 66  tdiff = 9.2447952719e-01  fdiff = 7.5520629099e-04  xdiff = 7.7339597093e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 23716
  i = 67  tdiff = 3.8829916767e-01  fdiff = 2.0938286237e-03  xdiff = 3.1318996125e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 24094
  i = 68  tdiff = 3.7542447551e-01  fdiff = 2.0934730300e-03  xdiff = 3.1142277330e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 24535
  i = 69  tdiff = 1.9508902267e-01  fdiff = 9.6630674795e-08  xdiff = 2.3053060060e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 24869
  i = 70  tdiff = 1.8870172931e-01  fdiff = 4.1068178191e-08  xdiff = 1.7298993333e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 25290
  i = 71  tdiff = 1.0700331844e-01  fdiff = 1.8519558553e-08  xdiff = 7.6996968226e-06  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 25789
  i = 72  tdiff = 1.8175309836e+00  fdiff = 7.7149527541e-03  xdiff = 2.0908520665e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 26003
  i = 73  tdiff = 1.8229249196e+00  fdiff = 7.7166101720e-03  xdiff = 2.0931688302e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 26371
  i = 74  tdiff = 1.7358932662e-01  fdiff = 1.6622671819e-06  xdiff = 1.9360324171e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 26858
  i = 75  tdiff = 7.4390714695e-01  fdiff = 1.4421053666e-03  xdiff = 6.1948722863e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 27143
  i = 76  tdiff = 1.3475505321e+00  fdiff = 1.2309680448e-03  xdiff = 6.3981972751e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 27695
  i = 77  tdiff = 1.1728115515e+00  fdiff = 2.1181669441e-04  xdiff = 1.9003115262e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 27980
  i = 78  tdiff = 1.5567455126e-01  fdiff = 6.7166501294e-07  xdiff = 3.8439661717e-06  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 28552
  i = 79  tdiff = 7.6490368885e-01  fdiff = 1.2053592950e-05  xdiff = 4.4335381206e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 29063
  i = 80  tdiff = 7.4753150949e-01  fdiff = 1.1619325983e-05  xdiff = 3.5002321499e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 29345
  i = 81  tdiff = 1.5730489872e-01  fdiff = 4.2662741735e-06  xdiff = 1.0552784967e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 29812
  i = 82  tdiff = 1.3701611846e-01  fdiff = 4.5973084923e-06  xdiff = 5.8556865882e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 30232
  i = 83  tdiff = 2.1638633046e+00  fdiff = 1.6145709054e-02  xdiff = 4.2573352907e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 30465
  i = 84  tdiff = 2.1680865211e+00  fdiff = 1.6148190351e-02  xdiff = 4.2562302002e-02  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 30953
  i = 85  tdiff = 5.1048265830e-01  fdiff = 3.1960066800e-06  xdiff = 9.6721984184e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 31312
  i = 86  tdiff = 4.8621936594e-01  fdiff = 9.2240348271e-07  xdiff = 1.1380625306e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 31846
  i = 87  tdiff = 1.6392263927e+00  fdiff = 3.9641031739e-05  xdiff = 4.0437242657e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 32193
  i = 88  tdiff = 5.3141829718e-01  fdiff = 9.2713489938e-04  xdiff = 6.3619903177e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 32643
  i = 89  tdiff = 2.0933319386e+00  fdiff = 9.6238776277e-04  xdiff = 6.2746941040e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 33144
  i = 90  tdiff = 1.3309888879e-01  fdiff = 1.4910126402e-03  xdiff = 4.6644261728e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 33664
  i = 91  tdiff = 8.0698767330e-01  fdiff = 1.4644330390e-03  xdiff = 4.9670888115e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 34222
  i = 92  tdiff = 4.3783533662e-01  fdiff = 3.0938530271e-05  xdiff = 5.5167328868e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 34609
  i = 93  tdiff = 1.5990706181e-01  fdiff = 6.8246215124e-08  xdiff = 9.9985375209e-06  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 34972
  i = 94  tdiff = 1.0511550932e-01  fdiff = 3.2700930852e-10  xdiff = 4.2809529118e-06  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 35460
  i = 95  tdiff = 1.0008572311e-01  fdiff = 2.4086659778e-08  xdiff = 8.1474567242e-07  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 35904
  i = 96  tdiff = 3.8874369673e-01  fdiff = 3.4442560391e-05  xdiff = 2.7663293751e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 36427
  i = 97  tdiff = 1.2926142105e+00  fdiff = 6.0073323303e-04  xdiff = 1.7220039267e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 36897
  i = 98  tdiff = 2.7048524243e+00  fdiff = 5.9758741261e-04  xdiff = 1.6330882507e-03  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 37523
  i = 99  tdiff = 2.9129779212e+00  fdiff = 4.2002366961e-05  xdiff = 2.0563157496e-04  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 38144
  i = 100  tdiff = 1.3723567880e-01  fdiff = 4.4078857688e-06  xdiff = 1.9806422160e-05  r = 1.0000000000e+06  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 38527
maxiter 100.0 reached
0.005442508008824471
.  i = 1  tdiff = 1.7320477765e+00  fdiff =         inf  xdiff = 1.7320477777e+00  r = 1.0000000000e+00  ttol = 1.00e+00  gtol = 1.00e-02  nfev = 2
  i = 2  tdiff = 6.3719816739e-03  fdiff = 1.5000152257e+00  xdiff = 1.7614690675e-15  r = 2.0000000000e+00  ttol = 1.00e-01  gtol = 1.00e-03  nfev = 4
  i = 3  tdiff = 6.3717805945e-03  fdiff = 2.0297753411e-05  xdiff = 6.3716707393e-03  r = 2.0000000000e+00  ttol = 1.00e-02  gtol = 1.00e-04  nfev = 6
  i = 4  tdiff = 4.2445642125e-05  fdiff = 1.0308575872e-05  xdiff = 3.1650298419e-03  r = 2.0000000000e+00  ttol = 1.00e-03  gtol = 1.00e-05  nfev = 10
  i = 5  tdiff = 6.8703014609e-05  fdiff = 1.6321241603e-07  xdiff = 2.1956015860e-05  r = 2.0000000000e+00  ttol = 1.00e-04  gtol = 1.00e-06  nfev = 14
  i = 6  tdiff = 9.9606451784e-05  fdiff = 1.2612468847e-08  xdiff = 6.6062713742e-05  r = 2.0000000000e+00  ttol = 1.00e-05  gtol = 1.00e-07  nfev = 19
  i = 7  tdiff = 7.1185215759e-05  fdiff = 5.2619545166e-08  xdiff = 4.3892678170e-05  r = 4.0000000000e+00  ttol = 1.00e-06  gtol = 1.00e-08  nfev = 29
  i = 8  tdiff = 1.1170339342e-04  fdiff = 3.1491847796e-08  xdiff = 3.8340963387e-06  r = 8.0000000000e+00  ttol = 1.00e-06  gtol = 1.00e-09  nfev = 38
  i = 9  tdiff = 3.0298565204e-04  fdiff = 3.3474410799e-08  xdiff = 4.9740118974e-06  r = 1.6000000000e+01  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 52
  i = 10  tdiff = 4.4803001765e-04  fdiff = 4.2389136645e-08  xdiff = 4.9396165526e-06  r = 3.2000000000e+01  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 70
  i = 11  tdiff = 8.2822758258e-04  fdiff = 9.9074358939e-08  xdiff = 1.8927510822e-05  r = 6.4000000000e+01  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 88
  i = 12  tdiff = 1.6266180828e-03  fdiff = 7.1032658422e-08  xdiff = 2.2246071311e-05  r = 1.2800000000e+02  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 110
  i = 13  tdiff = 5.1738448873e-03  fdiff = 1.5463132552e-08  xdiff = 1.0625146493e-05  r = 2.5600000000e+02  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 134
  i = 14  tdiff = 7.0335224375e-03  fdiff = 1.0233259262e-07  xdiff = 1.0152984798e-05  r = 5.1200000000e+02  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 161
  i = 15  tdiff = 8.1474042520e-03  fdiff = 8.4477256301e-09  xdiff = 1.1625583426e-05  r = 1.0240000000e+03  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 186
  i = 16  tdiff = 1.9073608353e-02  fdiff = 1.2652069881e-07  xdiff = 7.7323155817e-06  r = 2.0480000000e+03  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 233
  i = 17  tdiff = 2.5820301418e-02  fdiff = 2.8274226649e-07  xdiff = 1.5247773091e-05  r = 4.0960000000e+03  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 290
  i = 18  tdiff = 1.1802986387e-01  fdiff = 9.3398066658e-07  xdiff = 1.6944096394e-05  r = 8.1920000000e+03  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 354
  i = 19  tdiff = 1.3320214865e-01  fdiff = 1.6944679793e-07  xdiff = 1.5988935152e-05  r = 1.6384000000e+04  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 433
  i = 20  tdiff = 4.5991598092e-01  fdiff = 3.8391960895e-06  xdiff = 1.1353181293e-05  r = 3.2768000000e+04  ttol = 1.00e-06  gtol = 1.00e-10  nfev = 523
maxiter 20 reached
[2.58954552e-17 2.28409535e-13 1.75457426e-11 4.95970931e-10
 7.29207533e-09 6.33778769e-08 3.41029652e-07 1.11834716e-06
 1.94399370e-06 3.77338822e-07 5.21556536e-06 8.71693981e-06
 2.58008556e-06 3.96890701e-06 1.95908586e-07 5.44299731e-07
 4.83234064e-06 3.38488139e-07 2.46031480e-06 4.45518748e-06
 4.05074319e-06] c
..sEps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          2.037124453400126         3.5355028199933827        0.7355359973674371       
0.1          2.037124453400126         2.1869622900594488        0.07355359973674237      
0.01         2.037124453400126         2.0521082370660526        0.007355359973671534     
0.001        2.037124453400126         2.038622831766812         0.0007355359974130638    
0.0001       2.037124453400126         2.0372742912377895        7.355360022975286e-05    
1e-05        2.037124453400126         2.037139437160995         7.355348783125326e-06    
1e-06        2.037124453400126         2.0371259517038           7.354993316771841e-07    
1e-07        2.037124453400126         2.0371246023387357        7.311218008356609e-08    
1e-08        2.037124453400126         2.0371244691119728        7.712757513849003e-09    
1e-09        2.037124453400126         2.037124025022763         2.102853177185413e-07    
1e-10        2.037124453400126         2.0371282438702565        1.8606963969891666e-06   
1e-11        2.037124453400126         2.0370816145032222        2.1029101502411817e-05   
1e-12        2.037124453400126         2.037037205582237         4.2828909025650845e-05   
1e-13        2.037124453400126         2.036149027162537         0.00047882505949043146   
7.712757513849003e-09
`xtol` termination condition is satisfied.
Number of iterations: 527, function evaluations: 512, CG iterations: 2408, optimality: 1.53e-13, constraint violation: 0.00e+00, execution time: 0.67 s.
.Optimization terminated successfully    (Exit mode 0)
            Current function value: 2.000000000000014
            Iterations: 20
            Function evaluations: 21
            Gradient evaluations: 20
1.704737679105721e-06
.  NIT    FC           OBJFUN            GNORM
    1     2     3.718330E+00     2.828427E+00
    2     3     8.356871E-01     2.828427E+00
    3     4     1.338502E+00     2.828427E+00
    4     5     1.242289E+00     2.828427E+00
    5     6     1.494666E+00     2.828427E+00
    6     8     1.765524E+00     2.828427E+00
    7     9     1.784156E+00     2.828427E+00
    8    10     1.826355E+00     2.828427E+00
    9    11     1.889503E+00     2.828427E+00
   10    12     1.919764E+00     2.828427E+00
   11    14     1.953746E+00     2.828427E+00
   12    16     1.964089E+00     2.828427E+00
   13    17     1.974373E+00     2.828427E+00
   14    19     1.987309E+00     2.828427E+00
   15    20     1.993616E+00     2.828427E+00
   16    21     1.992906E+00     2.828427E+00
   17    22     1.993534E+00     2.828427E+00
   18    23     1.995778E+00     2.828427E+00
   19    25     1.997044E+00     2.828427E+00
   20    27     1.997404E+00     2.828427E+00
   21    28     1.997934E+00     2.828427E+00
   22    30     1.998230E+00     2.828427E+00
   23    31     1.998333E+00     2.828427E+00
   24    32     1.998337E+00     2.828427E+00
   25    32     1.998337E+00     2.828427E+00
Optimization terminated successfully    (Exit mode 0)
            Current function value: 1.998336829283126
            Iterations: 25
            Function evaluations: 32
            Gradient evaluations: 25
0.0054543057023745635
[-3.81115887e-14 -1.92837208e-13  2.93447241e-03 -4.59764313e-03
  4.99992165e-12 -3.04332600e-12  9.02653271e-13 -3.22050192e-12]
[-3.81115887e-14 -1.92837208e-13  1.00293447e+00  9.95402357e-01
  4.99992165e-12 -3.04332600e-12  9.02653271e-13 -3.22050192e-12] [0. 0. 1. 1. 0. 0. 0. 0.]
.Optimization terminated successfully    (Exit mode 0)
            Current function value: 8.000000001081286
            Iterations: 5
            Function evaluations: 5
            Gradient evaluations: 5
...F
pyapprox/optimization/tests/test_optimization.py ..[0.99850326] m 0.001
.
pyapprox/optimization/tests/test_quantile_regression.py s
pyapprox/optimization/tests/test_stochastic_dominance.py ROL requested by not available switching to scipy.minimize
| niter |f evals|CG iter|  obj func   |tr radius |   opt    |  c viol  |
|-------|-------|-------|-------------|----------|----------|----------|
|   1   |   1   |   0   | +1.9587e+00 | 1.00e+00 | 6.05e+00 | 0.00e+00 |
|   2   |   2   |   3   | +1.9587e+00 | 1.00e-01 | 6.05e+00 | 0.00e+00 |
|   3   |   3   |   4   | +1.2870e+00 | 7.00e-01 | 7.65e-01 | 0.00e+00 |
|   4   |   4   |   5   | +6.1244e-01 | 1.40e+00 | 1.26e-01 | 0.00e+00 |
|   5   |   5   |   6   | +5.8102e-01 | 1.40e+00 | 3.54e-02 | 0.00e+00 |
|   6   |   6   |   8   | +6.1216e-01 | 1.40e+00 | 6.52e-02 | 0.00e+00 |
|   7   |   6   |   8   | +6.1216e-01 | 7.00e+00 | 1.30e-01 | 0.00e+00 |
|   8   |   7   |  10   | +4.0138e-01 | 7.00e+00 | 6.77e-02 | 0.00e+00 |
|   9   |   8   |  12   | +2.6523e-01 | 7.00e+00 | 1.80e-02 | 0.00e+00 |
|  10   |   9   |  14   | +1.6061e-01 | 8.45e+00 | 8.93e-03 | 0.00e+00 |
|  11   |  10   |  15   | +1.3075e-01 | 8.45e+00 | 3.06e-03 | 0.00e+00 |
|  12   |  10   |  15   | +1.3075e-01 | 4.23e+01 | 6.45e-03 | 0.00e+00 |
|  13   |  11   |  16   | +6.4115e-02 | 4.23e+01 | 4.02e-03 | 0.00e+00 |
|  14   |  12   |  19   | +6.4115e-02 | 4.23e+00 | 4.02e-03 | 0.00e+00 |
|  15   |  13   |  20   | +4.6501e-02 | 8.68e+00 | 6.50e-03 | 0.00e+00 |
|  16   |  14   |  21   | +4.6501e-02 | 8.68e-01 | 6.50e-03 | 0.00e+00 |
|  17   |  15   |  22   | +3.2686e-02 | 1.74e+00 | 3.27e-03 | 0.00e+00 |
|  18   |  16   |  24   | +3.2686e-02 | 8.68e-01 | 3.27e-03 | 0.00e+00 |
|  19   |  17   |  26   | +2.4573e-02 | 1.74e+00 | 1.41e-03 | 0.00e+00 |
|  20   |  17   |  26   | +2.4573e-02 | 8.68e+00 | 9.45e-04 | 0.00e+00 |
|  21   |  18   |  28   | +1.0532e-02 | 1.06e+01 | 1.83e-03 | 0.00e+00 |
|  22   |  19   |  30   | +3.9339e-03 | 1.06e+01 | 1.72e-04 | 0.00e+00 |
|  23   |  19   |  30   | +3.9339e-03 | 5.28e+01 | 1.67e-04 | 0.00e+00 |
|  24   |  20   |  31   | +1.3329e-03 | 5.28e+01 | 9.36e-05 | 0.00e+00 |
|  25   |  21   |  32   | +7.8032e-04 | 5.28e+01 | 1.08e-05 | 0.00e+00 |
|  26   |  21   |  32   | +7.8032e-04 | 2.64e+02 | 1.17e-05 | 0.00e+00 |
|  27   |  22   |  33   | +3.0700e-04 | 2.64e+02 | 5.80e-06 | 0.00e+00 |
|  28   |  23   |  34   | +1.9469e-04 | 2.64e+02 | 3.22e-07 | 0.00e+00 |
|  29   |  23   |  34   | +1.9469e-04 | 1.32e+03 | 6.81e-07 | 0.00e+00 |
|  30   |  24   |  35   | +9.1505e-05 | 1.32e+03 | 8.97e-07 | 0.00e+00 |
|  31   |  25   |  36   | +6.5881e-05 | 1.32e+03 | 1.97e-07 | 0.00e+00 |
|  32   |  25   |  36   | +6.5881e-05 | 6.59e+03 | 2.71e-07 | 0.00e+00 |
|  33   |  26   |  38   | +4.1106e-05 | 6.59e+03 | 3.60e-08 | 0.00e+00 |
|  34   |  27   |  40   | +3.6148e-05 | 6.59e+03 | 6.47e-09 | 0.00e+00 |

`gtol` termination condition is satisfied.
Number of iterations: 34, function evaluations: 27, CG iterations: 40, optimality: 6.47e-09, constraint violation: 0.00e+00, execution time: 0.029 s.
.Eps          norm(jv)                  norm(jv_fd)               Abs. Errors              
FMethod trust-constr does not currently support gradient checking
| niter |f evals|CG iter|  obj func   |tr radius |   opt    |  c viol  | penalty  |barrier param|CG stop|
|-------|-------|-------|-------------|----------|----------|----------|----------|-------------|-------|
|   1   |   1   |   0   | +1.3875e-01 | 1.00e+00 | 1.01e-01 | 0.00e+00 | 1.00e+00 |  1.00e-01   |   0   |
|   2   |   2   |   1   | +1.4566e-01 | 7.00e+00 | 1.03e-01 | 0.00e+00 | 1.00e+00 |  1.00e-01   |   2   |
|   3   |   3   |   3   | +4.6394e-01 | 3.84e+01 | 4.65e-02 | 0.00e+00 | 1.00e+00 |  1.00e-01   |   1   |
|   4   |   3   |   3   | +4.6394e-01 | 1.92e+02 | 6.58e-02 | 0.00e+00 | 1.00e+00 |  2.00e-02   |   0   |
|   5   |   4   |   5   | +2.4465e-01 | 1.92e+02 | 2.48e-03 | 0.00e+00 | 1.00e+00 |  2.00e-02   |   1   |
|   6   |   4   |   5   | +2.4465e-01 | 9.60e+02 | 2.02e-02 | 0.00e+00 | 1.00e+00 |  4.00e-03   |   0   |
|   7   |   5   |   7   | +1.2709e-01 | 9.60e+02 | 2.78e-03 | 0.00e+00 | 1.00e+00 |  4.00e-03   |   1   |
|   8   |   5   |   7   | +1.2709e-01 | 4.80e+03 | 5.55e-03 | 0.00e+00 | 1.00e+00 |  8.00e-04   |   0   |
|   9   |   6   |   9   | +9.2813e-02 | 4.80e+03 | 6.34e-04 | 0.00e+00 | 1.00e+00 |  8.00e-04   |   1   |
|  10   |   7   |  11   | +8.0069e-02 | 4.80e+03 | 7.93e-05 | 0.00e+00 | 1.00e+00 |  8.00e-04   |   1   |
|  11   |   7   |  11   | +8.0069e-02 | 2.40e+04 | 2.00e-04 | 0.00e+00 | 1.00e+00 |  1.60e-04   |   0   |
|  12   |   8   |  13   | +7.2800e-02 | 2.40e+04 | 3.84e-05 | 0.00e+00 | 1.00e+00 |  1.60e-04   |   1   |
|  13   |   9   |  15   | +7.1423e-02 | 2.40e+04 | 7.00e-06 | 0.00e+00 | 1.00e+00 |  1.60e-04   |   1   |
|  14   |   9   |  15   | +7.1423e-02 | 1.20e+05 | 2.78e-05 | 0.00e+00 | 1.00e+00 |  3.20e-05   |   0   |
|  15   |  10   |  17   | +7.0421e-02 | 1.20e+05 | 1.44e-06 | 0.00e+00 | 1.00e+00 |  3.20e-05   |   1   |
|  16   |  10   |  17   | +7.0421e-02 | 6.00e+05 | 1.73e-06 | 0.00e+00 | 1.00e+00 |  6.40e-06   |   0   |
|  17   |  11   |  19   | +7.0348e-02 | 6.00e+05 | 1.96e-08 | 0.00e+00 | 1.00e+00 |  6.40e-06   |   1   |
|  18   |  11   |  19   | +7.0348e-02 | 3.00e+06 | 1.02e-08 | 0.00e+00 | 1.00e+00 |  1.28e-06   |   0   |
|  19   |  12   |  21   | +7.0337e-02 | 3.00e+06 | 4.16e-11 | 0.00e+00 | 1.00e+00 |  1.28e-06   |   1   |
|  20   |  12   |  21   | +7.0337e-02 | 1.50e+07 | 3.92e-09 | 0.00e+00 | 1.00e+00 |  2.56e-07   |   0   |
|  21   |  13   |  23   | +7.0335e-02 | 1.50e+07 | 1.37e-11 | 0.00e+00 | 1.00e+00 |  2.56e-07   |   1   |
|  22   |  13   |  23   | +7.0335e-02 | 7.50e+07 | 8.81e-10 | 0.00e+00 | 1.00e+00 |  5.12e-08   |   0   |
|  23   |  14   |  25   | +7.0334e-02 | 7.50e+07 | 5.83e-13 | 0.00e+00 | 1.00e+00 |  5.12e-08   |   1   |
|  24   |  14   |  25   | +7.0334e-02 | 3.75e+08 | 1.77e-10 | 0.00e+00 | 1.00e+00 |  1.02e-08   |   0   |
|  25   |  15   |  27   | +7.0334e-02 | 3.75e+08 | 2.32e-14 | 0.00e+00 | 1.00e+00 |  1.02e-08   |   1   |
|  26   |  15   |  27   | +7.0334e-02 | 1.88e+09 | 3.54e-11 | 0.00e+00 | 1.00e+00 |  2.05e-09   |   0   |
|  27   |  16   |  29   | +7.0334e-02 | 1.88e+09 | 9.30e-16 | 0.00e+00 | 1.00e+00 |  2.05e-09   |   1   |

`gtol` termination condition is satisfied.
Number of iterations: 27, function evaluations: 16, CG iterations: 29, optimality: 9.30e-16, constraint violation: 0.00e+00, execution time: 0.026 s.
[-9.73542407e-02 -9.70816943e-02 -8.85669204e-02 -5.11283893e-02
 -4.85606707e-02 -3.72980207e-02 -3.25240692e-02 -2.99636848e-02
 -1.90446348e-02 -8.22014364e-03  3.48343606e-05 -1.86137722e-03
 -1.91838384e-03 -2.29931374e-03 -2.01036769e-02 -2.47793180e-02
 -6.98438152e-02 -7.57912549e-02 -8.48871938e-02 -9.56022332e-02]
.
pyapprox/pde/tests/test_karhunen_loeve.py ..
pyapprox/pde/tests/test_spectral_diffusion.py pi**2*x**2*sin(pi*y) + 2*x*sin(pi*y) - 2*sin(pi*y)
.pi**2*y**2*sin(pi*x) + pi*y**2*cos(pi*x) - 2*sin(pi*x)
.s...s[[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]] d
[[-2.32638855e-16]
 [ 5.44051410e-03]
 [ 2.10083867e-02]
 [ 4.45860179e-02]
 [ 7.30794715e-02]
 [ 1.02942009e-01]
 [ 1.30705733e-01]
 [ 1.53418763e-01]
 [ 1.68933634e-01]
 [ 1.76039375e-01]
 [ 1.74460831e-01]
 [ 1.64761936e-01]
 [ 1.48189831e-01]
 [ 1.26490198e-01]
 [ 1.01715900e-01]
 [ 7.60435425e-02]
 [ 5.16069046e-02]
 [ 3.03523718e-02]
 [ 1.39190695e-02]
 [ 3.54503375e-03]
 [ 0.00000000e+00]] f
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.015129340538073103      0.033691298032298464      1.226884770523477        
0.1          0.015129340538073103      0.0183104216318572        0.21025907148952594      
0.01         0.015129340538073103      0.015457819509420712      0.021711387255841617     
0.001        0.015129340538073103      0.01516228381920337       0.002177443296181031     
0.0001       0.015129340538073103      0.015132635808323691      0.00021780660183405492   
1e-05        0.015129340538073103      0.015129669994617954      2.177600167183473e-05    
1e-06        0.015129340538073103      0.015129371672140124      2.05786015208791e-06     
1e-07        0.015129340538073103      0.015129344999031957      2.94854811604212e-07     
1e-08        0.015129340538073103      0.015129258956747549      5.392259189956104e-06    
1e-09        0.015129340538073103      0.01512853731178154       5.309063468702826e-05    
1e-10        0.015129340538073103      0.015115131368759194      0.0009391796871880855    
1e-11        0.015129340538073103      0.015044909762451654      0.005580598533622681     
1e-12        0.015129340538073103      0.014557799410397365      0.0377770019940691       
1e-13        0.015129340538073103      0.010685896612017132      0.29369713206428333      
.[[1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]
 [1.]] d
[[ 0.00000000e+00]
 [-5.56972060e-15]
 [-3.57265260e-15]
 [ 6.39390376e-16]
 [-3.37454613e-14]
 [-3.03415243e-14]
 [ 1.58039741e-14]
 [-1.90520699e-14]
 [-2.69024230e-14]
 [-1.04538026e-13]
 [-6.22082906e-14]
 [ 4.74682083e-14]
 [-1.30195577e-13]
 [-2.95186374e-15]
 [ 1.87230116e-14]
 [-7.78362962e-14]
 [-1.38565314e-14]
 [-2.51909558e-14]
 [ 1.02447800e-14]
 [ 6.63286494e-15]
 [ 0.00000000e+00]
 [ 1.91698030e-14]
 [ 2.13653572e-04]
 [ 6.66099663e-04]
 [ 1.17677298e-03]
 [ 1.66096071e-03]
 [ 2.07105307e-03]
 [ 2.38370160e-03]
 [ 2.59150270e-03]
 [ 2.69774420e-03]
 [ 2.71138922e-03]
 [ 2.64365032e-03]
 [ 2.50551102e-03]
 [ 2.30679772e-03]
 [ 2.05613121e-03]
 [ 1.76192839e-03]
 [ 1.43363211e-03]
 [ 1.08359581e-03]
 [ 7.28851304e-04]
 [ 3.94887966e-04]
 [ 1.22437178e-04]
 [ 1.51858877e-13]
 [-1.91887490e-14]
 [ 6.66099663e-04]
 [ 2.29897938e-03]
 [ 4.26964167e-03]
 [ 6.17328486e-03]
 [ 7.80014728e-03]
 [ 9.04922347e-03]
 [ 9.88791272e-03]
 [ 1.03267082e-02]
 [ 1.03996856e-02]
 [ 1.01500473e-02]
 [ 9.62069906e-03]
 [ 8.85007860e-03]
 [ 7.87231333e-03]
 [ 6.72076155e-03]
 [ 5.43351510e-03]
 [ 4.06055713e-03]
 [ 2.67347093e-03]
 [ 1.38416716e-03]
 [ 3.90769771e-04]
 [ 1.36616425e-14]
 [-2.37445680e-15]
 [ 1.17677298e-03]
 [ 4.26964167e-03]
 [ 8.29887016e-03]
 [ 1.23665798e-02]
 [ 1.59223239e-02]
 [ 1.86946088e-02]
 [ 2.05875910e-02]
 [ 2.16112215e-02]
 [ 2.18328630e-02]
 [ 2.13429750e-02]
 [ 2.02334886e-02]
 [ 1.85879460e-02]
 [ 1.64818551e-02]
 [ 1.39906361e-02]
 [ 1.12030628e-02]
 [ 8.23997468e-03]
 [ 5.28347074e-03]
 [ 2.62595555e-03]
 [ 7.08358537e-04]
 [-3.29660692e-14]
 [ 2.78570799e-15]
 [ 1.66096071e-03]
 [ 6.17328486e-03]
 [ 1.23665798e-02]
 [ 1.89281695e-02]
 [ 2.48628925e-02]
 [ 2.96041052e-02]
 [ 3.29171670e-02]
 [ 3.47777434e-02]
 [ 3.52774479e-02]
 [ 3.45579519e-02]
 [ 3.27700433e-02]
 [ 3.00549907e-02]
 [ 2.65447956e-02]
 [ 2.23771639e-02]
 [ 1.77220018e-02]
 [ 1.28196768e-02]
 [ 8.03245271e-03]
 [ 3.88739187e-03]
 [ 1.02627465e-03]
 [-9.02831890e-14]
 [-8.98917904e-15]
 [ 2.07105307e-03]
 [ 7.80014728e-03]
 [ 1.59223239e-02]
 [ 2.48628925e-02]
 [ 3.32384072e-02]
 [ 4.01277688e-02]
 [ 4.50747937e-02]
 [ 4.79622626e-02]
 [ 4.88748989e-02]
 [ 4.79934101e-02]
 [ 4.55268615e-02]
 [ 4.16813040e-02]
 [ 3.66594729e-02]
 [ 3.06850668e-02]
 [ 2.40456336e-02]
 [ 1.71476777e-02]
 [ 1.05652627e-02]
 [ 5.03115140e-03]
 [ 1.31297350e-03]
 [ 3.17199681e-14]
 [ 3.00690195e-14]
 [ 2.38370160e-03]
 [ 9.04922347e-03]
 [ 1.86946088e-02]
 [ 2.96041052e-02]
 [ 4.01277688e-02]
 [ 4.90303198e-02]
 [ 5.56013221e-02]
 [ 5.95777435e-02]
 [ 6.09947492e-02]
 [ 6.00461245e-02]
 [ 5.69876103e-02]
 [ 5.20899261e-02]
 [ 4.56367483e-02]
 [ 3.79577224e-02]
 [ 2.94831829e-02]
 [ 2.07998628e-02]
 [ 1.26708684e-02]
 [ 5.97426551e-03]
 [ 1.54873518e-03]
 [-5.56416484e-14]
 [ 1.59753919e-14]
 [ 2.59150270e-03]
 [ 9.88791272e-03]
 [ 2.05875910e-02]
 [ 3.29171670e-02]
 [ 4.50747937e-02]
 [ 5.56013221e-02]
 [ 6.35620764e-02]
 [ 6.85319789e-02]
 [ 7.04647034e-02]
 [ 6.95384906e-02]
 [ 6.60369159e-02]
 [ 6.02868862e-02]
 [ 5.26531621e-02]
 [ 4.35754792e-02]
 [ 3.36247197e-02]
 [ 2.35443716e-02]
 [ 1.42372359e-02]
 [ 6.67167240e-03]
 [ 1.72257700e-03]
 [-1.22621554e-14]
 [ 8.20116785e-15]
 [ 2.69774420e-03]
 [ 1.03267082e-02]
 [ 2.16112215e-02]
 [ 3.47777434e-02]
 [ 4.79622626e-02]
 [ 5.95777435e-02]
 [ 6.85319789e-02]
 [ 7.42622286e-02]
 [ 7.66346391e-02]
 [ 7.57932284e-02]
 [ 7.20296864e-02]
 [ 6.57106546e-02]
 [ 5.72676420e-02]
 [ 4.72326480e-02]
 [ 3.62875638e-02]
 [ 2.52872492e-02]
 [ 1.52215327e-02]
 [ 7.10654277e-03]
 [ 1.83047728e-03]
 [-1.44662201e-14]
 [ 2.35173112e-14]
 [ 2.71138922e-03]
 [ 1.03996856e-02]
 [ 2.18328630e-02]
 [ 3.52774479e-02]
 [ 4.88748989e-02]
 [ 6.09947492e-02]
 [ 7.04647034e-02]
 [ 7.66346391e-02]
 [ 7.93024568e-02]
 [ 7.85746687e-02]
 [ 7.47355094e-02]
 [ 6.81694691e-02]
 [ 5.93472341e-02]
 [ 4.88570262e-02]
 [ 3.74453354e-02]
 [ 2.60258345e-02]
 [ 1.56276146e-02]
 [ 7.28168172e-03]
 [ 1.87319589e-03]
 [-1.99331927e-14]
 [ 1.14362408e-14]
 [ 2.64365032e-03]
 [ 1.01500473e-02]
 [ 2.13429750e-02]
 [ 3.45579519e-02]
 [ 4.79934101e-02]
 [ 6.00461245e-02]
 [ 6.95384906e-02]
 [ 7.57932284e-02]
 [ 7.85746687e-02]
 [ 7.79611253e-02]
 [ 7.42194618e-02]
 [ 6.77284594e-02]
 [ 5.89631253e-02]
 [ 4.85225560e-02]
 [ 3.71651889e-02]
 [ 2.58113380e-02]
 [ 1.54871757e-02]
 [ 7.21178971e-03]
 [ 1.85447512e-03]
 [-5.16253743e-14]
 [ 9.13990410e-15]
 [ 2.50551102e-03]
 [ 9.62069906e-03]
 [ 2.02334886e-02]
 [ 3.27700433e-02]
 [ 4.55268615e-02]
 [ 5.69876103e-02]
 [ 6.60369159e-02]
 [ 7.20296864e-02]
 [ 7.47355094e-02]
 [ 7.42194618e-02]
 [ 7.07254289e-02]
 [ 6.46048220e-02]
 [ 5.63021009e-02]
 [ 4.63815568e-02]
 [ 3.55620768e-02]
 [ 2.47216771e-02]
 [ 1.48455505e-02]
 [ 6.91733857e-03]
 [ 1.77945080e-03]
 [ 4.22842072e-14]
 [ 2.10698758e-15]
 [ 2.30679772e-03]
 [ 8.85007860e-03]
 [ 1.85879460e-02]
 [ 3.00549907e-02]
 [ 4.16813040e-02]
 [ 5.20899261e-02]
 [ 6.02868862e-02]
 [ 6.57106546e-02]
 [ 6.81694691e-02]
 [ 6.77284594e-02]
 [ 6.46048220e-02]
 [ 5.91055545e-02]
 [ 5.16161384e-02]
 [ 4.26275284e-02]
 [ 3.27739244e-02]
 [ 2.28464753e-02]
 [ 1.37536276e-02]
 [ 6.42109726e-03]
 [ 1.65385085e-03]
 [ 2.70713786e-14]
 [-2.00758110e-14]
 [ 2.05613121e-03]
 [ 7.87231333e-03]
 [ 1.64818551e-02]
 [ 2.65447956e-02]
 [ 3.66594729e-02]
 [ 4.56367483e-02]
 [ 5.26531621e-02]
 [ 5.72676420e-02]
 [ 5.93472341e-02]
 [ 5.89631253e-02]
 [ 5.63021009e-02]
 [ 5.16161384e-02]
 [ 4.52145789e-02]
 [ 3.74899136e-02]
 [ 2.89579758e-02]
 [ 2.02845333e-02]
 [ 1.22663460e-02]
 [ 5.74740088e-03]
 [ 1.48375144e-03]
 [ 2.06749424e-14]
 [ 2.74661200e-15]
 [ 1.76192839e-03]
 [ 6.72076155e-03]
 [ 1.39906361e-02]
 [ 2.23771639e-02]
 [ 3.06850668e-02]
 [ 3.79577224e-02]
 [ 4.35754792e-02]
 [ 4.72326480e-02]
 [ 4.88570262e-02]
 [ 4.85225560e-02]
 [ 4.63815568e-02]
 [ 4.26275284e-02]
 [ 3.74899136e-02]
 [ 3.12553969e-02]
 [ 2.43052705e-02]
 [ 1.71519840e-02]
 [ 1.04466510e-02]
 [ 4.92381855e-03]
 [ 1.27605256e-03]
 [ 2.02378882e-15]
 [-4.13555895e-15]
 [ 1.43363211e-03]
 [ 5.43351510e-03]
 [ 1.12030628e-02]
 [ 1.77220018e-02]
 [ 2.40456336e-02]
 [ 2.94831829e-02]
 [ 3.36247197e-02]
 [ 3.62875638e-02]
 [ 3.74453354e-02]
 [ 3.71651889e-02]
 [ 3.55620768e-02]
 [ 3.27739244e-02]
 [ 2.89579758e-02]
 [ 2.43052705e-02]
 [ 1.90686659e-02]
 [ 1.35987480e-02]
 [ 8.37339518e-03]
 [ 3.98424835e-03]
 [ 1.03914591e-03]
 [ 1.70173592e-15]
 [-2.30881838e-15]
 [ 1.08359581e-03]
 [ 4.06055713e-03]
 [ 8.23997468e-03]
 [ 1.28196768e-02]
 [ 1.71476777e-02]
 [ 2.07998628e-02]
 [ 2.35443716e-02]
 [ 2.52872492e-02]
 [ 2.60258345e-02]
 [ 2.58113380e-02]
 [ 2.47216771e-02]
 [ 2.28464753e-02]
 [ 2.02845333e-02]
 [ 1.71519840e-02]
 [ 1.35987480e-02]
 [ 9.83278818e-03]
 [ 6.15265885e-03]
 [ 2.97304443e-03]
 [ 7.83989430e-04]
 [ 4.36110774e-16]
 [ 2.09753426e-15]
 [ 7.28851304e-04]
 [ 2.67347093e-03]
 [ 5.28347074e-03]
 [ 8.03245271e-03]
 [ 1.05652627e-02]
 [ 1.26708684e-02]
 [ 1.42372359e-02]
 [ 1.52215327e-02]
 [ 1.56276146e-02]
 [ 1.54871757e-02]
 [ 1.48455505e-02]
 [ 1.37536276e-02]
 [ 1.22663460e-02]
 [ 1.04466510e-02]
 [ 8.37339518e-03]
 [ 6.15265885e-03]
 [ 3.93579292e-03]
 [ 1.95066882e-03]
 [ 5.25066374e-04]
 [-4.44526222e-17]
 [-3.39616960e-16]
 [ 3.94887966e-04]
 [ 1.38416716e-03]
 [ 2.62595555e-03]
 [ 3.88739187e-03]
 [ 5.03115140e-03]
 [ 5.97426551e-03]
 [ 6.67167240e-03]
 [ 7.10654277e-03]
 [ 7.28168172e-03]
 [ 7.21178971e-03]
 [ 6.91733857e-03]
 [ 6.42109726e-03]
 [ 5.74740088e-03]
 [ 4.92381855e-03]
 [ 3.98424835e-03]
 [ 2.97304443e-03]
 [ 1.95066882e-03]
 [ 1.00526061e-03]
 [ 2.82621634e-04]
 [ 1.44004328e-15]
 [ 3.14653339e-15]
 [ 1.22437177e-04]
 [ 3.90769771e-04]
 [ 7.08358537e-04]
 [ 1.02627465e-03]
 [ 1.31297350e-03]
 [ 1.54873518e-03]
 [ 1.72257700e-03]
 [ 1.83047728e-03]
 [ 1.87319589e-03]
 [ 1.85447512e-03]
 [ 1.77945080e-03]
 [ 1.65385085e-03]
 [ 1.48375144e-03]
 [ 1.27605256e-03]
 [ 1.03914591e-03]
 [ 7.83989430e-04]
 [ 5.25066374e-04]
 [ 2.82621634e-04]
 [ 8.69358087e-05]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]
 [ 0.00000000e+00]] f
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.007899838889284265      0.015311792347938655      0.9382410910567217       
0.1          0.007899838889284265      0.009485973685107452      0.20078065110602428      
0.01         0.007899838889284265      0.008065984865456705      0.02103156513708247      
0.001        0.007899838889284265      0.007916523348701354      0.0021119999598625387    
0.0001       0.007899838889284265      0.007901507990265566      0.00021128291408129595   
1e-05        0.007899838889284265      0.007900005516425779      2.109247338441925e-05    
1e-06        0.007899838889284265      0.007899852529080764      1.7265917305433166e-06   
1e-07        0.007899838889284265      0.00789973787773679       1.2786532597824686e-05   
1e-08        0.007899838889284265      0.007898655340898841      0.00014981930669869572   
1e-09        0.007899838889284265      0.00788849541244474       0.00143591242789923      
1e-10        0.007899838889284265      0.007790781908489919      0.013804962648323985     
1e-11        0.007899838889284265      0.0065003558091802915     0.17715336979875657      
1e-12        0.007899838889284265      0.0028449465006019636     1.3601271545500746       
1e-13        0.007899838889284265      0.023869795029440866      4.021554662566479        
.1.1474775736713466e-13
.....8.422300329083119e-16
.0.3333333333333335
0.666666666666667
.......
pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py 0.000684019052401952
F0.0006715345407376053
F.Worst case relative interpolation error 1.126167833991441e-06
Median relative interpolation error 5.630839558493703e-07
[[-1.86807927e-08 -3.05041210e-15]
 [ 1.59231971e-09  1.45161660e-14]
 [ 5.41798495e-10 -1.57374114e-14]
 [-1.15138147e-08  2.46469511e-14]
 [-2.05566246e-08 -1.52100554e-14]
 [ 2.41127125e-08 -3.32440781e-15]
 [ 2.55507859e-09  1.88737914e-15]
 [-2.57138294e-09  3.45556916e-15]
 [ 8.43204229e-10 -8.43769499e-15]
 [ 1.12657687e-08  3.21964677e-15]
 [ 2.02055411e-09 -3.80251386e-15]
 [-9.95862293e-09 -4.44089210e-15]
 [-1.03061952e-08  6.66133815e-16]
 [-5.48197909e-09  3.44993131e-16]
 [-1.37810952e-09  3.10862447e-15]
 [ 1.39492441e-08 -8.32667268e-16]
 [ 2.27731195e-08 -1.99840144e-15]]
.1**2 * Matern(length_scale=[2.18, 1.92, 1.75, 1.64, 2.11, 1.84, 1.72, 1.6, 1.97, 10], nu=inf)
316**2 * Matern(length_scale=[0.4, 0.000115, 0.16, 0.08, 4, 6, 0.4, 1.2e+03, 0.8, 2.3e-06], nu=inf)
error 0.062341186013075846
std of realizations error 0.0022902331200145465
mean of realizations error 0.00022652256032957096
.FExplained variance 1.0000000000000002
FF0.010313960326154508
F0.011970043182373047
0.1827259063720703
[-1.93970369e-02  1.80609913e-03 -2.15924921e-04  3.63022263e-03
 -1.47681634e-06  3.62766993e-03 -9.07126477e-09  3.62766787e-03
 -5.37175256e-11  3.62766786e-03  5.37177019e-11  3.62766786e-03
  9.07126475e-09  3.62766787e-03  1.47681634e-06  3.62766993e-03
  2.15924921e-04  3.63022263e-03  1.93970369e-02  1.80609913e-03] 

 [-1.93970427e-02  1.80610269e-03 -2.15925276e-04  3.63022834e-03
 -1.48266554e-06  3.62766534e-03 -7.45058060e-09  3.62767279e-03
  0.00000000e+00  3.62766534e-03 -7.45058060e-09  3.62765789e-03
  7.45058060e-09  3.62766534e-03  1.47521496e-06  3.62765789e-03
  2.15925276e-04  3.63022834e-03  1.93970352e-02  1.80610269e-03]
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.8662866328130254        0.8989023659421616        0.5183473048730979       
0.1          1.8662866328130254        8.988988418819707         3.8165103806476846       
0.01         1.8662866328130254        89.88609517077415         47.163071025344635       
0.001        1.8662866328130254        898.850307680492          480.625004628952         
0.0001       1.8662866328130254        8988.491125927341         4815.243640381854        
1e-05        1.8662866328130254        89884.899167961           48161.429922657175       
1e-06        1.8662866328130254        898848.9795741518         481623.2927378302        
1e-07        1.8662866328130254        8988489.783634646         4816241.920888803        
1e-08        1.8662866328130254        89884897.82423942         48162428.20239844        
1e-09        1.8662866328130254        898848978.2302872         481624291.0174948        
1e-10        1.8662866328130254        8988489782.290766         4816242919.16846         
1e-11        1.8662866328130254        89884897822.89555         48162429200.6781         
1e-12        1.8662866328130254        898848978228.9435         481624292015.77454       
1e-13        1.8662866328130254        8988489782289.42          4816242920166.737        
F......FIter: 0, Objective: -0.4463363946185806
Iter: 1, Objective: -0.5751230638106184
Iter: 2, Objective: -0.6949952109573054
Iter: 3, Objective: -0.7646940661965688
Iter: 4, Objective: -0.8284265282904557
Iter: 5, Objective: -0.8665239252673373
Iter: 6, Objective: -0.8936305060872113
Iter: 7, Objective: -0.9165171119354276
Iter: 8, Objective: -0.9361062037979335
Iter: 9, Objective: -0.9512152135065829
Iter: 10, Objective: -0.960102037578836
Iter: 11, Objective: -0.965944431097828
Iter: 12, Objective: -0.9714658427926708
Iter: 13, Objective: -0.9768994899410236
Iter: 14, Objective: -0.9818374750631028
Iter: 15, Objective: -0.9848465164828895
Iter: 16, Objective: -0.9872884491404115
Iter: 17, Objective: -0.989316273495281
Iter: 18, Objective: -0.9910582688794451
Iter: 19, Objective: -0.9926049925719234
0.010213136672973633
Iter: 0, Objective: -0.4463363946185806
Iter: 1, Objective: -0.5751230638106182
Iter: 2, Objective: -0.6949952109573048
Iter: 3, Objective: -0.7646940661965689
Iter: 4, Objective: -0.8284265282904552
Iter: 5, Objective: -0.8665239252673365
Iter: 6, Objective: -0.8936305060872105
Iter: 7, Objective: -0.9165171119354238
Iter: 8, Objective: -0.9361062037979287
Iter: 9, Objective: -0.9512152135065817
Iter: 10, Objective: -0.9601020375788422
Iter: 11, Objective: -0.9659444310978252
Iter: 12, Objective: -0.971465842792672
Iter: 13, Objective: -0.9768994899410243
Iter: 14, Objective: -0.9818374750631018
Iter: 15, Objective: -0.9848465164828155
Iter: 16, Objective: -0.9872884491403715
Iter: 17, Objective: -0.9893162734952307
Iter: 18, Objective: -0.9910582688794586
Iter: 19, Objective: -0.9926049925718515
0.010213136672973633 1.0120270252227783
Iter: 0, Objective: -0.3802864076504844
Iter: 1, Objective: -0.4902798847257755
Iter: 2, Objective: -0.5882458820465379
Iter: 3, Objective: -0.6760281975167288
Iter: 4, Objective: -0.7473673572896082
Iter: 5, Objective: -0.7901557042733667
Iter: 6, Objective: -0.8173143800037314
Iter: 7, Objective: -0.8392352130095941
Iter: 8, Objective: -0.856180440403137
Iter: 9, Objective: -0.8718667374096365
Iter: 10, Objective: -0.8845819335314703
Iter: 11, Objective: -0.896080883557261
Iter: 12, Objective: -0.9055067621384998
Iter: 13, Objective: -0.9136806712737768
Iter: 14, Objective: -0.9209353683633794
Iter: 15, Objective: -0.9274024173219022
Iter: 16, Objective: -0.9337401435454646
Iter: 17, Objective: -0.9394053338495614
Iter: 18, Objective: -0.9442293769609416
Iter: 19, Objective: -0.9486908199771682
0.019546031951904297
Iter: 0, Objective: -0.3802864076504844
Iter: 1, Objective: -0.49027988472577544
Iter: 2, Objective: -0.5882458820465379
Iter: 3, Objective: -0.6760281975167288
Iter: 4, Objective: -0.7473673572896081
Iter: 5, Objective: -0.7901557042733665
Iter: 6, Objective: -0.8173143800037308
Iter: 7, Objective: -0.8392352130095941
Iter: 8, Objective: -0.8561804404031366
Iter: 9, Objective: -0.8718667374096367
Iter: 10, Objective: -0.8845819335314709
Iter: 11, Objective: -0.8960808835572613
Iter: 12, Objective: -0.9055067621384991
Iter: 13, Objective: -0.9136806712737733
Iter: 14, Objective: -0.9209353683633794
Iter: 15, Objective: -0.9274024173219029
Iter: 16, Objective: -0.9337401435454596
Iter: 17, Objective: -0.9394053338495623
Iter: 18, Objective: -0.9442293769609437
Iter: 19, Objective: -0.9486908199771689
0.019546031951904297 1.202124834060669
.F..Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.007429897223439014      0.0007552596747405529     0.8983485703734981       
0.1          0.007429897223439014      0.0047436645832004114     0.36154371446274847      
0.01         0.007429897223439014      0.007080006059937902      0.04709232886793034      
0.001        0.007429897223439014      0.007393896663687705      0.004845364433540016     
0.0001       0.007429897223439014      0.007426286816561867      0.00048592958537260685   
1e-05        0.007429897223439014      0.007429536079081068      4.8606911655149567e-05   
1e-06        0.007429897223439014      0.007429861108082178      4.860815129752138e-06    
1e-07        0.007429897223439014      0.007429893622004288      4.847220113087251e-07    
1e-08        0.007429897223439014      0.007429896910389477      4.213376408846712e-08    
1e-09        0.007429897223439014      0.007429897734383128      6.876866503560986e-08    
1e-10        0.007429897223439014      0.007429900878569429      4.919489872554644e-07    
1e-11        0.007429897223439014      0.007429918225804189      2.8267369713893907e-06   
1e-12        0.007429897223439014      0.007429820647608665      1.0306445438824026e-05   
1e-13        0.007429897223439014      0.007432205892388133      0.0003107269023635392    
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.013324842109866052      0.0010863167366819797     1.0815256742049981       
0.1          0.013324842109866052      0.021859504386304355      0.6405075727027957       
0.01         0.013324842109866052      0.014033249103362885      0.05316438181074662      
0.001        0.013324842109866052      0.013394115691069478      0.0051988294219361625    
0.0001       0.013324842109866052      0.013331753824768339      0.0005187089531942098    
1e-05        0.013324842109866052      0.013325533124945025      5.1859156999821224e-05   
1e-06        0.013324842109866052      0.01332491121001228       5.185813509719786e-06    
1e-07        0.013324842109866052      0.013324849019308305      5.185383958586659e-07    
1e-08        0.013324842109866052      0.013324842856703156      5.604847683406016e-08    
1e-09        0.013324842109866052      0.01332484231460207       1.5364986420213774e-08   
1e-10        0.013324842109866052      0.01332484036303816       1.3109557906963323e-07   
1e-11        0.013324842109866052      0.013324736279629601      7.94232573852814e-06     
1e-12        0.013324842109866052      0.013323977338108861      6.489921231791308e-05    
1e-13        0.013324842109866052      0.013340023530261647      0.00113933210393194      
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          4.891668891624826e-05     8.059459830930576e-06     0.8352411005428143       
0.1          4.891668891624826e-05     4.640674857779592e-05     0.05131051168957255      
0.01         4.891668891624826e-05     4.9360265643633984e-05    0.009068003931034406     
0.001        4.891668891624826e-05     4.896929597879144e-05     0.0010754420159805643    
0.0001       4.891668891624826e-05     4.892203285180254e-05     0.00010924565159003719   
1e-05        4.891668891624826e-05     4.891722414318528e-05     1.0941601912993693e-05   
1e-06        4.891668891624826e-05     4.891674244554317e-05     1.094295139375065e-06    
1e-07        4.891668891624826e-05     4.891669446620891e-05     1.1345740633840218e-07   
1e-08        4.891668891624826e-05     4.891669040045076e-05     3.034143432766881e-08    
1e-09        4.891668891624826e-05     4.891667955842904e-05     1.9130115770095353e-07   
1e-10        4.891668891624826e-05     4.891667786436314e-05     2.2593281270542576e-07   
1e-11        4.891668891624826e-05     4.8918016176419804e-05    2.7133074640827643e-05   
1e-12        4.891668891624826e-05     4.891954083572486e-05     5.8301564144852655e-05   
1e-13        4.891668891624826e-05     4.882297907973787e-05     0.001915702771110065     
.FF
pyapprox/surrogates/gaussianprocess/tests/test_gradient_enhanced_gp.py ...total train points (14, 1)
values train points (7, 1)
unique derivs train points (7, 1)
predict
.
pyapprox/surrogates/gaussianprocess/tests/test_multilevel_gp.py sKs1 [[1.         0.60653066 0.13533528]
 [0.60653066 1.         0.60653066]
 [0.13533528 0.60653066 1.        ]]
[1, 2, 2]
[[2.         1.76499381 1.21306132 0.64930493 0.27067057 5.         3.30861954 1.14787179]
 [1.76499381 2.         1.76499381 1.21306132 0.64930493 4.49922084 4.49922084 2.05344947]
 [1.21306132 1.76499381 2.         1.76499381 1.21306132 3.30861954 5.         3.30861954]
 [0.64930493 1.21306132 1.76499381 2.         1.76499381 2.05344947 4.49922084 4.49922084]
 [0.27067057 0.64930493 1.21306132 1.76499381 2.         1.14787179 3.30861954 5.        ]]
.
pyapprox/surrogates/interp/tests/test_barycentric_interpolation.py [ 8.02445071e-15 -4.59668010e-12  6.78320839e-10 -4.71940228e-08  1.96590248e-06 -5.56126649e-05  1.15378318e-03 -1.84813180e-02  2.37061358e-01 -2.50203393e+00  2.21864612e+01 -1.68032082e+02  1.10144840e+03 -6.31717348e+03  3.19881381e+04 -1.44097815e+05  5.81195831e+05 -2.11043539e+06  6.93200076e+06 -2.06801791e+07  5.62333593e+07 -1.39800729e+08  3.18607561e+08 -6.67168159e+08  1.28621033e+09 -2.28680358e+09  3.75507149e+09 -5.70179183e+09  8.01394383e+09 -1.04345502e+10  1.25939463e+10
 -1.40961242e+10  1.46352416e+10 -1.40961242e+10  1.25939463e+10 -1.04345502e+10  8.01394383e+09 -5.70179183e+09  3.75507149e+09 -2.28680358e+09  1.28621033e+09 -6.67168159e+08  3.18607561e+08 -1.39800729e+08  5.62333593e+07 -2.06801791e+07  6.93200076e+06 -2.11043539e+06  5.81195831e+05 -1.44097815e+05  3.19881381e+04 -6.31717348e+03  1.10144840e+03 -1.68032082e+02  2.21864612e+01 -2.50203393e+00  2.37061358e-01 -1.84813180e-02  1.15378318e-03 -5.56126649e-05  1.96590248e-06 -4.71940228e-08
  6.78320839e-10 -4.59668010e-12  8.02445071e-15]
14635241561.906559 8.024450709373004e-15
1.823830950174793e+24
....
pyapprox/surrogates/interp/tests/test_mixture_model.py .todo: replace with exact analytical integral
.
pyapprox/surrogates/interp/tests/test_sparse_grid.py .............[2 2] 2.220446049250313e-16
Accuracy misleadingly appears reached because admissibility  criterion is preventing new subspaces from being added to the active set
[1 3] 2.220446049250313e-16
Accuracy misleadingly appears reached because admissibility  criterion is preventing new subspaces from being added to the active set
................
pyapprox/surrogates/interp/tests/test_tensorprod.py .0.000533333333335051
0.00016666666667219943
2.6645352591003757e-15
....
pyapprox/surrogates/orthopoly/tests/test_leja_sequences.py Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.2826561654124786        0.40938385636658237       2.4483457516986076       
0.1          0.2826561654124786        0.3454862764156069        0.22228459411610801      
0.01         0.2826561654124786        0.29007083175854875       0.026232105481406885     
0.001        0.2826561654124786        0.28340546955418766       0.002650938608098585     
0.0001       0.2826561654124786        0.2827311714292957        0.000265361332938328     
1e-05        0.2826561654124786        0.2826636667785376        2.653883755917749e-05    
1e-06        0.2826561654124786        0.2826569155955916        2.6540482917409745e-06   
1e-07        0.2826561654124786        0.28265623952528074       2.6220125787598407e-07   
1e-08        0.2826561654124786        0.28265618179368346       5.795452865194349e-08    
1e-09        0.2826561654124786        0.28265634277602203       6.274886774497489e-07    
1e-10        0.2826561654124786        0.28265723095444173       3.769746050127296e-06    
1e-11        0.2826561654124786        0.282668333184688         4.304796320859663e-05    
1e-12        0.2826561654124786        0.2827182932207961        0.00021979994042170865   
1e-13        0.2826561654124786        0.28255175976710234       0.00036937331695533144   
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.5002877036187479        0.4950566273215642        1.329974461649902        
0.1          1.5002877036187479        1.0880053470611162        0.27480219664747757      
0.01         1.5002877036187479        1.47393836156553          0.017562859436668292     
0.001        1.5002877036187479        1.497765524312411         0.0016811304260197664    
0.0001       1.5002877036187479        1.500036581608688         0.0001673825689926799    
1e-05        1.5002877036187479        1.5002626023608021        1.6730962924779397e-05   
1e-06        1.5002877036187479        1.50028519368961          1.672965213143848e-06    
1e-07        1.5002877036187479        1.500287452604887         1.6731048331770209e-07   
1e-08        1.5002877036187479        1.5002876885272798        1.0059049366872214e-08   
1e-09        1.5002877036187479        1.500287838407388         8.9841861613655e-08      
1e-10        1.5002877036187479        1.5002882269854467        3.4884422341502186e-07   
1e-11        1.5002877036187479        1.5002998843272053        8.118915077456027e-06    
1e-12        1.5002877036187479        1.5005219289321303        0.00015612026467823708   
1e-13        1.5002877036187479        1.500466417780899         0.00011911992727804182   
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          4.58484545385771          0.30047123201278          1.065535738344236        
0.1          4.58484545385771          4.855621189171508         0.059058857716995865     
0.01         4.58484545385771          4.636003483161239         0.011158070608570662     
0.001        4.58484545385771          4.590250159329234         0.001178819553661668     
0.0001       4.58484545385771          4.585388856628847         0.00011852150232885726   
1e-05        4.58484545385771          4.584899823528721         1.1858561331646003e-05   
1e-06        4.58484545385771          4.584850891053982         1.1859061175791164e-06   
1e-07        4.58484545385771          4.584845998412135         1.1877268931842969e-07   
1e-08        4.58484545385771          4.584845519906011         1.4405785694486333e-08   
1e-09        4.58484545385771          4.584845503252666         1.0773526867666958e-08   
1e-10        4.58484545385771          4.5848480567656225        5.677198803133044e-07    
1e-11        4.58484545385771          4.584849166988647         8.098704687679293e-07    
1e-12        4.58484545385771          4.584888024794509         9.285141064679803e-06    
1e-13        4.58484545385771          4.5852210917018965        8.19303176010673e-05     
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          10.874580959596722        3.3485071580895687        1.307920569126348        
0.1          10.874580959596722        3.2382565787563067        0.7022178058365942       
0.01         10.874580959596722        9.915053504027771         0.08823580964949058      
0.001        10.874580959596722        10.776873484481087        0.00898494162475378      
0.0001       10.874580959596722        10.864793118328109        0.0009000660627732531    
1e-05        10.874580959596722        10.873602005201375        9.002226375286283e-05    
1e-06        10.874580959596722        10.874483061584783        9.00246292731214e-06     
1e-07        10.874580959596722        10.874571181318515        8.99186666872575e-07     
1e-08        10.874580959596722        10.874579992048439        8.897338544494903e-08    
1e-09        10.874580959596722        10.874581235498226        2.53712308855627e-08     
1e-10        10.874580959596722        10.87457235371403         7.913760286180926e-07    
1e-11        10.874580959596722        10.874590117282423        8.42118490389218e-07     
1e-12        10.874580959596722        10.873968392388633        5.633018967486665e-05    
1e-13        10.874580959596722        10.871303857129533        0.0003013543675259632    
...Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          6.477231351303263         3.272615826004599         0.4947508204495233       
0.1          6.477231351303263         7.663916639176389         0.1832087235288202       
0.01         6.477231351303263         6.623435364608598         0.02257199185511851      
0.001        6.477231351303263         6.49209855095656          0.0022953016261038463    
0.0001       6.477231351303263         6.478720508537039         0.0002299064450549367    
1e-05        6.477231351303263         6.477380291369793         2.2994402770666578e-05   
1e-06        6.477231351303263         6.477246245362167         2.299448343994634e-06    
1e-07        6.477231351303263         6.477232843415948         2.3036272815298998e-07   
1e-08        6.477231351303263         6.477231506707426         2.3992374965013494e-08   
1e-09        6.477231351303263         6.4772313068672815        6.860335644484653e-09    
1e-10        6.477231351303263         6.477234304469448         4.5593032349798754e-07   
1e-11        6.477231351303263         6.477240965807596         1.484354010481259e-06    
1e-12        6.477231351303263         6.477152147965626         1.2227961815962361e-05   
1e-13        6.477231351303263         6.478151348687788         0.00014203559123152835   
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.20648401356917206       0.4413690011272736        3.1375456312476953       
0.1          0.20648401356917206       0.24789315197772166       0.20054404063914397      
0.01         0.20648401356917206       0.2115735218485959        0.024648437384809232     
0.001        0.20648401356917206       0.20700008823304472       0.0024993444042086662    
0.0001       0.20648401356917206       0.20653569029360774       0.0002502698564524315    
1e-05        0.20648401356917206       0.20648918194199337       2.503037756755946e-05    
1e-06        0.20648401356917206       0.2064845304738938        2.503364366146249e-06    
1e-07        0.20648401356917206       0.2064840654014688        2.510232915202514e-07    
1e-08        0.20648401356917206       0.206484029874332         7.896572554261492e-08    
1e-09        0.20648401356917206       0.20648416310109496       7.241815979587518e-07    
1e-10        0.20648401356917206       0.20648593945793436       9.327059896840575e-06    
1e-11        0.20648401356917206       0.20650148258027912       8.460224501205654e-05    
1e-12        0.20648401356917206       0.20661250488274163       0.0006222821386921705    
1e-13        0.20648401356917206       0.20761170560490427       0.005461401181813197     
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          1.0336879322725656        0.6433755051602645        1.6224078709575354       
0.1          1.0336879322725656        0.6948520122619922        0.32779324342661303      
0.01         1.0336879322725656        1.009782615255758         0.023126241750981495     
0.001        1.0336879322725656        1.0313778365611537        0.0022348095970639707    
0.0001       1.0336879322725656        1.0334577096099418        0.00022271969657002932   
1e-05        1.0336879322725656        1.0336649178710466        2.2264361225902467e-05   
1e-06        1.0336879322725656        1.033685631007808         2.2262664444415524e-06   
1e-07        1.0336879322725656        1.0336877023786606        2.2240165319371807e-07   
1e-08        1.0336879322725656        1.0336879185945946        1.3232205340730244e-08   
1e-09        1.0336879322725656        1.033688024065782         8.880167166072723e-08    
1e-10        1.0336879322725656        1.0336886901995967        7.332261579857218e-07    
1e-11        1.0336879322725656        1.033692575980183         4.492368994881524e-06    
1e-12        1.0336879322725656        1.0338396805309458        0.00014680277639165116   
1e-13        1.0336879322725656        1.0341727474383333        0.0004690150195541485    
..
pyapprox/surrogates/orthopoly/tests/test_numeric_orthonormal_recursions.py ......[[0.25       1.        ]
 [0.46428571 0.22047928]
 [0.48548245 0.24224947]
 [0.49210308 0.2464317 ]
 [0.4950285  0.24795568]] [[0.25       1.        ]
 [0.46428571 0.22047928]
 [0.48548245 0.24224947]
 [0.49210308 0.2464317 ]
 [0.4950285  0.24795568]]
.0 50 inf
1 100 1.5543122344752192e-15
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 2.997671439839845e-27
adaptive quadrature converged in 1 iterations
Left 1 0.9697100821668916 7.591687291297809e-15 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 1.3183898417423734e-16
adaptive quadrature converged in 1 iterations
Right 1 0.999970901289614 0.03026081912272239 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 0.999970901289614 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
0 50 inf
1 100 5.551115123125783e-17
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 1.0428741167021776e-26
adaptive quadrature converged in 1 iterations
Left 1 0.4412542968781463 -2.6659116885831113e-14 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 4.163336342344337e-16
adaptive quadrature converged in 1 iterations
Right 1 0.5769078164336694 0.13565351955552313 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 0.5769078164336694 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
0 50 inf
1 100 6.661338147750939e-16
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 4.925016403475821e-26
adaptive quadrature converged in 1 iterations
Left 1 1.1508337814906906 1.2690681388419127e-13 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 8.326672684688674e-16
adaptive quadrature converged in 1 iterations
Right 1 1.641563709923321 0.49072992843263036 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 1.641563709923321 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
0 50 inf
1 100 8.881784197001252e-16
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 8.547465680250707e-26
adaptive quadrature converged in 1 iterations
Left 1 0.1742891596822037 -2.2249084893406533e-13 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 7.771561172376096e-16
adaptive quadrature converged in 1 iterations
Right 1 1.1256442252805696 0.9513550655983658 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 1.1256442252805696 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
0 50 inf
1 100 8.881784197001252e-16
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 4.737709270140144e-25
adaptive quadrature converged in 1 iterations
Left 1 1.3572959474021198 1.2392158115277785e-12 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 6.661338147750939e-16
adaptive quadrature converged in 1 iterations
Right 1 2.9973809135029175 1.6400849661007975 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 2.9973809135029175 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
0 50 inf
1 100 1.3322676295501878e-15
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 2.999249161650287e-27
adaptive quadrature converged in 1 iterations
Left 1 0.9697383003008424 7.591908206035975e-15 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 1.214306433183765e-16
adaptive quadrature converged in 1 iterations
Right 1 1.0000000000000002 0.03026169969915772 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 1.0000000000000002 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
0 50 inf
1 100 1.3322676295501878e-15
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 2.999249161650287e-27
adaptive quadrature converged in 1 iterations
Left 1 0.9697383003008424 7.591908206035975e-15 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 1.214306433183765e-16
adaptive quadrature converged in 1 iterations
Right 1 1.0000000000000002 0.03026169969915772 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 1.0000000000000002 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
0 50 inf
1 100 6.245004513516506e-16
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 9.48841896799833e-27
adaptive quadrature converged in 1 iterations
Left 1 -0.09225106043066619 -2.422579975083041e-14 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 2.220446049250313e-16
adaptive quadrature converged in 1 iterations
Right 1 -8.326672684688674e-17 0.09225106043066611 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 -8.326672684688674e-17 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
0 50 inf
1 100 6.245004513516506e-16
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 9.48841896799833e-27
adaptive quadrature converged in 1 iterations
Left 1 -0.09225106043066619 -2.422579975083041e-14 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 2.220446049250313e-16
adaptive quadrature converged in 1 iterations
Right 1 -8.326672684688674e-17 0.09225106043066611 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 -8.326672684688674e-17 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
0 50 inf
1 100 1.1102230246251565e-16
adaptive quadrature converged in 1 iterations
0 50 inf
1 100 3.001457972184906e-26
adaptive quadrature converged in 1 iterations
Left 1 0.7010594681972149 7.730849135920482e-14 [-17.40800359] [-17.40800359] [6.96320143]
Tolerance 1e-08 1e-08 for left integral reached in 1 iterations
0 50 inf
1 100 4.440892098500626e-16
adaptive quadrature converged in 1 iterations
Right 1 0.9999999999999999 0.298940531802785 [17.40800359] [17.40800359] [6.96320143]
0 50 inf
1 100 0.0
adaptive quadrature converged in 1 iterations
Right 2 0.9999999999999999 0.0 [24.37120502] [24.37120502] [6.96320143]
Tolerance 1e-08 1e-08 for right integral reached in 2 iterations
2.220446049250313e-16
...
pyapprox/surrogates/orthopoly/tests/test_orthonormal_polynomials.py .Cond number 116040.17650734672
..........[[3.         1.        ]
 [5.         1.73205081]
 [7.         2.82842712]]
...True
.
pyapprox/surrogates/orthopoly/tests/test_quadrature.py ....[[ 1.  1.]
 [ 3.  1.]
 [ 5.  2.]
 [ 7.  3.]
 [ 9.  4.]
 [11.  5.]
 [13.  6.]
 [15.  7.]
 [17.  8.]
 [19.  9.]]
..[10.  2. 18.  6. 15.  0. 19.] [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19.] 9.5 9.5
7622790.499999999 7622790.5 discrete_chebyshev
[ 8. 11.  4. 14.  6.  2. 16.] [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.] 8.5 8.5
767371.5 767371.5 binom
[5. 3. 7. 2. 8. 1. 6.] [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] 4.5 4.5
17841.55263157893 17841.552631578914 hypergeom
...[  0.67448975  -1.48260302   3.26007602  -3.35918866   1.79344369  -0.44811773 -11.75523506   4.53168921  -2.48470979  -4.57423086   2.51282696] [  0.67448975  -1.48260302   3.26007602  -3.35918866   1.79344369  -0.44811773 -11.75523506   4.53168921  -2.48470979  -4.57423086   2.51282696   5.72450533  -5.55599449]
...0.9976686663706482
0.9976686663706054
..
pyapprox/surrogates/orthopoly/tests/test_recursion_factory.py ksone {'n': 1000}
kstwobign {}
norm {}
alpha {'a': 1}
anglit {}
arcsine {}
beta {'a': 2, 'b': 3}
betaprime {'a': 2, 'b': 3}
bradford {'c': 2}
burr {'c': 2, 'd': 1}
burr12 {'c': 2, 'd': 1}
fisk {'c': 3}
cauchy {}
chi {'df': 10}
chi2 {'df': 10}
cosine {}
dgamma {'a': 3}
dweibull {'c': 3}
expon {}
exponnorm {'K': 2}
exponweib {'a': 2, 'c': 3}
exponpow {'b': 3}
fatiguelife {'c': 3}
foldcauchy {'c': 3}
foldnorm {'c': 1}
weibull_min {'c': 1}
weibull_max {'c': 1}
genlogistic {'c': 1}
genpareto {'c': 1}
genexpon {'a': 2, 'b': 3, 'c': 1}
genextreme {'c': 1}
gamma {'a': 2}
erlang {'a': 2}
gengamma {'a': 2, 'c': 1}
genhalflogistic {'c': 1}
gompertz {'c': 1}
gumbel_r {}
gumbel_l {}
halfcauchy {}
halflogistic {}
halfnorm {}
hypsecant {}
gausshyper {'a': 2, 'b': 3, 'c': 1, 'z': 1}
invgamma {'a': 1}
invgauss {'mu': 1}
norminvgauss {'a': 2, 'b': 1}
invweibull {'c': 1}
johnsonsb {'a': 2, 'b': 1}
johnsonsu {'a': 2, 'b': 1}
laplace {}
logistic {}
loggamma {'c': 1}
lognorm {'s': 1}
gilbrat {}
maxwell {}
mielke {'k': 1, 's': 1}
kappa4 {'h': 1, 'k': 1}
kappa3 {'a': 1}
moyal {}
nakagami {'nu': 1}
ncx2 {'df': 10, 'nc': 1}
t {'df': 10}
nct {'df': 10, 'nc': 1}
pareto {'b': 2}
lomax {'c': 2}
powerlognorm {'c': 2, 's': 1}
powernorm {'c': 2}
rdist {'c': 2}
rayleigh {}
reciprocal {'a': 2, 'b': 3}
rice {'b': 2}
recipinvgauss {'mu': 2}
semicircular {}
skewnorm {'a': 1}
trapz {'c': 0, 'd': 1}
triang {'c': 1}
truncexpon {'b': 2}
truncnorm {'a': 2, 'b': 3}
tukeylambda {'lam': 2}
uniform {}
vonmises_line {'kappa': 2}
wald {}
wrapcauchy {'c': 0.5}
gennorm {'beta': 2}
halfgennorm {'beta': 2}
argus {'chi': 1}
..[[ 3.          1.        ]
 [ 5.          1.73205081]
 [ 7.          2.82842712]
 [ 9.          3.87298335]
 [11.          4.89897949]
 [13.          5.91607978]]
.
pyapprox/surrogates/polychaos/tests/test_adaptive_polynomial_chaos.py induced sampling error 2.4527291089678024e-15
.probability sampling error 2.2471779144388108e-08
.leja sampling error 1.796227546452331e-16
leja sampling error 6.888647863132735e-17
.[2 1] 2.7553486112025156e-18
Accuracy misleadingly appears reached because admissibility  criterion is preventing new subspaces from being added to the active set
[1 2] 2.7553486112025156e-18
Accuracy misleadingly appears reached because admissibility  criterion is preventing new subspaces from being added to the active set
leja sampling error 9.830156615717116e-09
.
pyapprox/surrogates/polychaos/tests/test_arbitrary_polynomial_chaos.py ..Max num evaluations (100) reached
Error estimate 0.4122556056308737
Max num evaluations (100) reached
Error estimate 0.4122556056308737
Max num evaluations (100) reached
Error estimate 0.1516765703038719
Max num evaluations (100) reached
Error estimate 0.0012311941126541296
Max num evaluations (100) reached
Error estimate 0.0012311941126541296
Max num evaluations (100) reached
Error estimate 0.4122556056308737
Max num evaluations (100) reached
Error estimate 0.4122556056308737
Max num evaluations (100) reached
Error estimate 0.1516765703038719
Max num evaluations (100) reached
Error estimate 0.0012311941126541296
Max num evaluations (100) reached
Error estimate 0.0012311941126541296
.......
pyapprox/surrogates/polychaos/tests/test_gpc.py ..............[1.03199174e-04 4.33436533e-03 4.76780186e-02 1.98658411e-01 3.57585139e-01 2.86068111e-01 9.53560372e-02 1.02167183e-02]
.[4.45092554e+22] 4.454581559137093e+22
.......
pyapprox/surrogates/polychaos/tests/test_indexing.py .........
pyapprox/surrogates/polychaos/tests/test_induced_sampling.py .-5.0 5.0
.....
pyapprox/surrogates/polychaos/tests/test_leja_sequences.py ...
pyapprox/surrogates/polychaos/tests/test_manipulate_polynomials.py .........[6]
.
pyapprox/surrogates/polychaos/tests/test_monomial.py ......
pyapprox/surrogates/polychaos/tests/test_orthogonal_least_interpolation.py ..........
pyapprox/surrogates/polychaos/tests/test_polynomial_sampling.py .......
pyapprox/surrogates/polychaos/tests/test_sparse_grid_to_pce.py ......
pyapprox/surrogates/tests/test_approximate.py 0.9345554598991586 10
0.00988277061050155 20
4.74060362341744e-05 30
3.550221791603383e-08 40
8.069347127274334e-09 50
Tolerance reached. Iteration:58. Tol=0.0. Error=-1.5241664061924358e-17
4.901443737732886e-09 59
Exiting: Cannot add additional samples. Kernel is now ill conditioned. If more samples are really required increase alpha or manually fix kernel_length to a smaller value
.0.18943375189434314 10
0.0007034265579445862 20
3.313586690188437e-05 30
1.6194801769290196e-07 40
6.960548920651261e-08 50
matrix is not positive definite
6.960466410498328e-08 50
Exiting: Cannot add additional samples. Kernel is now ill conditioned. If more samples are really required increase alpha or manually fix kernel_length to a smaller value
6.960466410498328e-08
.Ntrain samples 31
...Independent Marginal Variable
Number of variables: 3
Unique variables and global id:
    uniform(loc=-3.141592653589793,scale=6.283185307179586): z0, z1, z2
{'activation_func': 'sigmoid', 'layers': [3, 75, 1], 'loss_func': 'squared_loss', 'var_trans': <pyapprox.variables.transforms.AffineTransform object at 0x13ecc1d60>, 'lag_mult': 0}
No. Parameters 376
No. Samples 500
Repeat 1/10 Loss 0.00022080309774064307 Success False gnorm 0.0004140834836556469
Repeat 2/10 Loss 0.0004993623386677807 Success True gnorm 0.000405114196729727
Repeat 3/10 Loss 0.0006119905225494247 Success True gnorm 0.0013359626179757535
Repeat 4/10 Loss 0.00011909348233800083 Success True gnorm 0.0008596861654268473
Repeat 5/10 Loss 0.00047118806622028147 Success False gnorm 0.0009856111963563096
Repeat 6/10 Loss 0.00020465627123286302 Success True gnorm 0.001252337721673452
Repeat 7/10 Loss 0.00031954710165863506 Success False gnorm 0.0015269323253889674
Repeat 8/10 Loss 0.0006383405339721546 Success False gnorm 0.0004644305936169367
Repeat 9/10 Loss 0.00041138609472076614 Success False gnorm 0.0005540974574371252
Repeat 10/10 Loss 0.00015008499885289355 Success True gnorm 0.0012273196706882369
No. restarts completed 5 / 10
Losses
[0.00022080309774064307, 0.0004993623386677807, 0.0006119905225494247, 0.00011909348233800083, 0.00047118806622028147, 0.00020465627123286302, 0.00031954710165863506, 0.0006383405339721546, 0.00041138609472076614, 0.00015008499885289355]
[0.0476839258310737]
.Initializing basis with hyperbolic cross of degree 11 and strength 1 with 364 terms
nterms     nnz terms  cv score          
364        59         0.004396004963086298
Expanding 59 restricted from 364 terms New number of terms 145
145        80         0.003920129640938281
Max number of inner expansion steps (1) reached
Expanding 80 restricted from 145 terms New number of terms 169
169        53         0.000612733940632222
Max number of inner expansion steps (1) reached
Expanding 53 restricted from 169 terms New number of terms 139
139        62         0.0005906701032305532
Max number of inner expansion steps (1) reached
Expanding 62 restricted from 139 terms New number of terms 155
155        75         0.0005886416823923565
Max number of inner expansion steps (1) reached
Expanding 75 restricted from 155 terms New number of terms 167
167        80         0.0005805224716279536
Max number of inner expansion steps (1) reached
Expanding 80 restricted from 167 terms New number of terms 183
183        85         0.0005849855417237408
Max number of inner expansion steps (1) reached
Terminating: error did not decrease in last 1 iterationsbest error: 0.0005805224716279536
Final basis has 80 terms selected from 167 using 400 samples
.refining index [0 0 0] with priority -inf
The current number of equivalent function evaluations is 1.0
Subspace [1 0 0] is admissible
Subspace [0 1 0] is admissible
Subspace [0 0 1] is admissible
adding new index [1 0 0] with priority [-0.0160332998017698]
adding new index [0 1 0] with priority [-0.1075196445419041]
adding new index [0 0 1] with priority [-0.0005485019774026]
refining index [0 1 0] with priority [-0.1075196445419041]
The current number of equivalent function evaluations is 7.0
Subspace [1 1 0] is not admissible
Subspace [0 2 0] is admissible
Subspace [0 1 1] is not admissible
adding new index [0 2 0] with priority [-0.0002139762038141]
refining index [1 0 0] with priority [-0.002792808950056]
The current number of equivalent function evaluations is 9.0
Subspace [2 0 0] is admissible
Subspace [1 1 0] is admissible
Subspace [1 0 1] is not admissible
adding new index [2 0 0] with priority [-1.8335743138550496e-05]
adding new index [1 1 0] with priority [-0.0002104645199479]
refining index [1 1 0] with priority [-0.0001578483899609]
The current number of equivalent function evaluations is 15.0
Subspace [2 1 0] is not admissible
Subspace [1 2 0] is not admissible
Subspace [1 1 1] is not admissible
refining index [0 2 0] with priority [-0.0001335086025863]
The current number of equivalent function evaluations is 15.0
Subspace [1 2 0] is admissible
Subspace [0 3 0] is admissible
Subspace [0 2 1] is not admissible
adding new index [1 2 0] with priority [-4.963368126820659e-06]
adding new index [0 3 0] with priority [-2.463100542670689e-06]
refining index [1 2 0] with priority [-7.445052190230989e-06]
The current number of equivalent function evaluations is 27.0
Subspace [2 2 0] is not admissible
Subspace [1 3 0] is not admissible
Subspace [1 2 1] is not admissible
refining index [0 3 0] with priority [-7.389301628012067e-06]
The current number of equivalent function evaluations is 27.0
Subspace [1 3 0] is admissible
Subspace [0 4 0] is admissible
Subspace [0 3 1] is not admissible
adding new index [1 3 0] with priority [-9.187381670167595e-06]
adding new index [0 4 0] with priority [-3.3847246269509564e-07]
refining index [1 3 0] with priority [-6.890536252625696e-06]
The current number of equivalent function evaluations is 33.0
Subspace [2 3 0] is not admissible
Subspace [1 4 0] is not admissible
Subspace [1 3 1] is not admissible
refining index [0 0 1] with priority [-1.6086871579521024e-06]
The current number of equivalent function evaluations is 33.0
Subspace [1 0 1] is admissible
Subspace [0 1 1] is admissible
Subspace [0 0 2] is admissible
adding new index [1 0 1] with priority [-9.51962399599178e-07]
adding new index [0 1 1] with priority [-2.0133239651793266e-09]
adding new index [0 0 2] with priority [-2.6752736711106245e-08]
refining index [1 0 1] with priority [-7.139717996993835e-07]
The current number of equivalent function evaluations is 42.0
Subspace [2 0 1] is not admissible
Subspace [1 1 1] is not admissible
Subspace [1 0 2] is not admissible
refining index [2 0 0] with priority [-2.1863467124982098e-07]
The current number of equivalent function evaluations is 42.0
Subspace [3 0 0] is admissible
Subspace [2 1 0] is admissible
Subspace [2 0 1] is admissible
adding new index [3 0 0] with priority [-2.8546188384989518e-09]
adding new index [2 1 0] with priority [-3.897216278482179e-09]
adding new index [2 0 1] with priority [-2.197666931533101e-08]
refining index [0 0 2] with priority [-1.0460301258399841e-07]
The current number of equivalent function evaluations is 51.0
Subspace [1 0 2] is admissible
Subspace [0 1 2] is not admissible
Subspace [0 0 3] is admissible
adding new index [1 0 2] with priority [-2.5212536013943555e-08]
adding new index [0 0 3] with priority [-9.576872188962298e-13]
refining index [1 0 2] with priority [-1.8909402010457667e-08]
The current number of equivalent function evaluations is 57.0
Subspace [2 0 2] is not admissible
Subspace [1 1 2] is not admissible
Subspace [1 0 3] is not admissible
refining index [2 0 1] with priority [-9.202037685302984e-11]
The current number of equivalent function evaluations is 57.0
Subspace [3 0 1] is not admissible
Subspace [2 1 1] is not admissible
Subspace [2 0 2] is admissible
adding new index [2 0 2] with priority [-2.2689241913969044e-12]
refining index [3 0 0] with priority [-3.27146277362015e-12]
The current number of equivalent function evaluations is 61.0
Subspace [4 0 0] is admissible
Subspace [3 1 0] is not admissible
Subspace [3 0 1] is admissible
adding new index [4 0 0] with priority [-8.216302978698685e-18]
adding new index [3 0 1] with priority [-9.238750623472797e-16]
refining index [2 0 2] with priority [-2.2701688941543017e-12]
The current number of equivalent function evaluations is 67.0
Subspace [3 0 2] is not admissible
Subspace [2 1 2] is not admissible
Subspace [2 0 3] is not admissible
refining index [0 4 0] with priority [-1.2563630077725857e-12]
The current number of equivalent function evaluations is 67.0
Subspace [1 4 0] is admissible
Cannot add subspace [0 5 0]
Max level of 4.0 reached in variable 1
Subspace [0 5 0] is not admissible
Subspace [0 4 1] is not admissible
adding new index [1 4 0] with priority [-3.651131920076633e-17]
refining index [3 0 1] with priority [-6.121975748769092e-16]
The current number of equivalent function evaluations is 71.0
Subspace [4 0 1] is not admissible
Subspace [3 1 1] is not admissible
Subspace [3 0 2] is admissible
adding new index [3 0 2] with priority [-4.725504230658146e-17]
refining index [3 0 2] with priority [-4.725504230658146e-17]
The current number of equivalent function evaluations is 75.0
Subspace [4 0 2] is not admissible
Subspace [3 1 2] is not admissible
Subspace [3 0 3] is not admissible
refining index [1 4 0] with priority [-3.963790086149219e-17]
The current number of equivalent function evaluations is 75.0
Subspace [2 4 0] is not admissible
Subspace [1 5 0] is not admissible
Subspace [1 4 1] is not admissible
refining index [2 1 0] with priority [-9.532208367177649e-18]
The current number of equivalent function evaluations is 75.0
Subspace [3 1 0] is admissible
Subspace [2 2 0] is admissible
Subspace [2 1 1] is not admissible
adding new index [3 1 0] with priority [-7.469221738557445e-17]
adding new index [2 2 0] with priority [-2.1482608334187963e-16]
refining index [2 2 0] with priority [-2.1482608334187963e-16]
The current number of equivalent function evaluations is 83.0
Subspace [3 2 0] is not admissible
Subspace [2 3 0] is admissible
Subspace [2 2 1] is not admissible
adding new index [2 3 0] with priority [-6.034442751841298e-18]
refining index [4 0 0] with priority [-3.213662709498113e-17]
The current number of equivalent function evaluations is 87.0
Cannot add subspace [5 0 0]
Max level of 4.0 reached in variable 0
Subspace [5 0 0] is not admissible
Subspace [4 1 0] is not admissible
Subspace [4 0 1] is admissible
adding new index [4 0 1] with priority [-9.128052798809316e-17]
refining index [4 0 1] with priority [-9.128052798809316e-17]
The current number of equivalent function evaluations is 91.0
Subspace [5 0 1] is not admissible
Subspace [4 1 1] is not admissible
Subspace [4 0 2] is admissible
adding new index [4 0 2] with priority [-1.0588244508672749e-16]
refining index [4 0 2] with priority [-1.0588244508672749e-16]
The current number of equivalent function evaluations is 95.0
Subspace [5 0 2] is not admissible
Subspace [4 1 2] is not admissible
Subspace [4 0 3] is not admissible
refining index [3 1 0] with priority [-8.029168411083031e-17]
The current number of equivalent function evaluations is 95.0
Subspace [4 1 0] is admissible
Subspace [3 2 0] is admissible
Subspace [3 1 1] is not admissible
adding new index [4 1 0] with priority [-1.9023375113745196e-16]
adding new index [3 2 0] with priority [-2.6137252434354693e-17]
refining index [2 3 0] with priority [-4.903777985685195e-16]
The current number of equivalent function evaluations is 103.0
Subspace [3 3 0] is not admissible
Max num evaluations (100) reached
Error estimate 3.413023869370973e-14
Subspace [2 4 0] is not admissible
Subspace [2 3 1] is not admissible
refining index [4 1 0] with priority [-1.9023375113745196e-16]
The current number of equivalent function evaluations is 103.0
Subspace [5 1 0] is not admissible
Subspace [4 2 0] is not admissible
Subspace [4 1 1] is not admissible
refining index [3 2 0] with priority [-2.6137252434354693e-17]
The current number of equivalent function evaluations is 103.0
Max num evaluations (100) reached
Error estimate 1.7111307915207167e-15
Subspace [4 2 0] is not admissible
Max num evaluations (100) reached
Error estimate 1.7111307915207167e-15
Subspace [3 3 0] is not admissible
Subspace [3 2 1] is not admissible
refining index [0 0 3] with priority [-1.653499804040007e-17]
The current number of equivalent function evaluations is 103.0
Max num evaluations (100) reached
Error estimate 4.661527900422577e-16
Subspace [1 0 3] is not admissible
Subspace [0 1 3] is not admissible
Max num evaluations (100) reached
Error estimate 4.661527900422577e-16
Subspace [0 0 4] is not admissible
refining index [0 1 1] with priority [-1.5189816500160827e-18]
The current number of equivalent function evaluations is 103.0
Max num evaluations (100) reached
Error estimate 7.235262174817666e-17
Subspace [1 1 1] is not admissible
Max num evaluations (100) reached
Error estimate 7.235262174817666e-17
Subspace [0 2 1] is not admissible
Max num evaluations (100) reached
Error estimate 7.235262174817666e-17
Subspace [0 1 2] is not admissible
[7.165909283781323e-07]
.....Initializing basis with hyperbolic cross of degree 7 and strength 1 with 36 terms
nterms     nnz terms  cv score          
36         15         3.637247976533328e-13
Expanding 15 restricted from 36 terms New number of terms 31
31         15         1.6072874899120499e-12
Max number of inner expansion steps (1) reached
Terminating: error did not decrease in last 1 iterationsbest error: 3.637247976533328e-13
Final basis has 15 terms selected from 36 using 32 samples
Initializing basis with hyperbolic cross of degree 7 and strength 1 with 36 terms
nterms     nnz terms  cv score          
36         11         2.209713660464254e-14
Expanding 10 restricted from 36 terms New number of terms 22
22         10         1.0797799761530758e-14
Max number of inner expansion steps (1) reached
Expanding 10 restricted from 22 terms New number of terms 22
22         10         1.0797799761530758e-14
Max number of inner expansion steps (1) reached
Terminating: error did not decrease in last 1 iterationsbest error: 1.0797799761530758e-14
Final basis has 10 terms selected from 22 using 32 samples
.lstsq {'alphas': [1e-14], 'cv': 22}
Approximating QoI: 0
degree   num_terms  cv score          
1        3          3.210201285956363  
2        6          2.5392025001866018e-15 
3        10         2.2626979405254516e-14 
4        15         1.1621082002231735e-12 
best degree: 2
Approximating QoI: 1
degree   num_terms  cv score          
1        3          2.8643972355247342 
2        6          1.4990949527218764 
3        10         1.3696859696263317e-14 
4        15         7.105730345645358e-13 
best degree: 3
lstsq {'cv': 10}
Approximating QoI: 0
degree   num_terms  cv score          
1        3          0.4999787878072802 
2        6          1.0408134810756679e-15 
3        10         8.714620133606048e-14 
4        15         3.2109627917276025e-13 
best degree: 2
Approximating QoI: 1
degree   num_terms  cv score          
1        3          1.912940064408354  
2        6          1.8615597573266938 
3        10         2.146918995004321e-13 
4        15         8.218816858136742e-13 
best degree: 3
lasso {'max_iter': 20, 'cv': 21}
Approximating QoI: 0
degree   num_terms  cv score          
1        3          1.3133487336591467 
2        6          1.7909241574607096e-15 
3        10         6.128828788436871e-14 
4        15         7.91449512525408e-13 
best degree: 2
Approximating QoI: 1
degree   num_terms  cv score          
1        3          3.8825817337747064 
2        6          2.01485283130838   
3        10         3.34138509245524e-14 
4        15         1.575885576298773e-12 
best degree: 3
.Approximating QoI: 0
Initializing basis with hyperbolic cross of degree 7 and strength 1 with 36 terms
nterms     nnz terms  cv score          
36         15         3.637247976533328e-13
Expanding 15 restricted from 36 terms New number of terms 31
31         15         1.6072874899120499e-12
Max number of inner expansion steps (1) reached
Expanding 15 restricted from 36 terms New number of terms 31
31         15         1.6072874899120499e-12
Max number of inner expansion steps (1) reached
Terminating: error did not decrease in last 2 iterationsbest error: 3.637247976533328e-13
Final basis has 15 terms selected from 36 using 32 samples
Approximating QoI: 1
Initializing basis with hyperbolic cross of degree 7 and strength 1 with 36 terms
nterms     nnz terms  cv score          
36         11         2.209713660464254e-14
Expanding 10 restricted from 36 terms New number of terms 22
22         10         1.0797799761530758e-14
Max number of inner expansion steps (1) reached
Expanding 10 restricted from 22 terms New number of terms 22
22         10         1.0797799761530758e-14
Max number of inner expansion steps (1) reached
Expanding 10 restricted from 22 terms New number of terms 22
22         10         1.0797799761530758e-14
Max number of inner expansion steps (1) reached
Terminating: error did not decrease in last 2 iterationsbest error: 1.0797799761530758e-14
Final basis has 10 terms selected from 22 using 32 samples
.
pyapprox/surrogates/tests/test_coupled_systems.py .......
pyapprox/surrogates/tests/test_function_train.py ......('sparsity', 4, 'num_samples', 21)
iteration 0 the residual norm is 7.24e-03
iteration 10 the residual norm is 5.73e-03
Terminating: relative norm or residual is below tolerance
...(14, 1)
('sparsity', 4, 'num_samples', 14, 'num_ft_params', 24)
[ 0.4412274868850414  0.                  0.                  0.                  0.                  1.5824811170615634  0.                  0.                  0.                  0.                  0.                  0.                  0.                  0.                  0.                 -0.7001790376899514  0.                  0.                  0.                  0.                 -0.9806078852186219  0.                  0.                  0.                ]
('sparsity', 'index', '  ||r||  ')
('   1    ', '  15 ', '4.175e+00')
('   2    ', '  5  ', '1.056e+00')
('   3    ', '  17 ', '1.056e+00')
('   4    ', '  0  ', '7.873e-07')
Terminating: relative residual norm is below tolerance
1.885903819690939e-07
.
pyapprox/surrogates/tests/test_neural_network.py .No. Parameters 10
No. Samples 25
Repeat 1/10 Loss 0.12391930678428598 Success True gnorm 4.905182246841425e-05
Repeat 2/10 Loss 0.07905960049246454 Success True gnorm 0.0010690117720008197
Repeat 3/10 Loss 0.00039254675974609104 Success True gnorm 6.92067797259151e-05
Repeat 4/10 Loss 0.10626416242622991 Success True gnorm 0.0006414732347537611
Repeat 5/10 Loss 0.07906047155573942 Success True gnorm 0.0007848278546418469
Repeat 6/10 Loss 0.11760088827441349 Success True gnorm 1.841861531140946e-05
Repeat 7/10 Loss 0.07905926408860316 Success True gnorm 0.0022767990976984708
Repeat 8/10 Loss 0.07912912985011657 Success True gnorm 9.3128699718057e-05
Repeat 9/10 Loss 0.0003957444753515905 Success True gnorm 0.0001230139048752047
Repeat 10/10 Loss 0.1415699138979552 Success True gnorm 1.6134120980717843e-05
No. restarts completed 10 / 10
Losses
[0.12391930678428598, 0.07905960049246454, 0.00039254675974609104, 0.10626416242622991, 0.07906047155573942, 0.11760088827441349, 0.07905926408860316, 0.07912912985011657, 0.0003957444753515905, 0.1415699138979552]
0.0673436771760394
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.42433603235718376       1.0319653169884266        2.738506815549417        
0.1          0.42433603235718376       13.196801754477491        31.591459563387886       
0.01         0.42433603235718376       135.2910914132712         319.3329865008356        
0.001        0.42433603235718376       1356.2122240460162        3196.5847127989823       
0.0001       0.42433603235718376       13565.420869075411        31969.08518086965        
1e-05        0.42433603235718376       135657.50704644865        319694.0881781596        
1e-06        0.42433603235718376       1356578.3687928417        3196944.1179826795       
1e-07        0.42433603235718376       13565786.986254038        31969444.416011035       
1e-08        0.42433603235718376       135657873.1608657         319694447.39629287       
1e-09        0.42433603235718376       1356578734.9069822        3196944477.199111        
1e-10        0.42433603235718376       13565787352.368147        31969444775.22729        
1e-11        0.42433603235718376       135657873526.97983        319694447755.50916       
1e-12        0.42433603235718376       1356578735273.0964        3196944477558.327        
1e-13        0.42433603235718376       13565787352734.26         31969444775586.504       
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.3709906816471008        0.6441711147372255        1.660003367501246        
0.1          0.3709906816471008        5.706948083635674         16.3392021629855         
0.01         0.3709906816471008        58.33225869393515         158.19396161947404       
0.001        0.3709906816471008        584.2842269589958         1575.8900991389014       
0.0001       0.3709906816471008        5843.775000762043         15752.771196869071       
1e-05        0.3709906816471008        58438.67986094327         157521.57419616738       
1e-06        0.3709906816471008        584387.7281751018         1575209.6033918487       
1e-07        0.3709906816471008        5843878.211287924         15752089.895268938       
1e-08        0.3709906816471008        58438783.04241326         157520892.8140318        
1e-09        0.3709906816471008        584387831.3536663         1575208922.0016599       
1e-10        0.3709906816471008        5843878314.466197         15752089213.87794        
1e-11        0.3709906816471008        58438783145.59151         157520892132.64075       
1e-12        0.3709906816471008        584387831456.8445         1575208921320.2686       
1e-13        0.3709906816471008        5843878314569.375         15752089213196.549       
Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
1.0          0.40781605358375106       0.4209765274869021        0.5500791611233855       
0.1          0.40781605358375106       4.9649242718673445        12.350621559021763       
0.01         0.40781605358375106       53.07492731132002         130.35604553800553       
0.001        0.40781605358375106       534.3186447138421         1310.4102853278428       
0.0001       0.40781605358375106       5346.769141994366         13110.952683226213       
1e-05        0.40781605358375106       53471.27543723291         131116.37666220998       
1e-06        0.40781605358375106       534716.3385217631         1311170.6164520474       
1e-07        0.40781605358375106       5347166.969380279         13111713.014350422       
1e-08        0.40781605358375106       53471673.27796676         131117136.99333416       
1e-09        0.40781605358375106       534716736.3638315         1311171376.7831712       
1e-10        0.40781605358375106       5347167367.222482         13111713774.681545       
1e-11        0.40781605358375106       53471673675.808975        131117137753.66528       
1e-12        0.40781605358375106       534716736761.6739         1311171377543.5024       
1e-13        0.40781605358375106       5347167367620.322         13111713775441.873       
.Eps          norm(jv)                  norm(jv_fd)               Rel. Errors              
F
pyapprox/surrogates/tests/test_system_analysis.py ------------------------------------
Initializing component M_0
--
------------------------------------
------------------------------------
Initializing component M_1
--
[[1. ]
 [0.5]]
------------------------------------
------------------------------------
Initializing component M_2
--
------------------------------------
------------------------------------
Refining component M_0
------------------------------------
------------------------------------
Refining component M_1
[[0.5 1.5 1.  1. ]
 [0.5 0.5 0.  1. ]]
------------------------------------
------------------------------------
Refining component M_2
------------------------------------
------------------------------------
Refining component M_2
By adding subspace [0 1]
Total work 13.0
--
------------------------------------
Adjusting ranges of local coupling variable  0 of component 1 from (array([0.5]), array([1.5])) to [array([0.5]), 1.9997558742761614]
Adjusting ranges of local coupling variable  0 of component 2 from (array([2.]), array([5.])) to [1.000000006144, array([5.])]
------------------------------------
Refining component M_1
By adding subspace [1 0]
Total work 15.0
--
[[1.9997558742761614 0.6909793719355051]
 [0.5                0.5               ]]
------------------------------------
Adjusting ranges of local coupling variable  0 of component 2 from (array([1.000000006144]), array([5.])) to [array([1.000000006144]), 7.373047113404027]
------------------------------------
Refining component M_0
By adding subspace [1]
Total work 17.0
--
------------------------------------
Adjusting ranges of local coupling variable  0 of component 2 from (array([1.000000006144]), array([7.373047113404027])) to [array([1.000000006144]), 8.373167814732437]
------------------------------------
Refining component M_2
By adding subspace [1 0]
Total work 19.0
--
------------------------------------
------------------------------------
Refining component M_1
By adding subspace [0 1]
Total work 25.0
--
[[0.5                1.5                0.5                1.5                1.                 1.                ]
 [0.                 0.                 1.                 1.                 0.1464466094067262 0.8535533905932737]]
------------------------------------
[[1.1739073524061536 1.5188673758527016 1.000000013081599  1.0914049844741867 1.0215372914895244 1.0085264160838763 1.0346928663424546 1.1194122160745197 1.1574244286073854 1.2903234728420443]
 [0.8007445686755367 0.9682615757193975 0.3134241781592428 0.6923226156693141 0.8763891522960383 0.8946066635038473 0.0850442113697779 0.0390547832328824 0.1698304195645689 0.8781425034294131]]
.------------------------------------
Initializing component M_0
--
------------------------------------
------------------------------------
Initializing component M_1
--
[[1. ]
 [0.5]]
------------------------------------
------------------------------------
Initializing component M_2
--
------------------------------------
------------------------------------
Refining component M_0
------------------------------------
------------------------------------
Refining component M_1
[[0.  2.  1.  1. ]
 [0.5 0.5 0.  1. ]]
------------------------------------
------------------------------------
Refining component M_2
------------------------------------
------------------------------------
Refining component M_1
By adding subspace [1 0]
Total work 13.0
--
[[0.2928932188134524 1.7071067811865475]
 [0.5                0.5               ]]
------------------------------------
------------------------------------
Refining component M_2
By adding subspace [0 1]
Total work 15.0
--
------------------------------------
------------------------------------
Refining component M_1
By adding subspace [0 1]
Total work 17.0
--
[[0.                 2.                 0.                 2.                 1.                 1.                ]
 [0.                 0.                 1.                 1.                 0.1464466094067262 0.8535533905932737]]
------------------------------------
------------------------------------
Refining component M_0
By adding subspace [1]
Total work 23.0
--
------------------------------------
------------------------------------
Refining component M_2
By adding subspace [1 0]
Total work 25.0
--
------------------------------------
------------------------------------
Refining component M_1
By adding subspace [2 0]
Total work 31.0
--
[[0.0761204674887133 0.6173165676349102 1.3826834323650896 1.9238795325112867]
 [0.5                0.5                0.5                0.5               ]]
------------------------------------
------------------------------------
Refining component M_1
By adding subspace [0 2]
Total work 35.0
--
[[1.                 1.                 1.                 1.                ]
 [0.0380602337443566 0.3086582838174551 0.6913417161825448 0.9619397662556434]]
------------------------------------
------------------------------------
Refining component M_0
By adding subspace [2]
Total work 39.0
--
------------------------------------
------------------------------------
Refining component M_2
By adding subspace [2 0]
Total work 43.0
--
------------------------------------
------------------------------------
Refining component M_2
By adding subspace [0 2]
Total work 47.0
--
------------------------------------
[[1.1739073524061536 1.5188673758527016 1.000000013081599  1.0914049844741867 1.0215372914895244 1.0085264160838763 1.0346928663424546 1.1194122160745197 1.1574244286073854 1.2903234728420443]
 [0.8007445686755367 0.9682615757193975 0.3134241781592428 0.6923226156693141 0.8763891522960383 0.8946066635038473 0.0850442113697779 0.0390547832328824 0.1698304195645689 0.8781425034294131]]
.------------------------------------
Initializing component M_0
--
------------------------------------
------------------------------------
Initializing component M_1
--
------------------------------------
------------------------------------
Initializing component M_2
--
------------------------------------
------------------------------------
Refining component M_0
refining index [0] with priority -inf
The current number of equivalent function evaluations is 1.0
Subspace [1] is admissible
------------------------------------
------------------------------------
Refining component M_1
refining index [0 0] with priority -inf
The current number of equivalent function evaluations is 1.0
Subspace [1 0] is admissible
Subspace [0 1] is admissible
------------------------------------
------------------------------------
Refining component M_2
refining index [0 0] with priority -inf
The current number of equivalent function evaluations is 1.0
Subspace [1 0] is admissible
Subspace [0 1] is admissible
------------------------------------
------------------------------------
Refining component M_2
By adding subspace [0 1]
Total work 13.0
--
refining index [0 1] with priority [-8775.000000000002]
The current number of equivalent function evaluations is 5.0
Subspace [0 2] is admissible
------------------------------------
------------------------------------
Refining component M_1
By adding subspace [0 1]
Total work 15.0
--
refining index [0 1] with priority [-8853.409176954869]
The current number of equivalent function evaluations is 5.0
Subspace [0 2] is admissible
------------------------------------
------------------------------------
Refining component M_1
By adding subspace [1 0]
Total work 17.0
--
refining index [1 0] with priority [-5414.743403357206]
The current number of equivalent function evaluations is 7.0
Subspace [2 0] is admissible
Subspace [1 1] is admissible
------------------------------------
------------------------------------
Refining component M_0
By adding subspace [1]
Total work 23.0
--
refining index [1] with priority [-7484.015213809388]
The current number of equivalent function evaluations is 3.0
Subspace [2] is admissible
------------------------------------
.
pyapprox/util/tests/test_linalg.py ............[[0.749887498871951  0.3826949216070707]
 [0.3826949216070707 0.619303291023972 ]]
...
pyapprox/util/tests/test_utilities.py ....[0.9599979213775065]
..
pyapprox/variables/tests/test_density.py ...........
pyapprox/variables/tests/test_joint.py .....
pyapprox/variables/tests/test_multivariate_gaussian.py ...........
pyapprox/variables/tests/test_nataf_transformation.py ....
pyapprox/variables/tests/test_probability_measure_sampling.py ...
pyapprox/variables/tests/test_random_variable_algebra.py ...........[0.9999950185418369 0.9999999566043284 0.9999999735581666 0.9999999928520078 0.999999978165084  0.9999999899591488 1.000000001513764  0.9999999891696685 0.9999999976500511 1.0000000104784723]
..[8.298319978471206] 8.298319978471206
.
pyapprox/variables/tests/test_risk_measures.py ...1.6
.0.3333333333333333 [-0.6117564139177631] [0.527298730905869] 0.5272987308523317
0.5 [-0.5281717534302002] [0.8902668469758259] 0.890266846742477
0.85 [1.6243453570997004] [1.6243453658510887] 1.6243453636632417
..........79 79
..2.399809602039042 2.399809602039042
.
pyapprox/variables/tests/test_rosenblatt_transformation.py .....6.586291475872216e-09
.
pyapprox/variables/tests/test_variable_transformations.py ............
pyapprox/variables/tests/test_variables.py ...ksone {'n': 1000}
kstwobign {}
norm {}
alpha {'a': 1}
anglit {}
arcsine {}
beta {'a': 2, 'b': 3}
betaprime {'a': 2, 'b': 3}
bradford {'c': 2}
burr {'c': 2, 'd': 1}
burr12 {'c': 2, 'd': 1}
fisk {'c': 3}
cauchy {}
chi {'df': 10}
chi2 {'df': 10}
cosine {}
dgamma {'a': 3}
dweibull {'c': 3}
expon {}
exponnorm {'K': 2}
exponweib {'a': 2, 'c': 3}
exponpow {'b': 3}
fatiguelife {'c': 3}
foldcauchy {'c': 3}
f {'dfn': 1, 'dfd': 1}
foldnorm {'c': 1}
weibull_min {'c': 1}
weibull_max {'c': 1}
genlogistic {'c': 1}
genpareto {'c': 1}
genexpon {'a': 2, 'b': 3, 'c': 1}
genextreme {'c': 1}
gamma {'a': 2}
erlang {'a': 2}
gengamma {'a': 2, 'c': 1}
genhalflogistic {'c': 1}
gompertz {'c': 1}
gumbel_r {}
gumbel_l {}
halfcauchy {}
halflogistic {}
halfnorm {}
hypsecant {}
gausshyper {'a': 2, 'b': 3, 'c': 1, 'z': 1}
invgamma {'a': 1}
invgauss {'mu': 1}
norminvgauss {'a': 2, 'b': 1}
invweibull {'c': 1}
johnsonsb {'a': 2, 'b': 1}
johnsonsu {'a': 2, 'b': 1}
laplace {}
levy {}
levy_l {}
levy_stable {'alpha': 1, 'beta': 1}
logistic {}
loggamma {'c': 1}
loglaplace {'c': 1}
lognorm {'s': 1}
gilbrat {}
maxwell {}
mielke {'k': 1, 's': 1}
kappa4 {'h': 1, 'k': 1}
kappa3 {'a': 1}
moyal {}
nakagami {'nu': 1}
ncx2 {'df': 10, 'nc': 1}
t {'df': 10}
nct {'df': 10, 'nc': 1}
pareto {'b': 2}
lomax {'c': 2}
pearson3 {'skew': 2}
powerlaw {'a': 1}
powerlognorm {'c': 2, 's': 1}
powernorm {'c': 2}
rdist {'c': 2}
rayleigh {}
reciprocal {'a': 2, 'b': 3}
rice {'b': 2}
recipinvgauss {'mu': 2}
semicircular {}
skewnorm {'a': 1}
trapz {'c': 0, 'd': 1}
triang {'c': 1}
truncexpon {'b': 2}
truncnorm {'a': 2, 'b': 3}
tukeylambda {'lam': 2}
uniform {}
vonmises {'kappa': 2}
vonmises_line {'kappa': 2}
wald {}
wrapcauchy {'c': 0.5}
gennorm {'beta': 2}
halfgennorm {'beta': 2}
crystalball {'beta': 2, 'm': 2}
argus {'chi': 1}
..

=================================== FAILURES ===================================
__ TestSensitivityAnalysis.test_analytic_sobol_indices_from_gaussian_process ___

self = <pyapprox.analysis.tests.test_sensitivity_analysis.TestSensitivityAnalysis testMethod=test_analytic_sobol_indices_from_gaussian_process>

    def test_analytic_sobol_indices_from_gaussian_process(self):
        from pyapprox.benchmarks.benchmarks import setup_benchmark
        from pyapprox.surrogates.approximate import approximate
        benchmark = setup_benchmark("ishigami", a=7, b=0.1)
        nvars = benchmark.variable.num_vars()
    
        ntrain_samples = 500
        train_samples = sobol_sequence(
            nvars, ntrain_samples, variable=benchmark.variable)
    
        train_vals = benchmark.fun(train_samples)
        approx = approximate(
            train_samples, train_vals, 'gaussian_process', {
                'nu': np.inf, 'normalize_y': True, 'alpha': 1e-10}).approx
    
        nsobol_samples = int(1e4)
        from pyapprox.surrogates.approximate import compute_l2_error
        error = compute_l2_error(
            approx, benchmark.fun, benchmark.variable,
            nsobol_samples, rel=True)
        print(error)
    
        order = 2
        interaction_terms = compute_hyperbolic_indices(nvars, order)
        interaction_terms = interaction_terms[:, np.where(
            interaction_terms.max(axis=0) == 1)[0]]
    
>       result = analytic_sobol_indices_from_gaussian_process(
            approx, benchmark.variable, interaction_terms,
            ngp_realizations=1000, stat_functions=(np.mean, np.std),
            ninterpolation_samples=2000, ncandidate_samples=3000,
            use_cholesky=False, alpha=1e-8)

pyapprox/analysis/tests/test_sensitivity_analysis.py:497: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/analysis/sensitivity_analysis.py:904: in analytic_sobol_indices_from_gaussian_process
    _compute_expected_sobol_indices(
pyapprox/surrogates/gaussianprocess/gaussian_process.py:2468: in _compute_expected_sobol_indices
    get_gaussian_process_squared_exponential_kernel_1d_integrals(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[-3.14159265,  0.        ,  1.57079633, ..., -0.89277682,
         2.24881584, -1.67817498],
       [-3.1415926...595, -1.42930256],
       [-3.14159265,  0.        , -1.57079633, ...,  1.84876444,
         3.10540151, -3.11746522]])
length_scale = array([1.67436079, 1.31055723, 1.26718937])
variable = <pyapprox.variables.joint.IndependentMarginalsVariable object at 0x13d9e6610>
transform_quad_rules = True, nquad_samples = 50, skip_xi_1 = True

    def get_gaussian_process_squared_exponential_kernel_1d_integrals(
            X_train, length_scale, variable, transform_quad_rules,
            nquad_samples=50, skip_xi_1=False):
        nvars = variable.num_vars()
        degrees = [nquad_samples]*nvars
>       univariate_quad_rules, pce = get_univariate_quadrature_rules_from_variable(
            variable, degrees)
E       ValueError: too many values to unpack (expected 2)

pyapprox/surrogates/gaussianprocess/gaussian_process.py:643: ValueError
________________ TestBenchmarks.test_cantilever_beam_gradients _________________

self = <pyapprox.benchmarks.tests.test_benchmarks.TestBenchmarks testMethod=test_cantilever_beam_gradients>

    def test_cantilever_beam_gradients(self):
        benchmark = setup_benchmark('cantilever_beam')
        from pyapprox.interface.wrappers import ActiveSetVariableModel
        fun = ActiveSetVariableModel(
            benchmark.fun,
            benchmark.variable.num_vars()+benchmark.design_variable.num_vars(),
            benchmark.variable.get_statistics('mean'),
            benchmark.design_var_indices)
        jac = ActiveSetVariableModel(
            benchmark.jac,
            benchmark.variable.num_vars()+benchmark.design_variable.num_vars(),
            benchmark.variable.get_statistics('mean'),
            benchmark.design_var_indices)
        init_guess = 2*np.ones((2, 1))
        errors = check_gradients(
            fun, jac, init_guess, disp=True)
        assert errors.min() < 4e-7
    
        constraint_fun = ActiveSetVariableModel(
            benchmark.constraint_fun,
            benchmark.variable.num_vars()+benchmark.design_variable.num_vars(),
            benchmark.variable.get_statistics('mean'),
            benchmark.design_var_indices)
        constraint_jac = ActiveSetVariableModel(
            benchmark.constraint_jac,
            benchmark.variable.num_vars()+benchmark.design_variable.num_vars(),
            benchmark.variable.get_statistics('mean'),
            benchmark.design_var_indices)
        init_guess = 2*np.ones((2, 1))
        errors = check_gradients(
            constraint_fun, constraint_jac, init_guess, disp=True)
        assert errors.min() < 4e-7
    
        nsamples = 10
        samples = benchmark.variable.rvs(nsamples)
        constraint_fun = ActiveSetVariableModel(
            benchmark.constraint_fun,
            benchmark.variable.num_vars()+benchmark.design_variable.num_vars(),
            samples, benchmark.design_var_indices)
        constraint_jac = ActiveSetVariableModel(
            benchmark.constraint_jac,
            benchmark.variable.num_vars()+benchmark.design_variable.num_vars(),
            samples, benchmark.design_var_indices)
        init_guess = 2*np.ones((2, 1))
        errors = check_gradients(
            lambda x: constraint_fun(x).flatten(order='F'), constraint_jac,
            init_guess, disp=True)
>       assert errors.min() < 4e-7
E       AssertionError: assert 0.924035256772789 < 4e-07
E        +  where 0.924035256772789 = <built-in method min of numpy.ndarray object at 0x14a450b70>()
E        +    where <built-in method min of numpy.ndarray object at 0x14a450b70> = array([9.24035257e-01, 2.60296222e+00, 2.19540512e+01, 2.15378107e+02,\n       2.14961429e+03, 2.14919758e+04, 2.149155...174e+06,\n       2.14915132e+07, 2.14915128e+08, 2.14915127e+09, 2.14915127e+10,\n       2.14915127e+11, 2.14915127e+12]).min

pyapprox/benchmarks/tests/test_benchmarks.py:94: AssertionError
____ TestBayesianOED.test_numerical_gaussian_prediction_deviation_based_oed ____

self = <pyapprox.expdesign.tests.test_bayesian_oed.TestBayesianOED testMethod=test_numerical_gaussian_prediction_deviation_based_oed>

    def test_numerical_gaussian_prediction_deviation_based_oed(self):
>       self.check_numerical_gaussian_prediction_deviation_based_oed(
            False, "dev-pred", None, None, 2, 30, "gauss", [1e-15, 1e-15])

pyapprox/expdesign/tests/test_bayesian_oed.py:1810: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pyapprox.expdesign.tests.test_bayesian_oed.TestBayesianOED testMethod=test_numerical_gaussian_prediction_deviation_based_oed>
nonlinear = False, oed_type = 'dev-pred', deviation_quantile = None
risk_quantile = None, nouter_loop_samples = 2, ninner_loop_samples_1d = 30
quad_method = 'gauss', tols = [1e-15, 1e-15]

    def check_numerical_gaussian_prediction_deviation_based_oed(
            self, nonlinear, oed_type, deviation_quantile, risk_quantile,
            nouter_loop_samples, ninner_loop_samples_1d, quad_method, tols):
        ndesign = 2
        degree = 1
        ncandidates = 3
        noise_std = 1
        pre_collected_design_indices = [1]
        risk_quantile = None
    
        if deviation_quantile is None:
            deviation_fun = get_deviation_fun("std")
        else:
            deviation_fun = get_deviation_fun(
                "cvar", {"quantile": deviation_quantile,
                         "samples_sorted": False})
    
        def basis_matrix(degree, samples):
            return samples.T**np.arange(degree+1)[None, :]
    
        nprediction_samples = ncandidates
        xx, ww = piecewise_univariate_linear_quad_rule(
            [-1, 1], nprediction_samples)
        ww /= 2.0  # assume uniform distribution over prediction space
        prediction_candidates = xx[None, :]
    
        if risk_quantile is None:
            risk_fun = oed_average_prediction_deviation
        else:
            risk_fun = partial(
                conditional_value_at_risk, alpha=risk_quantile, weights=ww,
                prob=False)
    
        nrandom_vars = degree+1
        prior_variable = IndependentMarginalsVariable(
            [stats.norm(0, .1)]*nrandom_vars)
    
        design_candidates = np.linspace(-1, 1, ncandidates)[None, :]
        obs_mat = basis_matrix(degree, design_candidates)
        pred_mat = basis_matrix(degree, prediction_candidates)
        # pred_mat = pred_mat[:1, :]  # this line is hack for debugging
    
        (prior_mean, prior_cov, noise_cov, prior_cov_inv,
         noise_cov_inv) = setup_linear_gaussian_model_inference(
            prior_variable, noise_std, obs_mat)
    
        round_decimals = 3
        collected_indices, analytical_results = \
            linear_gaussian_prediction_deviation_based_oed(
                design_candidates, ndesign, prior_mean, prior_cov,
                noise_cov, obs_mat, pred_mat, risk_fun,
                pre_collected_design_indices, oed_type, deviation_quantile,
                round_decimals=round_decimals, nonlinear=nonlinear)
    
        def obs_fun(x): return obs_mat.dot(x).T
    
        def qoi_fun(x):
            vals = pred_mat.dot(x).T
            if not nonlinear:
                return vals
            return np.exp(vals)
    
        oed, oed_results = run_bayesian_batch_deviation_oed(
            prior_variable, obs_fun, qoi_fun, noise_std,
            design_candidates, pre_collected_design_indices,
            deviation_fun, risk_fun,
            ndesign, nouter_loop_samples, ninner_loop_samples_1d,
            quad_method, return_all=True, rounding_decimals=round_decimals)
    
        # print(collected_indices, oed.collected_design_indices)
        for jj in range(ncandidates):
            print(analytical_results[0][jj]["expected_deviations"][:, 0],
                  oed_results[0][jj]["expected_deviations"][:, 0])
    
        for ii in range(len(oed_results)):
            # print(ii)
            kk = ii+len(pre_collected_design_indices)
            assert (collected_indices[kk] == oed.collected_design_indices[kk])
            # analytical_results stores data for pre_collected_design_indices
            # and the subsequently collected indices, but the numerical code
            # does not
            anlyt_utility_vals = np.array(
                [d["utility_val"] for d in analytical_results[kk]])
            oed_utility_vals = np.array(
                [d["utility_val"] for d in oed_results[ii]])
            # print((anlyt_utility_vals-oed_utility_vals)/anlyt_utility_vals)
            # print(anlyt_utility_vals, '\n', oed_utility_vals)
>           assert np.allclose(
               oed_utility_vals, anlyt_utility_vals, rtol=tols[0])
E           AssertionError: assert False
E            +  where False = <function allclose at 0x1179ee790>(array([-0.01276038, -0.01276062, -0.01276038]), array([-0.12659272, -0.12682252, -0.12659272]), rtol=1e-15)
E            +    where <function allclose at 0x1179ee790> = np.allclose

pyapprox/expdesign/tests/test_bayesian_oed.py:1797: AssertionError
_ TestOptimalExperimentalDesign.test_homoscedastic_least_squares_goptimal_design _

self = <pyapprox.expdesign.tests.test_linear_oed.TestOptimalExperimentalDesign testMethod=test_homoscedastic_least_squares_goptimal_design>

    def test_homoscedastic_least_squares_goptimal_design(self):
        """
        Create G-optimal design
        """
        poly_degree = 2
        num_design_pts = 7
        design_samples = np.linspace(-1, 1, num_design_pts)
        # noise_multiplier = None
        design_factors = univariate_monomial_basis_matrix(
            poly_degree, design_samples)
        # pred_factors = design_factors
        pred_factors = univariate_monomial_basis_matrix(
            poly_degree, np.linspace(-1, 1, num_design_pts))
    
        opts = {'pred_factors': pred_factors}
        opt_problem = AlphabetOptimalDesign('G', design_factors, opts=opts)
        mu = opt_problem.solve({'iprint': 1, 'ftol': 1e-8})
        II = np.where(mu > 1e-5)[0]
        assert np.allclose(II, [0, 3, 6])
        assert np.allclose(np.ones(3)/3, mu[II])
    
        # check G gives same as D optimality. This holds due to equivalence
        # theorem
        opt_problem = AlphabetOptimalDesign('D', design_factors)
        mu_d = opt_problem.solve({'iprint': 1, 'ftol': 1e-8})
        assert np.allclose(mu, mu_d)
    
        # test high-level api for D optimality
>       selected_pts, mu_d = optimal_experimental_design(
            design_samples[np.newaxis, :], design_factors, 'D',
            regresion_type='lstsq', noise_multiplier=None)
E       TypeError: optimal_experimental_design() got an unexpected keyword argument 'regresion_type'

pyapprox/expdesign/tests/test_linear_oed.py:703: TypeError
__ TestOptimalExperimentalDesign.test_r_oed_objective_and_constraint_wrappers __

self = <pyapprox.expdesign.tests.test_linear_oed.TestOptimalExperimentalDesign testMethod=test_r_oed_objective_and_constraint_wrappers>

    def test_r_oed_objective_and_constraint_wrappers(self):
        poly_degree = 10
        num_design_pts = 101
        num_pred_pts = 51
        pred_samples = np.random.uniform(-1, 1, num_pred_pts)
        design_samples = np.linspace(-1, 1, num_design_pts)
        design_factors = univariate_monomial_basis_matrix(
            poly_degree, design_samples)
        pred_factors = univariate_monomial_basis_matrix(
            poly_degree, pred_samples)
        homog_outer_prods = compute_homoscedastic_outer_products(
            design_factors)
        goptimality_criterion_wrapper = partial(
            goptimality_criterion, homog_outer_prods, design_factors,
            pred_factors)
        mu = np.random.uniform(0, 1, (num_design_pts))
        mu /= mu.sum()
        obj, jac = goptimality_criterion_wrapper(mu)
    
        beta = 0.75
        pred_weights = np.ones(num_pred_pts)/num_pred_pts
        r_oed_objective_wrapper = partial(r_oed_objective, beta, pred_weights)
        r_oed_jac_wrapper = partial(
            r_oed_objective_jacobian, beta, pred_weights)
        x0 = np.concatenate([np.ones(num_design_pts+1), mu])[:, np.newaxis]
        diffs = check_gradients(
            r_oed_objective_wrapper, r_oed_jac_wrapper, x0)
        assert diffs.min() < 6e-5, diffs
    
        r_oed_constraint_wrapper = partial(
            r_oed_constraint_objective, num_design_pts,
            lambda x: goptimality_criterion_wrapper(x)[0])
        r_oed_constraint_jac_wrapper = partial(
            r_oed_constraint_jacobian, num_design_pts,
            lambda x: goptimality_criterion_wrapper(x)[1])
        x0 = np.concatenate([np.ones(num_pred_pts+1), mu])[:, np.newaxis]
        # from pyapprox import approx_jacobian
        # print(x0.shape)
        # print(approx_jacobian(r_oed_constraint_wrapper,x0[:,0]))
        diffs = check_gradients(
            r_oed_constraint_wrapper, r_oed_constraint_jac_wrapper, x0)
>       assert diffs.min() < 6e-5, diffs
E       AssertionError: array([9.75217621e-01, 2.80391208e+00, 2.82847874e+01, 2.84576309e+02,
E                2.84763365e+03, 2.84782211e+04, 2.847840...286e+06,
E                2.84784305e+07, 2.84784307e+08, 2.84784307e+09, 2.84784307e+10,
E                2.84784307e+11, 2.84784307e+12])
E       assert 0.9752176207353341 < 6e-05
E        +  where 0.9752176207353341 = <built-in method min of numpy.ndarray object at 0x13d988810>()
E        +    where <built-in method min of numpy.ndarray object at 0x13d988810> = array([9.75217621e-01, 2.80391208e+00, 2.82847874e+01, 2.84576309e+02,\n       2.84763365e+03, 2.84782211e+04, 2.847840...286e+06,\n       2.84784305e+07, 2.84784307e+08, 2.84784307e+09, 2.84784307e+10,\n       2.84784307e+11, 2.84784307e+12]).min

pyapprox/expdesign/tests/test_linear_oed.py:207: AssertionError
_______________________ TestCVMC.test_variance_reduction _______________________

self = <pyapprox.multifidelity.tests.test_control_variate_monte_carlo.TestCVMC testMethod=test_variance_reduction>

    def test_variance_reduction(self):
        ntrials = 1e4
        setup_model = setup_model_ensemble_tunable
        for estimator_type in ["acvis", "acvmf", "mfmc"]:
>           self.check_variance(
                estimator_type, setup_model, 1e3, ntrials, 2e-2)

pyapprox/multifidelity/tests/test_control_variate_monte_carlo.py:475: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/multifidelity/tests/test_control_variate_monte_carlo.py:452: in check_variance
    estimate_variance(
pyapprox/multifidelity/monte_carlo_estimators.py:1017: in estimate_variance
    estimator.set_optimized_params(
pyapprox/multifidelity/monte_carlo_estimators.py:451: in set_optimized_params
    self.nsamples_per_model = get_nsamples_per_model(
pyapprox/multifidelity/control_variate_monte_carlo.py:1655: in get_nsamples_per_model
    return cast_to_integers(nsamples_per_model)
pyapprox/multifidelity/control_variate_monte_carlo.py:100: in cast_to_integers
    return check_safe_cast_to_integers(array)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

array = array([900.9009009, 900.9009009])

    def check_safe_cast_to_integers(array):
        array_int = np.array(np.round(array), dtype=int)
        if not np.allclose(array, array_int, 1e-15):
>           raise ValueError("Arrays entries are not integers")
E           ValueError: Arrays entries are not integers

pyapprox/multifidelity/control_variate_monte_carlo.py:95: ValueError
___________________ TestCVMC.test_variance_reduction_acvgmf ____________________

self = <pyapprox.multifidelity.tests.test_control_variate_monte_carlo.TestCVMC testMethod=test_variance_reduction_acvgmf>

    def test_variance_reduction_acvgmf(self):
        estimator_type = "acvgmf"
        target_cost = 1e3
        ntrials = 1e4
        setup_model = setup_model_ensemble_polynomial
        model_ensemble, cov, costs, variable = setup_model()
        nmodels = cov.shape[0]
        KL_sets = [[4, 1], [3, 1], [3, 2], [3, 3], [2, 1], [2, 2]]
        for K, L in KL_sets:
            if K == nmodels-1:
                recursion_index = np.zeros(nmodels-1, dtype=int)
            else:
                recursion_index = np.hstack(
                    (np.zeros(K), np.ones(nmodels-1-K)*L)).astype(int)
            print(K, L, recursion_index)
            estimator = get_estimator(
                estimator_type, cov, costs, variable,
                recursion_index=recursion_index)
            nsample_ratios, variance, rounded_target_cost = \
                estimator.allocate_samples(target_cost)
            nsamples_per_model = estimator.get_nsamples_per_model(
                rounded_target_cost, nsample_ratios)
            print(variance, rounded_target_cost, nsamples_per_model)
    
            samples, values = estimator.generate_data(model_ensemble)
            # Check sizes of samples allocated to each model are correct
            for ii in range(1, K+1):
                assert values[ii][0].shape[0] == nsamples_per_model[0]
            for ii in range(K+1, nmodels):
                assert values[ii][0].shape[0] == values[L][1].shape[0]
            for ii in range(1, K+1):
                assert values[ii][1].shape[0] == nsamples_per_model[ii]
            for ii in range(K+1, nmodels):
                assert values[ii][1].shape[0] == values[ii][1].shape[0]
    
>           self.check_variance(
                estimator_type, setup_model, target_cost, ntrials, 3e-2,
                {"recursion_index": recursion_index})

pyapprox/multifidelity/tests/test_control_variate_monte_carlo.py:526: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/multifidelity/tests/test_control_variate_monte_carlo.py:452: in check_variance
    estimate_variance(
pyapprox/multifidelity/monte_carlo_estimators.py:1017: in estimate_variance
    estimator.set_optimized_params(
pyapprox/multifidelity/monte_carlo_estimators.py:451: in set_optimized_params
    self.nsamples_per_model = get_nsamples_per_model(
pyapprox/multifidelity/control_variate_monte_carlo.py:1655: in get_nsamples_per_model
    return cast_to_integers(nsamples_per_model)
pyapprox/multifidelity/control_variate_monte_carlo.py:100: in cast_to_integers
    return check_safe_cast_to_integers(array)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

array = array([5.18134715, 5.18134715])

    def check_safe_cast_to_integers(array):
        array_int = np.array(np.round(array), dtype=int)
        if not np.allclose(array, array_int, 1e-15):
>           raise ValueError("Arrays entries are not integers")
E           ValueError: Arrays entries are not integers

pyapprox/multifidelity/control_variate_monte_carlo.py:95: ValueError
_ TestCVARRegression.test_smooth_conditional_value_at_risk_composition_gradient _

self = <pyapprox.optimization.tests.test_cvar_regression.TestCVARRegression testMethod=test_smooth_conditional_value_at_risk_composition_gradient>

    def test_smooth_conditional_value_at_risk_composition_gradient(self):
        nsamples, nvars = 4, 2
        smoother_type, eps, alpha = 0, 1e-1, 0.7
>       self.help_check_smooth_conditional_value_at_risk_composition_gradient(
            smoother_type, eps, alpha, nsamples, nvars)

pyapprox/optimization/tests/test_cvar_regression.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pyapprox.optimization.tests.test_cvar_regression.TestCVARRegression testMethod=test_smooth_conditional_value_at_risk_composition_gradient>
smoother_type = 0, eps = 0.1, alpha = 0.7, nsamples = 4, nvars = 2

    def help_check_smooth_conditional_value_at_risk_composition_gradient(
            self, smoother_type, eps, alpha, nsamples, nvars):
        samples = np.arange(nsamples*nvars).reshape(nvars, nsamples)
        t = 0.1
        x0 = np.array([2, 3, t])[:, np.newaxis]
        def fun(x): return (np.sum((x*samples)**2, axis=0).T)[:, np.newaxis]
        def jac(x): return 2*(x*samples**2).T
    
        errors = check_gradients(fun, jac, x0[:2], disp=False)
>       assert (errors.min() < 1e-6)
E       AssertionError: assert 3.1292811937997977 < 1e-06
E        +  where 3.1292811937997977 = <built-in method min of numpy.ndarray object at 0x13d987690>()
E        +    where <built-in method min of numpy.ndarray object at 0x13d987690> = array([3.12928119e+00, 3.54063673e+01, 3.57938470e+02, 3.58323520e+03,\n       3.58362001e+04, 3.58365848e+05, 3.583662...272e+07,\n       3.58366275e+08, 3.58366276e+09, 3.58366276e+10, 3.58366276e+11,\n       3.58366276e+12, 3.58366276e+13]).min

pyapprox/optimization/tests/test_cvar_regression.py:83: AssertionError
_______________ TestL1Minimization.test_smooth_l1_norm_gradients _______________

self = <pyapprox.optimization.tests.test_l1_minimization.TestL1Minimization testMethod=test_smooth_l1_norm_gradients>

>   ???

pyapprox/optimization/tests/test_l1_minimization.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fun = <pyapprox.optimization.optimization.ScipyMinimizeObjectiveAsPyapproxFunction object at 0x13de1f190>
jac = <pyapprox.optimization.optimization.ScipyMinimizeObjectiveJacAsPyapproxJac object at 0x13de1ffa0>
zz = array([[-0.75300038],
       [-0.56441821],
       [ 0.47139518],
       [-0.3798639 ],
       [-0.71455786]])
plot = False, disp = False, rel = True
direction = array([[-0.19617642],
       [-0.08428268],
       [ 0.43872622],
       [-0.78308364],
       [-0.38563007]])
jacp = None

    def check_gradients(fun, jac, zz, plot=False, disp=True, rel=True,
                        direction=None, jacp=None):
        """
        Compare a user specified jacobian with the jacobian computed with finite
        difference with multiple step sizes.
    
        Parameters
        ----------
        fun : callable
    
            A function with signature
    
            ``fun(z) -> np.ndarray``
    
            where ``z`` is a 2D np.ndarray with shape (nvars, 1) and the
            output is a 2D np.ndarray with shape (nqoi, 1)
    
        jac : callable
            The jacobian of ``fun`` with signature
    
            ``jac(z) -> np.ndarray``
    
            where ``z`` is a 2D np.ndarray with shape (nvars, 1) and the
            output is a 2D np.ndarray with shape (nqoi, nvars)
    
        zz : np.ndarray (nvars, 1)
            A sample of ``z`` at which to compute the gradient
    
        plot : boolean
            Plot the errors as a function of the finite difference step size
    
        disp : boolean
            True - print the errors
            False - do not print
    
        rel : boolean
            True - compute the relative error in the directional derivative,
            i.e. the absolute error divided by the directional derivative using
            ``jac``.
            False - compute the absolute error in the directional derivative
    
        direction : np.ndarray (nvars, 1)
            Direction to which Jacobian is applied. Default is None in which
            case random direction is chosen.
    
        Returns
        -------
        errors : np.ndarray (14, nqoi)
            The errors in the directional derivative of ``fun`` at 14 different
            values of finite difference tolerance for each quantity of interest
        """
        assert zz.ndim == 2
        assert zz.shape[1] == 1
    
        if direction is None:
            direction = np.random.normal(0, 1, (zz.shape[0], 1))
            direction /= np.linalg.norm(direction)
        assert direction.ndim == 2 and direction.shape[1] == 1
    
        if ((jacp is None and jac is None) or
                (jac is not None and jacp is not None)):
            raise Exception('Must specify jac or jacp')
    
        if callable(jac):
            function_val = fun(zz)
            grad_val = jac(zz)  # .squeeze()
            directional_derivative = grad_val.dot(direction).squeeze()
        elif callable(jacp):
            directional_derivative = jacp(zz, direction)
        elif jac is True:
            function_val, grad_val = fun(zz)
            directional_derivative = grad_val.dot(direction).squeeze()
        else:
            raise Exception
    
        fd_eps = np.logspace(-13, 0, 14)[::-1]
        errors = []
        row_format = "{:<12} {:<25} {:<25} {:<25}"
        if disp:
            if rel:
                print(
                    row_format.format(
                        "Eps", "norm(jv)", "norm(jv_fd)",
                        "Rel. Errors"))
            else:
                print(row_format.format(
                    "Eps", "norm(jv)", "norm(jv_fd)",
                    "Abs. Errors"))
        for ii in range(fd_eps.shape[0]):
            zz_perturbed = zz.copy()+fd_eps[ii]*direction
            perturbed_function_val = fun(zz_perturbed)
            if jac:
>               perturbed_function_val = perturbed_function_val[0].squeeze()
E               IndexError: invalid index to scalar variable.

pyapprox/util/utilities.py:1008: IndexError
_________ TestFirstOrderStochasticDominance.test_objective_derivatives _________

self = <pyapprox.optimization.tests.test_stochastic_dominance.TestFirstOrderStochasticDominance testMethod=test_objective_derivatives>

    def test_objective_derivatives(self):
        smoother_type, eps = 'log', 5e-1
        nsamples, degree = 10, 1
    
        samples, values, fun, jac, probabilities, ncoef, x0 = \
            self.setup_linear_regression_problem(nsamples, degree, eps)
    
        x0 -= eps/3
    
        eta = np.arange(nsamples//2, nsamples)
        problem = FSDOptProblem(
            values, fun, jac, None, eta, probabilities, smoother_type, eps,
            ncoef)
    
        # assert smooth function is shifted correctly (local methods only)
        # assert problem.smooth_fun(np.array([[0.]])) == 1.0
    
>       err = check_gradients(
            problem.objective_fun, problem.objective_jac, x0, rel=False)

pyapprox/optimization/tests/test_stochastic_dominance.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fun = <bound method FSDOptProblem.objective_fun of <pyapprox.optimization.first_order_stochastic_dominance.FSDOptProblem object at 0x13e9af310>>
jac = <bound method FSDOptProblem.objective_jac of <pyapprox.optimization.first_order_stochastic_dominance.FSDOptProblem object at 0x13e9af310>>
zz = array([[1.10004992],
       [2.8966678 ]]), plot = False, disp = True
rel = False, direction = array([[ 0.93583067],
       [-0.35244993]])
jacp = None

    def check_gradients(fun, jac, zz, plot=False, disp=True, rel=True,
                        direction=None, jacp=None):
        """
        Compare a user specified jacobian with the jacobian computed with finite
        difference with multiple step sizes.
    
        Parameters
        ----------
        fun : callable
    
            A function with signature
    
            ``fun(z) -> np.ndarray``
    
            where ``z`` is a 2D np.ndarray with shape (nvars, 1) and the
            output is a 2D np.ndarray with shape (nqoi, 1)
    
        jac : callable
            The jacobian of ``fun`` with signature
    
            ``jac(z) -> np.ndarray``
    
            where ``z`` is a 2D np.ndarray with shape (nvars, 1) and the
            output is a 2D np.ndarray with shape (nqoi, nvars)
    
        zz : np.ndarray (nvars, 1)
            A sample of ``z`` at which to compute the gradient
    
        plot : boolean
            Plot the errors as a function of the finite difference step size
    
        disp : boolean
            True - print the errors
            False - do not print
    
        rel : boolean
            True - compute the relative error in the directional derivative,
            i.e. the absolute error divided by the directional derivative using
            ``jac``.
            False - compute the absolute error in the directional derivative
    
        direction : np.ndarray (nvars, 1)
            Direction to which Jacobian is applied. Default is None in which
            case random direction is chosen.
    
        Returns
        -------
        errors : np.ndarray (14, nqoi)
            The errors in the directional derivative of ``fun`` at 14 different
            values of finite difference tolerance for each quantity of interest
        """
        assert zz.ndim == 2
        assert zz.shape[1] == 1
    
        if direction is None:
            direction = np.random.normal(0, 1, (zz.shape[0], 1))
            direction /= np.linalg.norm(direction)
        assert direction.ndim == 2 and direction.shape[1] == 1
    
        if ((jacp is None and jac is None) or
                (jac is not None and jacp is not None)):
            raise Exception('Must specify jac or jacp')
    
        if callable(jac):
            function_val = fun(zz)
            grad_val = jac(zz)  # .squeeze()
            directional_derivative = grad_val.dot(direction).squeeze()
        elif callable(jacp):
            directional_derivative = jacp(zz, direction)
        elif jac is True:
            function_val, grad_val = fun(zz)
            directional_derivative = grad_val.dot(direction).squeeze()
        else:
            raise Exception
    
        fd_eps = np.logspace(-13, 0, 14)[::-1]
        errors = []
        row_format = "{:<12} {:<25} {:<25} {:<25}"
        if disp:
            if rel:
                print(
                    row_format.format(
                        "Eps", "norm(jv)", "norm(jv_fd)",
                        "Rel. Errors"))
            else:
                print(row_format.format(
                    "Eps", "norm(jv)", "norm(jv_fd)",
                    "Abs. Errors"))
        for ii in range(fd_eps.shape[0]):
            zz_perturbed = zz.copy()+fd_eps[ii]*direction
            perturbed_function_val = fun(zz_perturbed)
            if jac:
>               perturbed_function_val = perturbed_function_val[0].squeeze()
E               IndexError: invalid index to scalar variable.

pyapprox/util/utilities.py:1008: IndexError
__ TestGaussianProcess.test_compute_sobol_indices_gaussian_process_uniform_2d __

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestGaussianProcess testMethod=test_compute_sobol_indices_gaussian_process_uniform_2d>

    def test_compute_sobol_indices_gaussian_process_uniform_2d(self):
        nvars = 2
        a = np.array([1, 0.25])
        # a = np.array([1, 1])
    
        def func(x):
            return np.sum(a[:, None]*(2*x-1)**2, axis=0)[:, np.newaxis]
    
        ntrain_samples = 100
        # train_samples = np.random.uniform(0, 1, (nvars, ntrain_samples))
        from pyapprox.expdesign.low_discrepancy_sequences import sobol_sequence
        train_samples = sobol_sequence(nvars, ntrain_samples)
        train_vals = func(train_samples)
    
        univariate_variables = [stats.uniform(0, 1)]*nvars
        variable = IndependentMarginalsVariable(
            univariate_variables)
        # var_trans = AffineTransform(variable)
    
        nu = np.inf
        kernel_var = 1.
        length_scale = np.array([1]*nvars)
        kernel = Matern(length_scale, length_scale_bounds=(1e-2, 10), nu=nu)
        kernel = ConstantKernel(
            constant_value=kernel_var, constant_value_bounds='fixed')*kernel
        gp = GaussianProcess(kernel, n_restarts_optimizer=1, alpha=1e-6)
        # gp.set_variable_transformation(var_trans)
        gp.fit(train_samples, train_vals)
    
        validation_samples = np.random.uniform(0, 1, (nvars, ntrain_samples))
        validation_vals = func(validation_samples)
        error = np.linalg.norm(validation_vals - gp(validation_samples)) / \
            np.linalg.norm(validation_vals)
        print(error)
    
        nquad_samples = 50
        expected_random_mean, variance_random_mean, expected_random_var,\
>           variance_random_var = integrate_gaussian_process(
                gp, variable, nquad_samples=nquad_samples)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:770: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/gaussianprocess/gaussian_process.py:575: in integrate_gaussian_process
    result = integrate_gaussian_process_squared_exponential_kernel(
pyapprox/surrogates/gaussianprocess/gaussian_process.py:787: in integrate_gaussian_process_squared_exponential_kernel
    get_gaussian_process_squared_exponential_kernel_1d_integrals(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[0.       , 0.5      , 0.75     , 0.25     , 0.375    , 0.875    ,
        0.625    , 0.125    , 0.1875   , 0.6...0.0546875, 0.5546875, 0.1796875, 0.6796875, 0.4296875, 0.9296875,
        0.1328125, 0.6328125, 0.3828125, 0.8828125]])
length_scale = array([0.50120915, 1.11192208])
variable = <pyapprox.variables.joint.IndependentMarginalsVariable object at 0x13cb1da90>
transform_quad_rules = True, nquad_samples = 50, skip_xi_1 = False

    def get_gaussian_process_squared_exponential_kernel_1d_integrals(
            X_train, length_scale, variable, transform_quad_rules,
            nquad_samples=50, skip_xi_1=False):
        nvars = variable.num_vars()
        degrees = [nquad_samples]*nvars
        univariate_quad_rules = get_univariate_quadrature_rules_from_variable(
            variable, degrees)
    
        lscale = np.atleast_1d(length_scale)
        # tau, u = 1, 1
        # ntrain_samples = X_train.shape[1]
        # P = np.ones((ntrain_samples, ntrain_samples))
        # lamda = np.ones(ntrain_samples)
        # Pi = np.ones((ntrain_samples, ntrain_samples))
        # xi_1, nu = 1, 1
    
        tau_list, P_list, u_list, lamda_list = [], [], [], []
        Pi_list, nu_list, xi_1_list = [], [], []
        for ii in range(nvars):
            # TODO only compute quadrature once for each unique quadrature rules
            # But all quantities must be computed for all dimensions because
            # distances depend on either of both dimension dependent length scale
            # and training sample values
            # But others like u only needed to be computed for each unique
            # Quadrature rule and raised to the power equal to the number of
            # instances of a unique rule
    
            # Define distance function
            # dist_func = partial(cdist, metric='sqeuclidean')
    
            # Training samples of ith variable
            xtr = X_train[ii:ii+1, :]
    
            # Get 1D quadrature rule
>           xx_1d, ww_1d = univariate_quad_rules[ii](degrees[ii]+1)
E           TypeError: 'functools.partial' object is not subscriptable

pyapprox/surrogates/gaussianprocess/gaussian_process.py:672: TypeError
__ TestGaussianProcess.test_compute_sobol_indices_gaussian_process_uniform_3d __

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestGaussianProcess testMethod=test_compute_sobol_indices_gaussian_process_uniform_3d>

    def test_compute_sobol_indices_gaussian_process_uniform_3d(self):
        nvars = 3
        coef = np.array([1, 0.25, 0.25])
    
        def func(x):
            return (np.prod(x[:2], axis=0)+np.sum(
                coef[:, None]*(2*x-1)**2, axis=0))[:, np.newaxis]
    
        ntrain_samples = 300
        # train_samples = np.random.uniform(0, 1, (nvars, ntrain_samples))
        train_samples = sobol_sequence(nvars, ntrain_samples)
        train_vals = func(train_samples)
    
        univariate_variables = [stats.uniform(0, 1)]*nvars
        variable = IndependentMarginalsVariable(
            univariate_variables)
        # var_trans = AffineTransform(variable)
    
        nu = np.inf
        kernel_var = 1.
        length_scale = np.array([1]*nvars)
        kernel = Matern(length_scale, length_scale_bounds=(1e-2, 10), nu=nu)
        kernel = ConstantKernel(
            constant_value=kernel_var, constant_value_bounds='fixed')*kernel
        gp = GaussianProcess(kernel, n_restarts_optimizer=1, alpha=1e-6,
                             normalize_y=True)
        # gp.set_variable_transformation(var_trans)
        gp.fit(train_samples, train_vals)
    
        validation_samples = np.random.uniform(0, 1, (nvars, ntrain_samples))
        validation_vals = func(validation_samples)
        error = np.linalg.norm(validation_vals - gp(validation_samples)) / \
            np.linalg.norm(validation_vals)
        print(error)
    
        pce = approximate(
            train_samples, train_vals, 'polynomial_chaos',
            {'basis_type': 'hyperbolic_cross', 'variable': variable,
             'options': {'max_degree': 4}}).approx
        assert np.linalg.norm(validation_vals - pce(validation_samples)) / \
            np.linalg.norm(validation_vals) < 1e-15
    
        pce_interaction_terms, pce_sobol_indices = get_sobol_indices(
            pce.get_coefficients(), pce.get_indices(), max_order=3)
        pce_main_effects, pce_total_effects = \
            get_main_and_total_effect_indices_from_pce(
                pce.coefficients, pce.indices)
    
        interaction_terms = np.zeros(
            (nvars, len(pce_interaction_terms)), dtype=int)
        for ii, idx in enumerate(pce_interaction_terms.T):
            interaction_terms[idx, ii] = 1
    
        nquad_samples = 100
        sobol_indices, total_effects, mean, variance = \
>           compute_expected_sobol_indices(
                gp, variable, interaction_terms, nquad_samples=nquad_samples)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:855: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/gaussianprocess/gaussian_process.py:2457: in compute_expected_sobol_indices
    result = _compute_expected_sobol_indices(
pyapprox/surrogates/gaussianprocess/gaussian_process.py:2468: in _compute_expected_sobol_indices
    get_gaussian_process_squared_exponential_kernel_1d_integrals(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[0.        , 0.5       , 0.75      , 0.25      , 0.375     ,
        0.875     , 0.625     , 0.125     , 0.1875..., 0.84960938, 0.72460938, 0.22460938, 0.97460938,
        0.47460938, 0.91210938, 0.41210938, 0.66210938, 0.16210938]])
length_scale = array([0.35072826, 0.84989217, 0.88493658])
variable = <pyapprox.variables.joint.IndependentMarginalsVariable object at 0x13de281f0>
transform_quad_rules = True, nquad_samples = 100, skip_xi_1 = True

    def get_gaussian_process_squared_exponential_kernel_1d_integrals(
            X_train, length_scale, variable, transform_quad_rules,
            nquad_samples=50, skip_xi_1=False):
        nvars = variable.num_vars()
        degrees = [nquad_samples]*nvars
>       univariate_quad_rules = get_univariate_quadrature_rules_from_variable(
            variable, degrees)
E       ValueError: too many values to unpack (expected 2)

pyapprox/surrogates/gaussianprocess/gaussian_process.py:643: ValueError
_________ TestGaussianProcess.test_integrate_gaussian_process_gaussian _________

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestGaussianProcess testMethod=test_integrate_gaussian_process_gaussian>

    def test_integrate_gaussian_process_gaussian(self):
    
        nvars = 2
        def func(x): return np.sum(x**2, axis=0)[:, np.newaxis]
    
        mu_scalar, sigma_scalar = 3, 1
        # mu_scalar, sigma_scalar = 0, 1
    
        univariate_variables = [stats.norm(mu_scalar, sigma_scalar)]*nvars
        variable = IndependentMarginalsVariable(
            univariate_variables)
    
        lb, ub = univariate_variables[0].interval(0.99999)
    
        ntrain_samples = 5
        # ntrain_samples = 20
    
        train_samples = cartesian_product(
            [np.linspace(lb, ub, ntrain_samples)]*nvars)
        train_vals = func(train_samples)
    
        nu = np.inf
        nvars = train_samples.shape[0]
        length_scale = np.array([1]*nvars)
        kernel = Matern(length_scale, length_scale_bounds=(1e-2, 10), nu=nu)
        # fix kernel variance
        kernel = ConstantKernel(
            constant_value=2., constant_value_bounds='fixed')*kernel
        # optimize kernel variance
        # kernel = ConstantKernel(
        #    constant_value=3,constant_value_bounds=(0.1, 10))*kernel
        # optimize gp noise
        # kernel += WhiteKernel(noise_level_bounds=(1e-8, 1))
        # fix gp noise
        # kernel += WhiteKernel(noise_level=1e-5, noise_level_bounds='fixed')
        # white kernel K(x_i,x_j) is only nonzeros when x_i=x_j, i.e.
        # it is not used when calling gp.predict
        gp = GaussianProcess(kernel, n_restarts_optimizer=10, alpha=1e-8)
        gp.fit(train_samples, train_vals)
        # print(gp.kernel_)
    
        # xx=np.linspace(lb,ub,101)
        # plt.plot(xx,func(xx[np.newaxis,:]))
        # gp_mean,gp_std = gp(xx[np.newaxis,:],return_std=True)
        # gp_mean = gp_mean[:,0]
        # plt.plot(xx,gp_mean)
        # plt.plot(train_samples[0,:],train_vals[:,0],'o')
        # plt.fill_between(xx,gp_mean-2*gp_std,gp_mean+2*gp_std,alpha=0.5)
        # plt.show()
    
        # import time
        # t0 = time.time()
        expected_random_mean, variance_random_mean, expected_random_var,\
            variance_random_var, intermediate_quantities =\
>           integrate_gaussian_process(gp, variable, return_full=True,
                                       nquad_samples=100)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/gaussianprocess/gaussian_process.py:575: in integrate_gaussian_process
    result = integrate_gaussian_process_squared_exponential_kernel(
pyapprox/surrogates/gaussianprocess/gaussian_process.py:787: in integrate_gaussian_process_squared_exponential_kernel
    get_gaussian_process_squared_exponential_kernel_1d_integrals(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[-1.41717341,  0.79141329,  3.        ,  5.20858671,  7.41717341,
        -1.41717341,  0.79141329,  3.        ...8671,  5.20858671,  5.20858671,  5.20858671,
         7.41717341,  7.41717341,  7.41717341,  7.41717341,  7.41717341]])
length_scale = array([2.26040798, 2.26040798])
variable = <pyapprox.variables.joint.IndependentMarginalsVariable object at 0x13e9cb9d0>
transform_quad_rules = True, nquad_samples = 100, skip_xi_1 = False

    def get_gaussian_process_squared_exponential_kernel_1d_integrals(
            X_train, length_scale, variable, transform_quad_rules,
            nquad_samples=50, skip_xi_1=False):
        nvars = variable.num_vars()
        degrees = [nquad_samples]*nvars
        univariate_quad_rules = get_univariate_quadrature_rules_from_variable(
            variable, degrees)
    
        lscale = np.atleast_1d(length_scale)
        # tau, u = 1, 1
        # ntrain_samples = X_train.shape[1]
        # P = np.ones((ntrain_samples, ntrain_samples))
        # lamda = np.ones(ntrain_samples)
        # Pi = np.ones((ntrain_samples, ntrain_samples))
        # xi_1, nu = 1, 1
    
        tau_list, P_list, u_list, lamda_list = [], [], [], []
        Pi_list, nu_list, xi_1_list = [], [], []
        for ii in range(nvars):
            # TODO only compute quadrature once for each unique quadrature rules
            # But all quantities must be computed for all dimensions because
            # distances depend on either of both dimension dependent length scale
            # and training sample values
            # But others like u only needed to be computed for each unique
            # Quadrature rule and raised to the power equal to the number of
            # instances of a unique rule
    
            # Define distance function
            # dist_func = partial(cdist, metric='sqeuclidean')
    
            # Training samples of ith variable
            xtr = X_train[ii:ii+1, :]
    
            # Get 1D quadrature rule
>           xx_1d, ww_1d = univariate_quad_rules[ii](degrees[ii]+1)
E           TypeError: 'functools.partial' object is not subscriptable

pyapprox/surrogates/gaussianprocess/gaussian_process.py:672: TypeError
_________ TestGaussianProcess.test_integrate_gaussian_process_uniform __________

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestGaussianProcess testMethod=test_integrate_gaussian_process_uniform>

    def test_integrate_gaussian_process_uniform(self):
        nvars = 1
        constant = 1e3
        nugget = 0
        normalize_y = True
        def func(x): return constant*np.sum((2*x-.5)**2, axis=0)[:, np.newaxis]
    
        univariate_variables = [stats.uniform(0, 1)]
        variable = IndependentMarginalsVariable(
            univariate_variables)
        var_trans = AffineTransform(variable)
    
        ntrain_samples = 7
        train_samples = (np.cos(
            np.linspace(0, np.pi, ntrain_samples))[np.newaxis, :]+1)/2
        train_vals = func(train_samples)
    
        nu = np.inf
        kernel = Matern(length_scale_bounds=(1e-2, 10), nu=nu)
        # kernel needs to be multiplied by a constant kernel
        if not normalize_y:
            kernel = ConstantKernel(
                constant_value=constant, constant_value_bounds='fixed')*kernel
        gp = GaussianProcess(
            kernel, n_restarts_optimizer=5, normalize_y=normalize_y,
            alpha=nugget)
    
        # This code block shows that for same rand_noise different samples xx
        # will produce different realizations
        # xx = np.linspace(0, 1, 101)
        # rand_noise = np.random.normal(0, 1, (xx.shape[0], 1))
        # yy = gp.predict_random_realization(xx[None, :], rand_noise)
        # plt.plot(xx, yy)
        # xx = np.linspace(0, 1, 97)
        # rand_noise = np.random.normal(0, 1, (xx.shape[0], 1))
        # yy = gp.predict_random_realization(xx[None, :], rand_noise)
        # plt.plot(xx, yy)
        # plt.show()
    
        gp.set_variable_transformation(var_trans)
        gp.fit(train_samples, train_vals)
        # avoid training data when checking variance
        zz = np.linspace(0.01, 0.99, 100)
        mean, std = gp(zz[None, :], return_std=True)
        vals = gp.predict_random_realization(
            zz[None, :], 100000,
            truncated_svd={'nsingular_vals': 10, 'tol': 1e-8})
        assert np.allclose(vals.std(axis=1), std, rtol=5e-3)
    
        expected_random_mean, variance_random_mean, expected_random_var, \
>           variance_random_var = integrate_gaussian_process(gp, variable)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:483: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/gaussianprocess/gaussian_process.py:575: in integrate_gaussian_process
    result = integrate_gaussian_process_squared_exponential_kernel(
pyapprox/surrogates/gaussianprocess/gaussian_process.py:787: in integrate_gaussian_process_squared_exponential_kernel
    get_gaussian_process_squared_exponential_kernel_1d_integrals(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[ 1.       ,  0.8660254,  0.5      ,  0.       , -0.5      ,
        -0.8660254, -1.       ]])
length_scale = 0.648279035509516
variable = <pyapprox.variables.joint.IndependentMarginalsVariable object at 0x13fb97ee0>
transform_quad_rules = False, nquad_samples = 50, skip_xi_1 = False

    def get_gaussian_process_squared_exponential_kernel_1d_integrals(
            X_train, length_scale, variable, transform_quad_rules,
            nquad_samples=50, skip_xi_1=False):
        nvars = variable.num_vars()
        degrees = [nquad_samples]*nvars
>       univariate_quad_rules = get_univariate_quadrature_rules_from_variable(
            variable, degrees)
E       ValueError: not enough values to unpack (expected 2, got 1)

pyapprox/surrogates/gaussianprocess/gaussian_process.py:643: ValueError
___ TestGaussianProcess.test_integrate_gaussian_process_uniform_mixed_bounds ___

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestGaussianProcess testMethod=test_integrate_gaussian_process_uniform_mixed_bounds>

    def test_integrate_gaussian_process_uniform_mixed_bounds(self):
        nvars = 2
        def func(x): return np.sum(x**2, axis=0)[:, np.newaxis]
    
        ntrain_samples = 25
        train_samples = np.cos(
            np.random.uniform(0, np.pi, (nvars, ntrain_samples)))
        train_samples[1, :] = (train_samples[1, :]+1)/2
        train_vals = func(train_samples)
    
        univariate_variables = [stats.uniform(-1, 2), stats.uniform(0, 1)]
        variable = IndependentMarginalsVariable(
            univariate_variables)
        var_trans = AffineTransform(variable)
    
        nu = np.inf
        length_scale = np.ones(nvars)
        kernel = Matern(length_scale, length_scale_bounds=(1e-2, 10), nu=nu)
        gp = GaussianProcess(kernel, n_restarts_optimizer=1)
        gp.set_variable_transformation(var_trans)
        gp.fit(train_samples, train_vals)
    
        expected_random_mean, variance_random_mean, expected_random_var, \
>           variance_random_var = integrate_gaussian_process(gp, variable)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:561: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/gaussianprocess/gaussian_process.py:575: in integrate_gaussian_process
    result = integrate_gaussian_process_squared_exponential_kernel(
pyapprox/surrogates/gaussianprocess/gaussian_process.py:787: in integrate_gaussian_process_squared_exponential_kernel
    get_gaussian_process_squared_exponential_kernel_1d_integrals(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[ 0.25774059, -0.63820914,  0.99999994,  0.58184105,  0.89558709,
         0.95821806,  0.8336272 ,  0.46637227...2017,  0.63634863, -0.78876533,  0.9478759 ,
         0.1629672 , -0.95905333,  0.60389618,  0.61841442,  0.91771897]])
length_scale = array([0.84121492, 1.83410512])
variable = <pyapprox.variables.joint.IndependentMarginalsVariable object at 0x13e9cba60>
transform_quad_rules = False, nquad_samples = 50, skip_xi_1 = False

    def get_gaussian_process_squared_exponential_kernel_1d_integrals(
            X_train, length_scale, variable, transform_quad_rules,
            nquad_samples=50, skip_xi_1=False):
        nvars = variable.num_vars()
        degrees = [nquad_samples]*nvars
        univariate_quad_rules = get_univariate_quadrature_rules_from_variable(
            variable, degrees)
    
        lscale = np.atleast_1d(length_scale)
        # tau, u = 1, 1
        # ntrain_samples = X_train.shape[1]
        # P = np.ones((ntrain_samples, ntrain_samples))
        # lamda = np.ones(ntrain_samples)
        # Pi = np.ones((ntrain_samples, ntrain_samples))
        # xi_1, nu = 1, 1
    
        tau_list, P_list, u_list, lamda_list = [], [], [], []
        Pi_list, nu_list, xi_1_list = [], [], []
        for ii in range(nvars):
            # TODO only compute quadrature once for each unique quadrature rules
            # But all quantities must be computed for all dimensions because
            # distances depend on either of both dimension dependent length scale
            # and training sample values
            # But others like u only needed to be computed for each unique
            # Quadrature rule and raised to the power equal to the number of
            # instances of a unique rule
    
            # Define distance function
            # dist_func = partial(cdist, metric='sqeuclidean')
    
            # Training samples of ith variable
            xtr = X_train[ii:ii+1, :]
    
            # Get 1D quadrature rule
>           xx_1d, ww_1d = univariate_quad_rules[ii](degrees[ii]+1)
E           TypeError: 'functools.partial' object is not subscriptable

pyapprox/surrogates/gaussianprocess/gaussian_process.py:672: TypeError
________ TestGaussianProcess.test_marginalize_gaussian_process_uniform _________

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestGaussianProcess testMethod=test_marginalize_gaussian_process_uniform>

    def test_marginalize_gaussian_process_uniform(self):
        nvars = 2
        a = np.array([1, 0.25])
        # a = np.array([1, 1])
    
        def func(x):
            return np.sum(a[:, None]*(2*x-1)**2, axis=0)[:, np.newaxis]
    
        ntrain_samples = 30
        # train_samples = np.random.uniform(0, 1, (nvars, ntrain_samples))
        train_samples = sobol_sequence(nvars, ntrain_samples)
        train_vals = func(train_samples)
    
        univariate_variables = [stats.uniform(0, 1)]*nvars
        variable = IndependentMarginalsVariable(
            univariate_variables)
        var_trans = AffineTransform(variable)
    
        nu = np.inf
        kernel_var = 2.
        length_scale = np.array([1]*nvars)
        kernel = Matern(length_scale, length_scale_bounds=(1e-2, 10), nu=nu)
        kernel = ConstantKernel(
            constant_value=kernel_var, constant_value_bounds='fixed')*kernel
        gp = GaussianProcess(kernel, n_restarts_optimizer=1, alpha=1e-8,
                             normalize_y=True)
        gp.set_variable_transformation(var_trans)
        gp.fit(train_samples, train_vals)
    
        validation_samples = np.random.uniform(0, 1, (nvars, 100))
        validation_vals = func(validation_samples)
        error = np.linalg.norm(validation_vals-gp(validation_samples)) / \
            np.linalg.norm(validation_vals)
        print(error)
        # only satisfied with more training data. But more training data
        # makes it hard to test computation of marginal gp mean and variances
        # assert error < 1e-3
    
        # true_mean = 1/3*a.sum()
        expected_random_mean, variance_random_mean, expected_random_var, \
            variance_random_var, intermediate_quantities = \
>           integrate_gaussian_process(gp, variable, return_full=True)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:636: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/gaussianprocess/gaussian_process.py:575: in integrate_gaussian_process
    result = integrate_gaussian_process_squared_exponential_kernel(
pyapprox/surrogates/gaussianprocess/gaussian_process.py:787: in integrate_gaussian_process_squared_exponential_kernel
    get_gaussian_process_squared_exponential_kernel_1d_integrals(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[-1.    ,  0.    ,  0.5   , -0.5   , -0.25  ,  0.75  ,  0.25  ,
        -0.75  , -0.625 ,  0.375 ,  0.875 , -0....5,  0.4375, -0.8125,
         0.1875, -0.3125,  0.6875, -0.6875,  0.3125, -0.1875,  0.8125,
        -0.4375,  0.5625]])
length_scale = array([0.56243456, 1.668816  ])
variable = <pyapprox.variables.joint.IndependentMarginalsVariable object at 0x148f10f10>
transform_quad_rules = False, nquad_samples = 50, skip_xi_1 = False

    def get_gaussian_process_squared_exponential_kernel_1d_integrals(
            X_train, length_scale, variable, transform_quad_rules,
            nquad_samples=50, skip_xi_1=False):
        nvars = variable.num_vars()
        degrees = [nquad_samples]*nvars
        univariate_quad_rules = get_univariate_quadrature_rules_from_variable(
            variable, degrees)
    
        lscale = np.atleast_1d(length_scale)
        # tau, u = 1, 1
        # ntrain_samples = X_train.shape[1]
        # P = np.ones((ntrain_samples, ntrain_samples))
        # lamda = np.ones(ntrain_samples)
        # Pi = np.ones((ntrain_samples, ntrain_samples))
        # xi_1, nu = 1, 1
    
        tau_list, P_list, u_list, lamda_list = [], [], [], []
        Pi_list, nu_list, xi_1_list = [], [], []
        for ii in range(nvars):
            # TODO only compute quadrature once for each unique quadrature rules
            # But all quantities must be computed for all dimensions because
            # distances depend on either of both dimension dependent length scale
            # and training sample values
            # But others like u only needed to be computed for each unique
            # Quadrature rule and raised to the power equal to the number of
            # instances of a unique rule
    
            # Define distance function
            # dist_func = partial(cdist, metric='sqeuclidean')
    
            # Training samples of ith variable
            xtr = X_train[ii:ii+1, :]
    
            # Get 1D quadrature rule
>           xx_1d, ww_1d = univariate_quad_rules[ii](degrees[ii]+1)
E           TypeError: 'functools.partial' object is not subscriptable

pyapprox/surrogates/gaussianprocess/gaussian_process.py:672: TypeError
________ TestSamplers.test_RBF_posterior_variance_gradient_wrt_samples _________

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestSamplers testMethod=test_RBF_posterior_variance_gradient_wrt_samples>

    def test_RBF_posterior_variance_gradient_wrt_samples(self):
        nvars = 2
        lb, ub = 0, 1
        ntrain_samples_1d = 10
    
        train_samples = cartesian_product(
            [np.linspace(lb, ub, ntrain_samples_1d)]*nvars)
    
        length_scale = [0.1, 0.2][:nvars]
        kernel = RBF(length_scale, length_scale_bounds='fixed')
    
        pred_samples = np.random.uniform(0, 1, (nvars, 3))
        x0 = train_samples[:, :1]
        grad = RBF_gradient_wrt_samples(
            x0, pred_samples, length_scale)
    
        fd_grad = approx_jacobian(
            lambda x: kernel(x, pred_samples.T)[0, :], x0[:, 0])
        assert np.allclose(grad, fd_grad, atol=1e-6)
        errors = check_gradients(
            lambda x: kernel(x.T, pred_samples.T)[0, :],
            lambda x: RBF_gradient_wrt_samples(
                x, pred_samples, length_scale), x0)
>       assert errors.min() < 1e-6
E       AssertionError: assert 0.5183473048730979 < 1e-06
E        +  where 0.5183473048730979 = <built-in method min of numpy.ndarray object at 0x13dd79990>()
E        +    where <built-in method min of numpy.ndarray object at 0x13dd79990> = array([5.18347305e-01, 3.81651038e+00, 4.71630710e+01, 4.80625005e+02,\n       4.81524364e+03, 4.81614299e+04, 4.816232...192e+06,\n       4.81624282e+07, 4.81624291e+08, 4.81624292e+09, 4.81624292e+10,\n       4.81624292e+11, 4.81624292e+12]).min

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:1216: AssertionError
___________ TestSamplers.test_greedy_gauss_quadrature_ivar_sampler_I ___________

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestSamplers testMethod=test_greedy_gauss_quadrature_ivar_sampler_I>

    def test_greedy_gauss_quadrature_ivar_sampler_I(self):
        nvars = 2
        variables = IndependentMarginalsVariable(
            [stats.beta(20, 20)]*nvars)
        generate_random_samples = partial(
            generate_independent_random_samples, variables)
    
        kernel = Matern(.1, length_scale_bounds='fixed', nu=np.inf)
        np.random.seed(1)
        sampler1 = GreedyIntegratedVarianceSampler(
            nvars, 100, 10, generate_random_samples,
            variables, use_gauss_quadrature=True, econ=True)
>       sampler1.set_kernel(kernel)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:1763: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/gaussianprocess/gaussian_process.py:2013: in set_kernel
    self.precompute_gauss_quadrature()
pyapprox/surrogates/gaussianprocess/gaussian_process.py:2171: in precompute_gauss_quadrature
    xx_1d, ww_1d = self.get_univariate_quadrature_rule(ii)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pyapprox.surrogates.gaussianprocess.gaussian_process.GreedyIntegratedVarianceSampler object at 0x13ddab640>
ii = 0

    def get_univariate_quadrature_rule(self, ii):
>       xx_1d, ww_1d = self.univariate_quad_rules[ii](self.degrees[ii]+1)
E       TypeError: 'functools.partial' object is not subscriptable

pyapprox/surrogates/gaussianprocess/gaussian_process.py:1818: TypeError
______________ TestSamplers.test_greedy_variance_of_mean_sampler _______________

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestSamplers testMethod=test_greedy_variance_of_mean_sampler>

    def test_greedy_variance_of_mean_sampler(self):
        nvars = 2
        variables = IndependentMarginalsVariable(
            [stats.beta(20, 20)]*nvars)
        generate_random_samples = partial(
            generate_independent_random_samples, variables)
    
        sampler = GreedyVarianceOfMeanSampler(
            nvars, 1000, 10, generate_random_samples,
            variables, use_gauss_quadrature=True, econ=True)
        kernel = Matern(.4, length_scale_bounds='fixed', nu=np.inf)
>       sampler.set_kernel(kernel)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:1885: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/gaussianprocess/gaussian_process.py:2013: in set_kernel
    self.precompute_gauss_quadrature()
pyapprox/surrogates/gaussianprocess/gaussian_process.py:1839: in precompute_gauss_quadrature
    xx_1d, ww_1d = self.get_univariate_quadrature_rule(ii)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pyapprox.surrogates.gaussianprocess.gaussian_process.GreedyVarianceOfMeanSampler object at 0x13d9e6c10>
ii = 0

    def get_univariate_quadrature_rule(self, ii):
>       xx_1d, ww_1d = self.univariate_quad_rules[ii](self.degrees[ii]+1)
E       TypeError: 'functools.partial' object is not subscriptable

pyapprox/surrogates/gaussianprocess/gaussian_process.py:1818: TypeError
__________ TestSamplers.test_monte_carlo_gradient_based_ivar_sampler ___________

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestSamplers testMethod=test_monte_carlo_gradient_based_ivar_sampler>

    def test_monte_carlo_gradient_based_ivar_sampler(self):
        nvars = 2
        variables = IndependentMarginalsVariable(
            [stats.beta(20, 20)]*nvars)
        generate_random_samples = partial(
            generate_independent_random_samples, variables)
    
        # correlation length affects ability to check gradient.
        # As kernel matrix gets more ill conditioned then gradients get worse
        greedy_method = 'ivar'
        # greedy_method = 'chol'
        use_gauss_quadrature = False
        kernel = Matern(.1, length_scale_bounds='fixed', nu=np.inf)
        sampler = IVARSampler(
            nvars, 1000, 1000, generate_random_samples, variables,
            greedy_method, use_gauss_quadrature=use_gauss_quadrature,
            nugget=1e-14)
        sampler.set_kernel(copy.deepcopy(kernel))
    
        def weight_function(samples):
            return np.prod([variables.marginals()[ii].pdf(samples[ii, :])
                            for ii in range(samples.shape[0])], axis=0)
    
        if greedy_method == 'chol':
            sampler.set_weight_function(weight_function)
    
        # nature of training samples affects ability to check gradient. As
        # training samples makes kernel matrix more ill conditioned then
        # gradients get worse
        ntrain_samples_1d = 10
        train_samples = cartesian_product(
            [np.linspace(0, 1, ntrain_samples_1d)]*nvars)
        x0 = train_samples.flatten(order='F')
        if not use_gauss_quadrature:
            # gradients not currently implemented when using quadrature
>           errors = check_gradients(
                sampler.objective, sampler.objective_gradient,
                x0[:, np.newaxis], disp=False)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:1592: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fun = <bound method IVARSampler.monte_carlo_objective of <pyapprox.surrogates.gaussianprocess.gaussian_process.IVARSampler object at 0x13e109100>>
jac = <bound method IVARSampler.monte_carlo_objective_gradient of <pyapprox.surrogates.gaussianprocess.gaussian_process.IVARSampler object at 0x13e109100>>
zz = array([[0.        ],
       [0.        ],
       [0.11111111],
       [0.        ],
       [0.22222222],
       [0.   ...[0.77777778],
       [1.        ],
       [0.88888889],
       [1.        ],
       [1.        ],
       [1.        ]])
plot = False, disp = False, rel = True
direction = array([[ 0.02258396],
       [-0.0026509 ],
       [-0.13044233],
       [-0.11404641],
       [-0.04986718],
       [...645008],
       [-0.05374342],
       [-0.04847228],
       [ 0.04329824],
       [-0.04552003],
       [ 0.02735317]])
jacp = None

    def check_gradients(fun, jac, zz, plot=False, disp=True, rel=True,
                        direction=None, jacp=None):
        """
        Compare a user specified jacobian with the jacobian computed with finite
        difference with multiple step sizes.
    
        Parameters
        ----------
        fun : callable
    
            A function with signature
    
            ``fun(z) -> np.ndarray``
    
            where ``z`` is a 2D np.ndarray with shape (nvars, 1) and the
            output is a 2D np.ndarray with shape (nqoi, 1)
    
        jac : callable
            The jacobian of ``fun`` with signature
    
            ``jac(z) -> np.ndarray``
    
            where ``z`` is a 2D np.ndarray with shape (nvars, 1) and the
            output is a 2D np.ndarray with shape (nqoi, nvars)
    
        zz : np.ndarray (nvars, 1)
            A sample of ``z`` at which to compute the gradient
    
        plot : boolean
            Plot the errors as a function of the finite difference step size
    
        disp : boolean
            True - print the errors
            False - do not print
    
        rel : boolean
            True - compute the relative error in the directional derivative,
            i.e. the absolute error divided by the directional derivative using
            ``jac``.
            False - compute the absolute error in the directional derivative
    
        direction : np.ndarray (nvars, 1)
            Direction to which Jacobian is applied. Default is None in which
            case random direction is chosen.
    
        Returns
        -------
        errors : np.ndarray (14, nqoi)
            The errors in the directional derivative of ``fun`` at 14 different
            values of finite difference tolerance for each quantity of interest
        """
        assert zz.ndim == 2
        assert zz.shape[1] == 1
    
        if direction is None:
            direction = np.random.normal(0, 1, (zz.shape[0], 1))
            direction /= np.linalg.norm(direction)
        assert direction.ndim == 2 and direction.shape[1] == 1
    
        if ((jacp is None and jac is None) or
                (jac is not None and jacp is not None)):
            raise Exception('Must specify jac or jacp')
    
        if callable(jac):
            function_val = fun(zz)
            grad_val = jac(zz)  # .squeeze()
            directional_derivative = grad_val.dot(direction).squeeze()
        elif callable(jacp):
            directional_derivative = jacp(zz, direction)
        elif jac is True:
            function_val, grad_val = fun(zz)
            directional_derivative = grad_val.dot(direction).squeeze()
        else:
            raise Exception
    
        fd_eps = np.logspace(-13, 0, 14)[::-1]
        errors = []
        row_format = "{:<12} {:<25} {:<25} {:<25}"
        if disp:
            if rel:
                print(
                    row_format.format(
                        "Eps", "norm(jv)", "norm(jv_fd)",
                        "Rel. Errors"))
            else:
                print(row_format.format(
                    "Eps", "norm(jv)", "norm(jv_fd)",
                    "Abs. Errors"))
        for ii in range(fd_eps.shape[0]):
            zz_perturbed = zz.copy()+fd_eps[ii]*direction
            perturbed_function_val = fun(zz_perturbed)
            if jac:
>               perturbed_function_val = perturbed_function_val[0].squeeze()
E               IndexError: invalid index to scalar variable.

pyapprox/util/utilities.py:1008: IndexError
___________ TestSamplers.test_quadrature_gradient_based_ivar_sampler ___________

self = <pyapprox.surrogates.gaussianprocess.tests.test_gaussian_process.TestSamplers testMethod=test_quadrature_gradient_based_ivar_sampler>

    def test_quadrature_gradient_based_ivar_sampler(self):
        nvars = 2
        variables = IndependentMarginalsVariable(
            [stats.beta(20, 20)]*nvars)
        generate_random_samples = partial(
            generate_independent_random_samples, variables)
    
        # correlation length affects ability to check gradient.
        # As kerenl matrix gets more ill conditioned then gradients get worse
        greedy_method = 'ivar'
        # greedy_method = 'chol'
        use_gauss_quadrature = True
        kernel = Matern(.1, length_scale_bounds='fixed', nu=np.inf)
>       sampler = IVARSampler(
            nvars, 1000, 1000, generate_random_samples, variables,
            greedy_method, use_gauss_quadrature=use_gauss_quadrature,
            nugget=1e-8)

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py:1667: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/gaussianprocess/gaussian_process.py:1508: in __init__
    self.precompute_gauss_quadrature()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pyapprox.surrogates.gaussianprocess.gaussian_process.IVARSampler object at 0x148f9a4c0>

    def precompute_gauss_quadrature(self):
        degrees = [min(100, self.nquad_samples)]*self.nvars
        self.univariate_quad_rules, self.pce = \
            get_univariate_quadrature_rules_from_variable(
                self.greedy_sampler.variables, degrees)
        self.quad_rules = []
        for ii in range(self.nvars):
>           xx_1d, ww_1d = self.univariate_quad_rules[ii](degrees[ii]+1)
E           TypeError: 'functools.partial' object is not subscriptable

pyapprox/surrogates/gaussianprocess/gaussian_process.py:1538: TypeError
___________________ TestNeuralNetwork.test_nn_loss_gradient ____________________

self = <pyapprox.surrogates.tests.test_neural_network.TestNeuralNetwork testMethod=test_nn_loss_gradient>

    def test_nn_loss_gradient(self):
>       self.check_nn_loss_gradients('sigmoid')

pyapprox/surrogates/tests/test_neural_network.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyapprox/surrogates/tests/test_neural_network.py:64: in check_nn_loss_gradients
    errors = check_gradients(fun, jac, zz, plot=False, disp=disp, rel=True,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fun = <function TestNeuralNetwork.check_nn_loss_gradients.<locals>.fun at 0x13e07c4c0>
jac = functools.partial(<bound method NeuralNetwork.objective_jacobian of MLP(3,3,2)>, array([[4.1702200470257400e-01, 7.203....8092444932388259],
       [0.5089351333266514, 0.2636029501067915],
       [1.204745548622378 , 1.0328152017241092]]))
zz = array([[-1.546724068394848 ],
       [ 1.0452006271485816],
       [ 1.0103754805795542],
       [ 0.0708366439445842]...      [ 0.5819251837754401],
       [-0.4102075176817786],
       [ 2.2968661024329244],
       [ 1.688497054196278 ]])
plot = False, disp = True, rel = True
direction = array([[ 0.1701593718141148],
       [-0.4381329951484476],
       [ 0.0163406933293638],
       [ 0.1257331237810846]...      [-0.1768328117884189],
       [-0.2118407159457112],
       [-0.2114480225586478],
       [ 0.169276051013187 ]])
jacp = None

    def check_gradients(fun, jac, zz, plot=False, disp=True, rel=True,
                        direction=None, jacp=None):
        """
        Compare a user specified jacobian with the jacobian computed with finite
        difference with multiple step sizes.
    
        Parameters
        ----------
        fun : callable
    
            A function with signature
    
            ``fun(z) -> np.ndarray``
    
            where ``z`` is a 2D np.ndarray with shape (nvars, 1) and the
            output is a 2D np.ndarray with shape (nqoi, 1)
    
        jac : callable
            The jacobian of ``fun`` with signature
    
            ``jac(z) -> np.ndarray``
    
            where ``z`` is a 2D np.ndarray with shape (nvars, 1) and the
            output is a 2D np.ndarray with shape (nqoi, nvars)
    
        zz : np.ndarray (nvars, 1)
            A sample of ``z`` at which to compute the gradient
    
        plot : boolean
            Plot the errors as a function of the finite difference step size
    
        disp : boolean
            True - print the errors
            False - do not print
    
        rel : boolean
            True - compute the relative error in the directional derivative,
            i.e. the absolute error divided by the directional derivative using
            ``jac``.
            False - compute the absolute error in the directional derivative
    
        direction : np.ndarray (nvars, 1)
            Direction to which Jacobian is applied. Default is None in which
            case random direction is chosen.
    
        Returns
        -------
        errors : np.ndarray (14, nqoi)
            The errors in the directional derivative of ``fun`` at 14 different
            values of finite difference tolerance for each quantity of interest
        """
        assert zz.ndim == 2
        assert zz.shape[1] == 1
    
        if direction is None:
            direction = np.random.normal(0, 1, (zz.shape[0], 1))
            direction /= np.linalg.norm(direction)
        assert direction.ndim == 2 and direction.shape[1] == 1
    
        if ((jacp is None and jac is None) or
                (jac is not None and jacp is not None)):
            raise Exception('Must specify jac or jacp')
    
        if callable(jac):
            function_val = fun(zz)
            grad_val = jac(zz)  # .squeeze()
            directional_derivative = grad_val.dot(direction).squeeze()
        elif callable(jacp):
            directional_derivative = jacp(zz, direction)
        elif jac is True:
            function_val, grad_val = fun(zz)
            directional_derivative = grad_val.dot(direction).squeeze()
        else:
            raise Exception
    
        fd_eps = np.logspace(-13, 0, 14)[::-1]
        errors = []
        row_format = "{:<12} {:<25} {:<25} {:<25}"
        if disp:
            if rel:
                print(
                    row_format.format(
                        "Eps", "norm(jv)", "norm(jv_fd)",
                        "Rel. Errors"))
            else:
                print(row_format.format(
                    "Eps", "norm(jv)", "norm(jv_fd)",
                    "Abs. Errors"))
        for ii in range(fd_eps.shape[0]):
            zz_perturbed = zz.copy()+fd_eps[ii]*direction
            perturbed_function_val = fun(zz_perturbed)
            if jac:
>               perturbed_function_val = perturbed_function_val[0].squeeze()
E               IndexError: invalid index to scalar variable.

pyapprox/util/utilities.py:1008: IndexError
=============================== warnings summary ===============================
pyapprox/analysis/tests/test_active_subspace.py::TestActiveSubspace::test_get_chebyhsev_center_of_inactive_subspace
  /Users/jdjakem/software/pyapprox/pyapprox/analysis/active_subspace.py:48: OptimizeWarning: Solving system with option 'cholesky':True failed. It is normal for this to happen occasionally, especially as the solution is approached. However, if you see this frequently, consider setting option 'cholesky' to False.
    res = scipy_linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds,

pyapprox/analysis/tests/test_active_subspace.py::TestActiveSubspace::test_get_chebyhsev_center_of_inactive_subspace
  /Users/jdjakem/software/pyapprox/pyapprox/analysis/active_subspace.py:48: OptimizeWarning: Solving system with option 'sym_pos':True failed. It is normal for this to happen occasionally, especially as the solution is approached. However, if you see this frequently, consider setting option 'sym_pos' to False.
    res = scipy_linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds,

pyapprox/analysis/tests/test_active_subspace.py::TestActiveSubspace::test_get_chebyhsev_center_of_inactive_subspace
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/optimize/_linprog_ip.py:117: LinAlgWarning: Ill-conditioned matrix (rcond=5.76581e-18): result may not be accurate.
    return sp.linalg.solve(M, r, sym_pos=sym_pos)

pyapprox/analysis/tests/test_active_subspace.py::TestActiveSubspace::test_get_chebyhsev_center_of_inactive_subspace
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/optimize/_linprog_ip.py:117: LinAlgWarning: Ill-conditioned matrix (rcond=1.34303e-18): result may not be accurate.
    return sp.linalg.solve(M, r, sym_pos=sym_pos)

pyapprox/analysis/tests/test_sensitivity_analysis.py: 6 warnings
pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py: 1 warning
pyapprox/surrogates/tests/test_approximate.py: 17 warnings
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
    return array(a, dtype, copy=False, order=order)

pyapprox/analysis/tests/test_sensitivity_analysis.py: 8 warnings
pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py: 4 warnings
pyapprox/surrogates/tests/test_approximate.py: 35 warnings
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
  If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:
  
  from sklearn.pipeline import make_pipeline
  
  model = make_pipeline(StandardScaler(with_mean=False), LassoLarsCV())
  
  If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:
  
  kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
  model.fit(X, y, **kwargs)
  
  Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
    warnings.warn(

pyapprox/benchmarks/tests/test_benchmarks.py::TestBenchmarks::test_piston_gradient
  /Users/jdjakem/software/pyapprox/pyapprox/benchmarks/surrogate_benchmarks.py:172: RuntimeWarning: invalid value encountered in sqrt
    V = S/(2.*k)*(np.sqrt(A**2+4.*k*Z) - A)

pyapprox/benchmarks/tests/test_benchmarks.py::TestBenchmarks::test_piston_gradient
  /Users/jdjakem/software/pyapprox/pyapprox/benchmarks/surrogate_benchmarks.py:173: RuntimeWarning: invalid value encountered in sqrt
    C = 2.*np.pi*np.sqrt(M/(k+S**2*Z/V**2))

pyapprox/benchmarks/tests/test_benchmarks.py::TestBenchmarks::test_wing_weight_gradient
  /Users/jdjakem/software/pyapprox/pyapprox/benchmarks/surrogate_benchmarks.py:218: RuntimeWarning: invalid value encountered in power
    (np.cos(Lamda)**-.9)*(q**.006)*(lamda**.04)*(100**-.3) *

pyapprox/expdesign/tests/test_linear_oed.py::TestNonLinearOptimalExeprimentalDesign::test_exponential_growth_model_bayesian_d_optimal_design
  /Users/jdjakem/software/pyapprox/pyapprox/expdesign/linear_oed.py:1577: OptimizeWarning: Unknown solver options: tol
    res = minimize(

pyapprox/multifidelity/tests/test_mfnets.py::TestMFNets::test_least_squares_optimization
  /Users/jdjakem/software/pyapprox/pyapprox/multifidelity/mfnets.py:358: OptimizeWarning: Unknown solver options: iprint, ftol
    res = minimize(

pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_basis_pursuit
  /Users/jdjakem/software/pyapprox/pyapprox/optimization/l1_minimization.py:44: OptimizeWarning: Sparse constraint matrix detected; setting 'sparse':True.
    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq,

pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_basis_pursuit_denoising_smooth_l1_norm
pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_basis_pursuit_smooth_l1_norm
  /Users/jdjakem/software/pyapprox/pyapprox/optimization/l1_minimization.py:277: OptimizeWarning: Unknown solver options: verbose
    res = minimize(

pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_nonlinear_basis_pursuit
pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_nonlinear_basis_pursuit_denoising_with_linear_model
pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_nonlinear_basis_pursuit_with_linear_model
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/optimize/_minimize.py:533: RuntimeWarning: Method slsqp does not use Hessian-vector product information (hessp).
    warn('Method %s does not use Hessian-vector product '

pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_nonlinear_basis_pursuit
  /Users/jdjakem/software/pyapprox/pyapprox/optimization/l1_minimization.py:124: OptimizeWarning: Unknown solver options: verbose, xtol
    res = minimize(

pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_nonlinear_basis_pursuit_with_linear_model
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/optimize/_constraints.py:356: OptimizeWarning: Constraint options `finite_diff_jac_sparsity`, `finite_diff_rel_step`, `keep_feasible`, and `hess`are ignored by this method.
    warn("Constraint options `finite_diff_jac_sparsity`, "

pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_nonlinear_basis_pursuit_with_linear_model
  /Users/jdjakem/software/pyapprox/pyapprox/optimization/l1_minimization.py:124: OptimizeWarning: Unknown solver options: gtol, verbose, xtol
    res = minimize(

pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestGaussianProcess::test_generate_gp_realizations
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 9 of parameter k2__length_scale is close to the specified upper bound 10.0. Increasing the bound and calling fit again may find a better value.
    warnings.warn(

pyapprox/surrogates/gaussianprocess/tests/test_gradient_enhanced_gp.py::TestGradientEnhancedGP::test_gradient_of_gp_examples
pyapprox/surrogates/tests/test_approximate.py::TestApproximate::test_adaptive_approximate_gaussian_process
pyapprox/surrogates/tests/test_approximate.py::TestApproximate::test_adaptive_approximate_gaussian_process_normalize_inputs
pyapprox/surrogates/tests/test_approximate.py::TestApproximate::test_adaptive_approximate_gaussian_process_normalize_inputs
pyapprox/surrogates/tests/test_approximate.py::TestApproximate::test_approximate_gaussian_process
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py:610: ConvergenceWarning: lbfgs failed to converge (status=2):
  ABNORMAL_TERMINATION_IN_LNSRCH.
  
  Increase the number of iterations (max_iter) or scale the data as shown in:
      https://scikit-learn.org/stable/modules/preprocessing.html
    _check_optimize_result("lbfgs", opt_res)

pyapprox/surrogates/gaussianprocess/tests/test_gradient_enhanced_gp.py::TestGradientEnhancedGP::test_gradient_of_gp_examples
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-08. Decreasing the bound and calling fit again may find a better value.
    warnings.warn(

pyapprox/surrogates/gaussianprocess/tests/test_gradient_enhanced_gp.py::TestGradientEnhancedGP::test_gradient_of_gp_examples
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.
    warnings.warn(

pyapprox/surrogates/interp/tests/test_mixture_model.py: 8 warnings
pyapprox/surrogates/interp/tests/test_sparse_grid.py: 16 warnings
pyapprox/surrogates/orthopoly/tests/test_quadrature.py: 2 warnings
pyapprox/surrogates/polychaos/tests/test_arbitrary_polynomial_chaos.py: 2 warnings
pyapprox/surrogates/polychaos/tests/test_sparse_grid_to_pce.py: 2 warnings
pyapprox/variables/tests/test_joint.py: 1 warning
pyapprox/variables/tests/test_nataf_transformation.py: 1 warning
pyapprox/variables/tests/test_risk_measures.py: 1 warning
pyapprox/variables/tests/test_rosenblatt_transformation.py: 1 warning
pyapprox/variables/tests/test_variable_transformations.py: 2 warnings
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:624: RuntimeWarning: overflow encountered in _beta_ppf
    return _boost._beta_ppf(q, a, b)

pyapprox/surrogates/interp/tests/test_mixture_model.py: 4 warnings
pyapprox/surrogates/orthopoly/tests/test_leja_sequences.py: 8 warnings
pyapprox/surrogates/orthopoly/tests/test_quadrature.py: 4 warnings
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/optimize/optimize.py:282: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds
    warnings.warn("Values in x were outside bounds during a "

pyapprox/surrogates/orthopoly/tests/test_quadrature.py::TestQuadrature::test_sampled_based_christoffel_leja_quadrature_rule
  /Users/jdjakem/software/pyapprox/pyapprox/surrogates/orthopoly/leja_sequences.py:843: UserWarning: artificial bounds [-10.8101910848134, 11.930609422520126] reached. Variable should be scaled.
   Nsamples: 8 Initial guess: [4.19979305], bounds: Bounds(array([3.99980291]), array([11.93060942]))
    warn(msg, UserWarning)

pyapprox/surrogates/orthopoly/tests/test_recursion_factory.py::TestRecursionFactory::test_get_recursion_coefficients_from_variable_continuous
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:960: RuntimeWarning: divide by zero encountered in reciprocal
    return (q**(-1.0/d) - 1)**(-1.0/c)

pyapprox/surrogates/orthopoly/tests/test_recursion_factory.py::TestRecursionFactory::test_get_recursion_coefficients_from_variable_continuous
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:1775: RuntimeWarning: overflow encountered in exp
    f = 1 + np.log(b) + sc.xlogy(b - 1.0, x) + xb - np.exp(xb)

pyapprox/surrogates/orthopoly/tests/test_recursion_factory.py::TestRecursionFactory::test_get_recursion_coefficients_from_variable_continuous
pyapprox/surrogates/polychaos/tests/test_gpc.py::TestGPC::test_pce_for_gumbel_variable
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:3252: RuntimeWarning: overflow encountered in exp
    return -x - np.exp(-x)

pyapprox/surrogates/orthopoly/tests/test_recursion_factory.py::TestRecursionFactory::test_get_recursion_coefficients_from_variable_continuous
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:3346: RuntimeWarning: overflow encountered in exp
    return x - np.exp(x)

pyapprox/surrogates/orthopoly/tests/test_recursion_factory.py::TestRecursionFactory::test_get_recursion_coefficients_from_variable_continuous
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:3555: RuntimeWarning: overflow encountered in cosh
    return 1.0/(np.pi*np.cosh(x))

pyapprox/surrogates/orthopoly/tests/test_recursion_factory.py::TestRecursionFactory::test_get_recursion_coefficients_from_variable_continuous
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:5247: RuntimeWarning: overflow encountered in exp
    return np.exp(c*x-np.exp(x)-sc.gammaln(c))

pyapprox/surrogates/orthopoly/tests/test_recursion_factory.py::TestRecursionFactory::test_get_recursion_coefficients_from_variable_continuous
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:6005: RuntimeWarning: overflow encountered in exp
    return np.exp(-0.5 * (x + np.exp(-x))) / np.sqrt(2*np.pi)

pyapprox/surrogates/polychaos/tests/test_induced_sampling.py::TestInducedSampling::test_continous_induced_measure_ppf
  /Users/jdjakem/software/pyapprox/pyapprox/surrogates/polychaos/induced_sampling.py:791: IntegrationWarning: The occurrence of roundoff error is detected, which prevents 
    the requested tolerance from being achieved.  The error may be 
    underestimated.
    integral, err = integrate.quad(

pyapprox/surrogates/polychaos/tests/test_induced_sampling.py::TestInducedSampling::test_multivariate_sampling_jacobi
pyapprox/surrogates/polychaos/tests/test_induced_sampling.py::TestInducedSampling::test_multivariate_sampling_jacobi
pyapprox/surrogates/polychaos/tests/test_induced_sampling.py::TestInducedSampling::test_multivariate_sampling_jacobi
  /Users/jdjakem/software/pyapprox/pyapprox/surrogates/polychaos/induced_sampling.py:507: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead
    n = np.asscalar(n)

pyapprox/surrogates/polychaos/tests/test_sparse_grid_to_pce.py::TestMultivariatePolynomials::test_convert_sparse_grid_to_pce_mixed_basis
  /Users/jdjakem/software/pyapprox/pyapprox/surrogates/orthopoly/leja_sequences.py:843: UserWarning: artificial bounds [-11.251319153177224, 11.251319153177226] reached. Variable should be scaled.
   Nsamples: 6 Initial guess: [3.423079825354603], bounds: Bounds(array([3.2600760241472404]), array([11.251319153177226]))
    warn(msg, UserWarning)

pyapprox/surrogates/tests/test_approximate.py::TestApproximate::test_adaptive_approximate_gaussian_process
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.
    warnings.warn(

pyapprox/surrogates/tests/test_approximate.py: 20 warnings
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
  If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:
  
  from sklearn.pipeline import make_pipeline
  
  model = make_pipeline(StandardScaler(with_mean=False), LassoLars())
  
  If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:
  
  kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
  model.fit(X, y, **kwargs)
  
  Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
    warnings.warn(

pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_estimate_coupling_bounds
pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_estimate_coupling_bounds
pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_fixed_coupling_bounds
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
    return array(a, dtype, copy=False, order=order, subok=True)

pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_estimate_coupling_bounds
pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_estimate_coupling_bounds
pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_estimate_coupling_bounds
pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_estimate_coupling_bounds
pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_estimate_coupling_bounds
pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_estimate_coupling_bounds
pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_fixed_coupling_bounds
pyapprox/surrogates/tests/test_system_analysis.py::TestSystemAnalysis::test_feed_forward_system_of_polynomials_fixed_coupling_bounds
  /Users/jdjakem/software/pyapprox/pyapprox/variables/transforms.py:151: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.
    self.scale_parameters[ii, :] = transform_scale_parameters(var)

pyapprox/util/tests/test_utilities.py::TestUtilities::test_cartesian_product
  /Users/jdjakem/software/pyapprox/pyapprox/util/tests/test_utilities.py:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    [1, 4], [2, 4]], np.int)

pyapprox/variables/tests/test_random_variable_algebra.py::TestRandomVariableAlgebra::test_variable_transformation_standard_normal_squared
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/numpy/lib/polynomial.py:771: RuntimeWarning: invalid value encountered in multiply
    y = y * x + p[i]

pyapprox/variables/tests/test_risk_measures.py::TestRiskMeasures::test_equivalent_formulations_of_cvar
  /Users/jdjakem/software/pyapprox/pyapprox/variables/tests/test_risk_measures.py:43: IntegrationWarning: The occurrence of roundoff error is detected, which prevents 
    the requested tolerance from being achieved.  The error may be 
    underestimated.
    integral, err = integrate.quad(

pyapprox/variables/tests/test_variables.py::TestMarginals::test_get_pdf
  /Users/jdjakem/software/pyapprox/pyapprox/variables/tests/test_variables.py:169: UserWarning: variable kstwo is not tested
    warn(f"variable {name} is not tested", UserWarning)

pyapprox/variables/tests/test_variables.py::TestMarginals::test_get_pdf
  /Users/jdjakem/software/pyapprox/pyapprox/variables/tests/test_variables.py:169: UserWarning: variable genhyperbolic is not tested
    warn(f"variable {name} is not tested", UserWarning)

pyapprox/variables/tests/test_variables.py::TestMarginals::test_get_pdf
  /Users/jdjakem/software/pyapprox/pyapprox/variables/tests/test_variables.py:169: UserWarning: variable geninvgauss is not tested
    warn(f"variable {name} is not tested", UserWarning)

pyapprox/variables/tests/test_variables.py::TestMarginals::test_get_pdf
  /Users/jdjakem/software/pyapprox/pyapprox/variables/tests/test_variables.py:169: UserWarning: variable laplace_asymmetric is not tested
    warn(f"variable {name} is not tested", UserWarning)

pyapprox/variables/tests/test_variables.py::TestMarginals::test_get_pdf
  /Users/jdjakem/software/pyapprox/pyapprox/variables/tests/test_variables.py:169: UserWarning: variable loguniform is not tested
    warn(f"variable {name} is not tested", UserWarning)

pyapprox/variables/tests/test_variables.py::TestMarginals::test_get_pdf
  /Users/jdjakem/software/pyapprox/pyapprox/variables/tests/test_variables.py:169: UserWarning: variable skewcauchy is not tested
    warn(f"variable {name} is not tested", UserWarning)

pyapprox/variables/tests/test_variables.py::TestMarginals::test_get_pdf
  /Users/jdjakem/software/pyapprox/pyapprox/variables/tests/test_variables.py:169: UserWarning: variable trapezoid is not tested
    warn(f"variable {name} is not tested", UserWarning)

pyapprox/variables/tests/test_variables.py::TestMarginals::test_get_pdf
  /Users/jdjakem/software/pyapprox/pyapprox/variables/tests/test_variables.py:169: UserWarning: variable studentized_range is not tested
    warn(f"variable {name} is not tested", UserWarning)

pyapprox/variables/tests/test_variables.py::TestMarginals::test_get_pdf
  /Users/jdjakem/miniconda3/envs/pyapprox-base/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:2246: RuntimeWarning: divide by zero encountered in reciprocal
    vals = -np.log(pow(q, -1.0/c)-1)

-- Docs: https://docs.pytest.org/en/stable/warnings.html

---------- coverage: platform darwin, python 3.8.11-final-0 ----------
Name                                                                         Stmts   Miss  Cover
------------------------------------------------------------------------------------------------
pyapprox/__init__.py                                                             4      0   100%
pyapprox/analysis/__init__.py                                                    3      0   100%
pyapprox/analysis/active_subspace.py                                           318     77    76%
pyapprox/analysis/convergence_studies.py                                        88     88     0%
pyapprox/analysis/parameter_sweeps.py                                          161     92    43%
pyapprox/analysis/sensitivity_analysis.py                                      374    106    72%
pyapprox/analysis/tests/__init__.py                                              0      0   100%
pyapprox/analysis/tests/test_active_subspace.py                                188     10    95%
pyapprox/analysis/tests/test_parameter_sweeps.py                                33      2    94%
pyapprox/analysis/tests/test_sensitivity_analysis.py                           299     11    96%
pyapprox/analysis/visualize.py                                                 130    130     0%
pyapprox/bayes/__init__.py                                                       3      0   100%
pyapprox/bayes/gaussian_network.py                                             418    232    44%
pyapprox/bayes/laplace.py                                                      239     35    85%
pyapprox/bayes/tests/__init__.py                                                 0      0   100%
pyapprox/bayes/tests/test_gaussian_network.py                                  268      2    99%
pyapprox/bayes/tests/test_laplace.py                                           469     48    90%
pyapprox/benchmarks/__init__.py                                                  2      0   100%
pyapprox/benchmarks/benchmarks.py                                              126     54    57%
pyapprox/benchmarks/genz.py                                                    228    160    30%
pyapprox/benchmarks/multifidelity_benchmarks.py                                143     31    78%
pyapprox/benchmarks/sensitivity_benchmarks.py                                  140     32    77%
pyapprox/benchmarks/surrogate_benchmarks.py                                    420    201    52%
pyapprox/benchmarks/tests/__init__.py                                            0      0   100%
pyapprox/benchmarks/tests/test_benchmarks.py                                    82      2    98%
pyapprox/cython/__init__.py                                                      0      0   100%
pyapprox/expdesign/__init__.py                                                   4      0   100%
pyapprox/expdesign/bayesian_oed.py                                             593    112    81%
pyapprox/expdesign/linear_oed.py                                               641     98    85%
pyapprox/expdesign/low_discrepancy_sequences.py                                121     29    76%
pyapprox/expdesign/tests/__init__.py                                             0      0   100%
pyapprox/expdesign/tests/test_bayesian_oed.py                                  841     18    98%
pyapprox/expdesign/tests/test_linear_oed.py                                    648     12    98%
pyapprox/expdesign/tests/test_low_discrepancy_sequences.py                      34      2    94%
pyapprox/interface/__init__.py                                                   4      0   100%
pyapprox/interface/async_model.py                                              148     25    83%
pyapprox/interface/file_io_model.py                                             42      9    79%
pyapprox/interface/tests/__init__.py                                             0      0   100%
pyapprox/interface/tests/test_async_model.py                                   130     11    92%
pyapprox/interface/tests/test_datafunction_model.py                             65      2    97%
pyapprox/interface/tests/test_wrappers.py                                       87      2    98%
pyapprox/interface/wrappers.py                                                 394     92    77%
pyapprox/multifidelity/__init__.py                                               3      0   100%
pyapprox/multifidelity/control_variate_monte_carlo.py                          809    228    72%
pyapprox/multifidelity/low_rank_multifidelity.py                                93     13    86%
pyapprox/multifidelity/mfnets.py                                               216     12    94%
pyapprox/multifidelity/monte_carlo_estimators.py                               377    165    56%
pyapprox/multifidelity/multifidelity.py                                          7      7     0%
pyapprox/multifidelity/tests/__init__.py                                         0      0   100%
pyapprox/multifidelity/tests/test_control_variate_monte_carlo.py               478     29    94%
pyapprox/multifidelity/tests/test_low_rank_multi_fidelilty.py                  130     12    91%
pyapprox/multifidelity/tests/test_mfnets.py                                    228      2    99%
pyapprox/optimization/__init__.py                                                5      0   100%
pyapprox/optimization/_rol_minimize.py                                         242    242     0%
pyapprox/optimization/cvar_regression.py                                       217    140    35%
pyapprox/optimization/first_order_stochastic_dominance.py                      292    129    56%
pyapprox/optimization/l1_minimization.py                                       374    106    72%
pyapprox/optimization/optimization.py                                          328    238    27%
pyapprox/optimization/pya_minimize.py                                           46     14    70%
pyapprox/optimization/quantile_regression.py                                    68     48    29%
pyapprox/optimization/second_order_stochastic_dominance.py                      54     11    80%
pyapprox/optimization/tests/__init__.py                                          0      0   100%
pyapprox/optimization/tests/test_cvar_regression.py                             54      7    87%
pyapprox/optimization/tests/test_l1_minimization.py                            279     94    66%
pyapprox/optimization/tests/test_optimization.py                                56      2    96%
pyapprox/optimization/tests/test_quantile_regression.py                         30     17    43%
pyapprox/optimization/tests/test_stochastic_dominance.py                        82     16    80%
pyapprox/pde/__init__.py                                                         0      0   100%
pyapprox/pde/karhunen_loeve_expansion.py                                       193    132    32%
pyapprox/pde/spectral_diffusion.py                                             514    121    76%
pyapprox/pde/tests/__init__.py                                                   0      0   100%
pyapprox/pde/tests/test_karhunen_loeve.py                                       41      3    93%
pyapprox/pde/tests/test_spectral_diffusion.py                                  394     56    86%
pyapprox/surrogates/__init__.py                                                 10      0   100%
pyapprox/surrogates/approximate.py                                             667     89    87%
pyapprox/surrogates/coupled_systems.py                                         347    106    69%
pyapprox/surrogates/function_train.py                                          463    130    72%
pyapprox/surrogates/gaussianprocess/__init__.py                                  2      0   100%
pyapprox/surrogates/gaussianprocess/gaussian_process.py                       1226    532    57%
pyapprox/surrogates/gaussianprocess/gradient_enhanced_gp.py                    245     30    88%
pyapprox/surrogates/gaussianprocess/multilevel_gp.py                           151     49    68%
pyapprox/surrogates/gaussianprocess/tests/__init__.py                            0      0   100%
pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py            1009    374    63%
pyapprox/surrogates/gaussianprocess/tests/test_gradient_enhanced_gp.py         158     13    92%
pyapprox/surrogates/gaussianprocess/tests/test_multilevel_gp.py                116     58    50%
pyapprox/surrogates/interp/__init__.py                                           0      0   100%
pyapprox/surrogates/interp/adaptive_sparse_grid.py                             831    217    74%
pyapprox/surrogates/interp/barycentric_interpolation.py                        255    148    42%
pyapprox/surrogates/interp/indexing.py                                         306    144    53%
pyapprox/surrogates/interp/manipulate_polynomials.py                           184     70    62%
pyapprox/surrogates/interp/mixture_model.py                                     61      5    92%
pyapprox/surrogates/interp/monomial.py                                          46      0   100%
pyapprox/surrogates/interp/sparse_grid.py                                      415    114    73%
pyapprox/surrogates/interp/tensorprod.py                                       135     21    84%
pyapprox/surrogates/interp/tests/__init__.py                                     0      0   100%
pyapprox/surrogates/interp/tests/test_barycentric_interpolation.py             259     40    85%
pyapprox/surrogates/interp/tests/test_mixture_model.py                          58      2    97%
pyapprox/surrogates/interp/tests/test_sparse_grid.py                           866     16    98%
pyapprox/surrogates/interp/tests/test_tensorprod.py                             73      2    97%
pyapprox/surrogates/neural_networks.py                                         242     37    85%
pyapprox/surrogates/orthopoly/__init__.py                                        0      0   100%
pyapprox/surrogates/orthopoly/leja_quadrature.py                               122      6    95%
pyapprox/surrogates/orthopoly/leja_sequences.py                                370     23    94%
pyapprox/surrogates/orthopoly/numeric_orthonormal_recursions.py                301     36    88%
pyapprox/surrogates/orthopoly/orthonormal_polynomials.py                       109     40    63%
pyapprox/surrogates/orthopoly/orthonormal_recursions.py                        107     11    90%
pyapprox/surrogates/orthopoly/quadrature.py                                    110     31    72%
pyapprox/surrogates/orthopoly/recursion_factory.py                             108     27    75%
pyapprox/surrogates/orthopoly/tests/__init__.py                                  0      0   100%
pyapprox/surrogates/orthopoly/tests/test_leja_sequences.py                     148      7    95%
pyapprox/surrogates/orthopoly/tests/test_numeric_orthonormal_recursions.py     186      3    98%
pyapprox/surrogates/orthopoly/tests/test_orthonormal_polynomials.py            237      2    99%
pyapprox/surrogates/orthopoly/tests/test_quadrature.py                         189      2    99%
pyapprox/surrogates/orthopoly/tests/test_recursion_factory.py                   97      3    97%
pyapprox/surrogates/polychaos/__init__.py                                        0      0   100%
pyapprox/surrogates/polychaos/adaptive_polynomial_chaos.py                     268     44    84%
pyapprox/surrogates/polychaos/arbitrary_polynomial_chaos.py                    201     16    92%
pyapprox/surrogates/polychaos/equilibrium_sampling.py                           30     30     0%
pyapprox/surrogates/polychaos/gpc.py                                           458    113    75%
pyapprox/surrogates/polychaos/induced_sampling.py                              426     85    80%
pyapprox/surrogates/polychaos/leja_sequences.py                                169     18    89%
pyapprox/surrogates/polychaos/orthogonal_least_interpolation.py                389     90    77%
pyapprox/surrogates/polychaos/polynomial_sampling.py                            65     13    80%
pyapprox/surrogates/polychaos/sparse_grid_to_gpc.py                             73      5    93%
pyapprox/surrogates/polychaos/tests/__init__.py                                  0      0   100%
pyapprox/surrogates/polychaos/tests/test_adaptive_polynomial_chaos.py           91      2    98%
pyapprox/surrogates/polychaos/tests/test_arbitrary_polynomial_chaos.py         238      7    97%
pyapprox/surrogates/polychaos/tests/test_gpc.py                                559      3    99%
pyapprox/surrogates/polychaos/tests/test_indexing.py                           110      2    98%
pyapprox/surrogates/polychaos/tests/test_induced_sampling.py                   212     10    95%
pyapprox/surrogates/polychaos/tests/test_leja_sequences.py                      70      2    97%
pyapprox/surrogates/polychaos/tests/test_manipulate_polynomials.py             234      2    99%
pyapprox/surrogates/polychaos/tests/test_monomial.py                            55      2    96%
pyapprox/surrogates/polychaos/tests/test_orthogonal_least_interpolation.py     335     22    93%
pyapprox/surrogates/polychaos/tests/test_polynomial_sampling.py                197      2    99%
pyapprox/surrogates/polychaos/tests/test_sparse_grid_to_pce.py                 197      2    99%
pyapprox/surrogates/system_analysis.py                                         275     56    80%
pyapprox/surrogates/tests/__init__.py                                            0      0   100%
pyapprox/surrogates/tests/test_approximate.py                                  302      2    99%
pyapprox/surrogates/tests/test_coupled_systems.py                              260      2    99%
pyapprox/surrogates/tests/test_function_train.py                               296      3    99%
pyapprox/surrogates/tests/test_neural_network.py                               116      7    94%
pyapprox/surrogates/tests/test_system_analysis.py                              124      2    98%
pyapprox/util/__init__.py                                                        3      0   100%
pyapprox/util/configure_plots.py                                                20      4    80%
pyapprox/util/convert_to_latex_table.py                                         77     77     0%
pyapprox/util/linalg.py                                                        274     32    88%
pyapprox/util/print_profile_stats.py                                            29     29     0%
pyapprox/util/pya_numba.py                                                      15      5    67%
pyapprox/util/randomized_svd.py                                                201     60    70%
pyapprox/util/sympy_utilities.py                                                34     34     0%
pyapprox/util/sys_utilities.py                                                  35      9    74%
pyapprox/util/tests/__init__.py                                                  0      0   100%
pyapprox/util/tests/test_linalg.py                                             406     43    89%
pyapprox/util/tests/test_utilities.py                                          105      2    98%
pyapprox/util/utilities.py                                                     438     94    79%
pyapprox/util/visualization.py                                                 267    244     9%
pyapprox/variables/__init__.py                                                   4      0   100%
pyapprox/variables/algebra.py                                                  173      8    95%
pyapprox/variables/density.py                                                  376    128    66%
pyapprox/variables/gaussian.py                                                 275     13    95%
pyapprox/variables/joint.py                                                    159     37    77%
pyapprox/variables/marginals.py                                                176     15    91%
pyapprox/variables/nataf.py                                                    158      9    94%
pyapprox/variables/risk.py                                                     206     50    76%
pyapprox/variables/rosenblatt.py                                               147      6    96%
pyapprox/variables/sampling.py                                                  62     33    47%
pyapprox/variables/tests/__init__.py                                             0      0   100%
pyapprox/variables/tests/test_density.py                                       162      1    99%
pyapprox/variables/tests/test_joint.py                                          45      2    96%
pyapprox/variables/tests/test_multivariate_gaussian.py                         231      2    99%
pyapprox/variables/tests/test_nataf_transformation.py                           93      2    98%
pyapprox/variables/tests/test_probability_measure_sampling.py                   52      2    96%
pyapprox/variables/tests/test_random_variable_algebra.py                       209      3    99%
pyapprox/variables/tests/test_risk_measures.py                                 529    175    67%
pyapprox/variables/tests/test_rosenblatt_transformation.py                     206     17    92%
pyapprox/variables/tests/test_variable_transformations.py                      194      2    99%
pyapprox/variables/tests/test_variables.py                                     149      2    99%
pyapprox/variables/transforms.py                                               259     58    78%
------------------------------------------------------------------------------------------------
TOTAL                                                                        35226   7697    78%

=========================== short test summary info ============================
FAILED pyapprox/analysis/tests/test_sensitivity_analysis.py::TestSensitivityAnalysis::test_analytic_sobol_indices_from_gaussian_process
FAILED pyapprox/benchmarks/tests/test_benchmarks.py::TestBenchmarks::test_cantilever_beam_gradients
FAILED pyapprox/expdesign/tests/test_bayesian_oed.py::TestBayesianOED::test_numerical_gaussian_prediction_deviation_based_oed
FAILED pyapprox/expdesign/tests/test_linear_oed.py::TestOptimalExperimentalDesign::test_homoscedastic_least_squares_goptimal_design
FAILED pyapprox/expdesign/tests/test_linear_oed.py::TestOptimalExperimentalDesign::test_r_oed_objective_and_constraint_wrappers
FAILED pyapprox/multifidelity/tests/test_control_variate_monte_carlo.py::TestCVMC::test_variance_reduction
FAILED pyapprox/multifidelity/tests/test_control_variate_monte_carlo.py::TestCVMC::test_variance_reduction_acvgmf
FAILED pyapprox/optimization/tests/test_cvar_regression.py::TestCVARRegression::test_smooth_conditional_value_at_risk_composition_gradient
FAILED pyapprox/optimization/tests/test_l1_minimization.py::TestL1Minimization::test_smooth_l1_norm_gradients
FAILED pyapprox/optimization/tests/test_stochastic_dominance.py::TestFirstOrderStochasticDominance::test_objective_derivatives
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestGaussianProcess::test_compute_sobol_indices_gaussian_process_uniform_2d
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestGaussianProcess::test_compute_sobol_indices_gaussian_process_uniform_3d
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestGaussianProcess::test_integrate_gaussian_process_gaussian
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestGaussianProcess::test_integrate_gaussian_process_uniform
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestGaussianProcess::test_integrate_gaussian_process_uniform_mixed_bounds
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestGaussianProcess::test_marginalize_gaussian_process_uniform
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestSamplers::test_RBF_posterior_variance_gradient_wrt_samples
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestSamplers::test_greedy_gauss_quadrature_ivar_sampler_I
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestSamplers::test_greedy_variance_of_mean_sampler
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestSamplers::test_monte_carlo_gradient_based_ivar_sampler
FAILED pyapprox/surrogates/gaussianprocess/tests/test_gaussian_process.py::TestSamplers::test_quadrature_gradient_based_ivar_sampler
FAILED pyapprox/surrogates/tests/test_neural_network.py::TestNeuralNetwork::test_nn_loss_gradient
===== 22 failed, 529 passed, 8 skipped, 207 warnings in 545.72s (0:09:05) ======
