
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_paper_demo.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_paper_demo.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_paper_demo.py:


End-to-End Model Analysis
=========================
This tutorial describes how to use each of the major model analyses in Pyapprox
following the exposition in [PYAPPROX2022]_.

First lets load all the necessary modules and set the random seeds for reproducibility.

.. GENERATED FROM PYTHON SOURCE LINES 9-34

.. code-block:: default

    from scipy import stats
    import numpy as np
    import matplotlib.pyplot as plt
    from functools import partial
    import torch
    import time
    import warnings
    from pyapprox.util.configure_plots import mathrm_label, mathrm_labels
    from pyapprox.variables import (
        IndependentMarginalsVariable, print_statistics, AffineTransform)
    from pyapprox.benchmarks import setup_benchmark, list_benchmarks
    from pyapprox.interface.wrappers import (
        ModelEnsemble, TimerModel, WorkTrackingModel,
        evaluate_1darray_function_on_2d_array)
    from pyapprox.surrogates import adaptive_approximate
    from pyapprox.analysis.sensitivity_analysis import (
        run_sensitivity_analysis, plot_sensitivity_indices)
    from pyapprox.bayes.markov_chain_monte_carlo import (
        loglike_from_negloglike, MCMCVariable)
    from pyapprox.expdesign.bayesian_oed import get_bayesian_oed_optimizer
    from pyapprox import multifidelity
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    np.random.seed(2)
    torch.manual_seed(2)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <torch._C.Generator object at 0x140e81d90>



.. GENERATED FROM PYTHON SOURCE LINES 35-37

The tutorial can save the figures to file if desired. If you do want the plots
set savefig=True

.. GENERATED FROM PYTHON SOURCE LINES 37-39

.. code-block:: default

    savefig = False








.. GENERATED FROM PYTHON SOURCE LINES 40-41

The following code shows how to create and sample from two independent uniform random variables defined on [âˆ’2, 2]. We use uniform variables here, but any marginal from the scipy.stats module can be used.

.. GENERATED FROM PYTHON SOURCE LINES 41-47

.. code-block:: default

    nsamples = 30
    univariate_variables = [stats.uniform(-2, 4), stats.uniform(-2, 4)]
    variable = IndependentMarginalsVariable(univariate_variables)
    samples = variable.rvs(nsamples)
    print_statistics(samples)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

               z0         z1    
    count   30.000000  30.000000
    mean    -0.486291   0.116079
    std      0.926995   1.107798
    min     -1.896295  -1.891191
    max      1.415901   1.975408




.. GENERATED FROM PYTHON SOURCE LINES 48-51

PyApprox supports various types of variable transformations. The following code
shows how to use an affinte transformation to map samples from variables
to samples from the variable's canonical form.

.. GENERATED FROM PYTHON SOURCE LINES 51-55

.. code-block:: default

    var_trans = AffineTransform(variable)
    canonical_samples = var_trans.map_to_canonical(samples)
    print_statistics(canonical_samples)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

               z0         z1    
    count   30.000000  30.000000
    mean    -0.243145   0.058040
    std      0.463497   0.553899
    min     -0.948148  -0.945595
    max      0.707951   0.987704




.. GENERATED FROM PYTHON SOURCE LINES 56-62

Pyapprox provides many utilities for interfacing with complex numerical codes
and show how to use the ModelEnsemble and
WorkTrackingModel to evaluate two models at once and time the wall time
of each evaluation of each model. The last print statement
prints the median execution time of each model. First lest defined
two functions with different execution times that can be evaluated for multiple samples

.. GENERATED FROM PYTHON SOURCE LINES 62-80

.. code-block:: default

    def fun_pause_1(sample):
        assert sample.ndim == 1
        time.sleep(np.random.uniform(0, .05))
        return np.sum(sample**2)


    def pyapprox_fun_1(samples):
        return evaluate_1darray_function_on_2d_array(fun_pause_1, samples)


    def fun_pause_2(sample):
        time.sleep(np.random.uniform(.05, .1))
        return np.sum(sample**2)


    def pyapprox_fun_2(samples):
        return evaluate_1darray_function_on_2d_array(fun_pause_2, samples)








.. GENERATED FROM PYTHON SOURCE LINES 81-83

Now wrap these functions and run them as an ensemble while tracking
their execution times

.. GENERATED FROM PYTHON SOURCE LINES 83-94

.. code-block:: default

    model_ensemble = ModelEnsemble([pyapprox_fun_1, pyapprox_fun_2])
    timer_fun_ensemble = TimerModel(model_ensemble)
    worktracking_fun_ensemble = WorkTrackingModel(
        timer_fun_ensemble, num_config_vars=1)
    fun_ids = np.ones(nsamples)
    fun_ids[:nsamples//2] = 0
    ensemble_samples = np.vstack([samples, fun_ids])
    values = worktracking_fun_ensemble(ensemble_samples)
    query_fun_ids = np.atleast_2d([0, 1])
    print(worktracking_fun_ensemble.work_tracker(query_fun_ids))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [0.02750206 0.07620621]




.. GENERATED FROM PYTHON SOURCE LINES 95-102

Pyapprox provide numerous benchmarks for verifying, validating and comparing
model analysis algorithms. The following list the names of all benchmarks and
then creates a benchmark that can be used to test the creation of surrogates,
Bayesian inference, and optimal experimental design. This benchmark requires
determining the true coefficients of the Karhunene Loeve expansion (KLE)
used to characterize the uncertain diffusivity field of an advection
diffusion equation. See documentation of the benchmark for more details).

.. GENERATED FROM PYTHON SOURCE LINES 102-109

.. code-block:: default

    print(list_benchmarks())
    noise_stdev = 1e-1
    inv_benchmark = setup_benchmark(
        "advection_diffusion_kle_inversion", kle_nvars=3,
        noise_stdev=noise_stdev, nobs=10, kle_length_scale=0.5)
    print(inv_benchmark.keys())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    ['sobol_g', 'ishigami', 'oakley', 'rosenbrock', 'genz', 'cantilever_beam', 'wing_weight', 'piston', 'chemical_reaction', 'random_oscillator', 'coupled_springs', 'hastings_ecology', 'multi_index_advection_diffusion', 'advection_diffusion_kle_inversion', 'polynomial_ensemble', 'tunable_model_ensemble', 'short_column_ensemble', 'parameterized_nonlinear_model']
    dict_keys(['negloglike', 'variable', 'noiseless_obs', 'obs', 'true_sample', 'obs_indices', 'obs_fun', 'KLE', 'mesh'])




.. GENERATED FROM PYTHON SOURCE LINES 110-111

The following plots the modes of the KLE

.. GENERATED FROM PYTHON SOURCE LINES 111-117

.. code-block:: default

    fig, axs = plt.subplots(
        1, inv_benchmark.KLE.nterms, figsize=(8*inv_benchmark.KLE.nterms, 6))
    for ii in range(inv_benchmark.KLE.nterms):
        inv_benchmark.mesh.plot(inv_benchmark.KLE.eig_vecs[:, ii:ii+1], 50,
                                ax=axs[ii])




.. image-sg:: /auto_examples/images/sphx_glr_plot_paper_demo_001.png
   :alt: plot paper demo
   :srcset: /auto_examples/images/sphx_glr_plot_paper_demo_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 118-126

PyApprox provides many popular methods for constructing surrogates
that once constructed can be evaluated in place of a computaionally
expensive simulation model in model analyses. The following code creates
a Gaussian process (GP) surrogate. The function used to construct the surrogate
takes a callback which is evaluated each time the adaptive surrogate is refined.
Here we use to compute the error of the surrogate as it is constructed using
validation data. Uncomment the code to use a polynomial based surrogate instead
of a GP. The user does not have to change any subsequent code

.. GENERATED FROM PYTHON SOURCE LINES 126-148

.. code-block:: default

    validation_samples = inv_benchmark.variable.rvs(100)
    validation_values = inv_benchmark.negloglike(validation_samples)
    nsamples, errors = [], []
    def callback(approx):
        nsamples.append(approx.num_training_samples())
        error = np.linalg.norm(
            approx(validation_samples)-validation_values, axis=0)
        error /= np.linalg.norm(validation_values, axis=0)
        errors.append(error)

    approx_result = adaptive_approximate(
        inv_benchmark.negloglike, inv_benchmark.variable, "gaussian_process",
        {"max_nsamples": 30, "ncandidate_samples": 2e3, "verbose": 0,
         "callback": callback, "kernel_variance": 400})

    # approx_result = adaptive_approximate(
    #     inv_benchmark.negloglike, inv_benchmark.variable, "polynomial_chaos",
    #     {"method": "leja", "options": {
    #         "max_nsamples": 100, "ncandidate_samples": 3e3, "verbose": 0,
    #         "callback": callback}})
    approx = approx_result.approx








.. GENERATED FROM PYTHON SOURCE LINES 149-150

We can plot the errors obtained from the callback with

.. GENERATED FROM PYTHON SOURCE LINES 150-157

.. code-block:: default

    ax = plt.subplots(figsize=(8, 6))[1]
    ax.loglog(nsamples, errors, "o-")
    ax.set_xlabel(mathrm_label("No. Samples"))
    ax.set_ylabel(mathrm_label("Error"))
    if savefig:
        plt.savefig("gp-error-plot.pdf")




.. image-sg:: /auto_examples/images/sphx_glr_plot_paper_demo_002.png
   :alt: plot paper demo
   :srcset: /auto_examples/images/sphx_glr_plot_paper_demo_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 158-167

Now we will perform a sensitivity analysis. Specifically we compute
variance based sensitivity indices that measure the impact of each KLE mode
on the mismatch between the observed data and the model predictions.
We use the negative log likelihood to characterize this mismatch.
Here we have used the surrogate to speed up the computation of the sensitivity
indices. Uncomment the commented code to use the numerical model. Note
the drastic increase in computational cost. Warning: using the numerical model
will take many minutes. The plots in the figure, generated from
left to right are: main effect, largest Sobol indices and total effect indices.

.. GENERATED FROM PYTHON SOURCE LINES 167-176

.. code-block:: default

    sa_result = run_sensitivity_analysis(
        "surrogate_sobol", approx, inv_benchmark.variable)
    # sa_result = run_sensitivity_analysis(
    #     "sobol", benchmark.negloglike, inv_benchmark.variable)
    axs = plot_sensitivity_indices(
        sa_result)[1]
    if savefig:
        plt.savefig("gp-sa-indices.pdf", bbox_inches="tight")




.. image-sg:: /auto_examples/images/sphx_glr_plot_paper_demo_003.png
   :alt: plot paper demo
   :srcset: /auto_examples/images/sphx_glr_plot_paper_demo_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 177-186

Now we will use the surrogate with Bayesian inference to learn the
coefficients of the KL. Specifically we will draw a set of samples from
the posterior distribution of the KLE given the observed data provided
in the benchmark.

But First we will improve the accuracy of the surrogate
and print out the error which can be compared to the errors previously plotted.
The error of the original surrogate was kept low to demonstrate the ability
to quantify error in the sensitivity indices from using a surrogate.

.. GENERATED FROM PYTHON SOURCE LINES 186-192

.. code-block:: default

    approx.refine(100)
    error = np.linalg.norm(
        approx(validation_samples)-validation_values, axis=0)
    error /= np.linalg.norm(validation_values, axis=0)
    print("Surrogate", error)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Surrogate [0.03358284]




.. GENERATED FROM PYTHON SOURCE LINES 193-202

Now create a MCMCVariable to sample from the posterior. The benchmark
has already formulated the negative log likelihood that is needed. Here
we will use the NUTS sampler from PyMC3. This can run many MCMC chains at once
by setting njobs > 1. However using njobs>1 cannot be used unless the srcipt
is invoked inside if __name__ == __main__: and so njobs>1
is not used for this tutorial.
Uncomment the commented code to use the numerical model instead of the surrogate
with the MCMC algorithm. Again note the significant increase in computational
time

.. GENERATED FROM PYTHON SOURCE LINES 202-211

.. code-block:: default

    algorithm, npost_samples, njobs = "nuts", 100, 1
    # loglike = partial(loglike_from_negloglike, inv_benchmark.negloglike)
    loglike = partial(loglike_from_negloglike, approx)
    mcmc_variable = MCMCVariable(
        inv_benchmark.variable, loglike, algorithm, njobs=njobs, loglike_grad=True)
    print(mcmc_variable)
    post_samples = mcmc_variable.rvs(npost_samples)
    map_sample = mcmc_variable.maximum_aposteriori_point()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    MCMCVariable with prior:
    Independent Marginal Variable
    Number of variables: 3
    Unique variables and global id:
        norm(loc=0,scale=1): z0, z1, z2




.. GENERATED FROM PYTHON SOURCE LINES 212-216

Now plot the posterior samples with the 2D Marginals of the posterior. Note
do not do this with the numerical model as this would take an eternity due
to the cost of evaluating the numerical model, which is much higher relative
to the cost of running the surrogate.

.. GENERATED FROM PYTHON SOURCE LINES 216-223

.. code-block:: default

    mcmc_variable.plot_2d_marginals(
        nsamples_1d=30,
        plot_samples=[
            [post_samples, {}], [map_sample, {"c": "k", "marker": "X", "s": 100}]])
    if savefig:
        plt.savefig("posterior-samples.pdf", bbox_inches="tight")




.. image-sg:: /auto_examples/images/sphx_glr_plot_paper_demo_004.png
   :alt: plot paper demo
   :srcset: /auto_examples/images/sphx_glr_plot_paper_demo_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 224-230

In the Bayesian inference above we used a fixed number of observations
at randomly chosen spatial locations. However choosing observation locations
is usually a poor idea. Not all observations can reduce the uncertainty
in the parameters equally. Here we use Bayesian optimal experimental design
to choose the 3 best design locations from the previously observed 10 pretending
that we do not know the value of the observations.

.. GENERATED FROM PYTHON SOURCE LINES 230-251

.. code-block:: default

    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
    #plot a single solution to the PDE before overlaying the designs
    inv_benchmark.mesh.plot(
        inv_benchmark.obs_fun._fwd_solver.solve()[:, None], 50, ax=ax)

    design_candidates = inv_benchmark.mesh.mesh_pts[:, inv_benchmark.obs_indices]
    oed = get_bayesian_oed_optimizer(
        "kl_params", design_candidates, inv_benchmark.obs_fun, noise_stdev,
        inv_benchmark.variable)
    oed_results = []
    ndesign = 3
    for step in range(ndesign):
        results_step = oed.update_design()[1]
        oed_results.append(results_step)
    selected_candidates = design_candidates[:, np.hstack(oed_results)]
    ax.plot(design_candidates[0, :], design_candidates[1, :], "rs")
    ax.plot(selected_candidates[0, :], selected_candidates[1, :], "ko")
    if savefig:
        plt.savefig("oed-selected-design.pdf")





.. image-sg:: /auto_examples/images/sphx_glr_plot_paper_demo_005.png
   :alt: plot paper demo
   :srcset: /auto_examples/images/sphx_glr_plot_paper_demo_005.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Running 1000 model evaluations
    Running 8000 model evaluations




.. GENERATED FROM PYTHON SOURCE LINES 252-262

Note that typically optimal
experimental design (OED) would be used before conducting Bayesian inference.
However, because understanding of Bayesian inference is needed to understand
Bayesian OED we reversed the order. OED is much more expensive than a single
Bayesian calibration because it requires solving many calibration problems.
So typically we do not solve the calibration problems in the OED procedure
to the same degree of accuracy as a final calibration. The accuracy of the
calibrations used by OED must only be sufficient to distinguish between designs.
This accuracy is typically much lower than the accuracy required in
estimates of uncertainty in the parameters or predictions needed for decision making tasks such as risk assessment.

.. GENERATED FROM PYTHON SOURCE LINES 264-275

Here we will set up a related benchmark to the one we have been using,
which can be used to demonstrate the forward propagation of uncertainty.
This benchmark uses the steady state solution of the advection diffusion,
obtained with a constant addition of a tracer into the domain at a single
source model as initial condition. A pump at another locations is then activated
to extract the tracer from the domain. The benchmark quantity of interest
measures the change of the tracer concentration in a subomain.
The benchmark provides models of varying cost
and accuracy that use different discretizations of the spatial PDE mesh
and number of time steps which can be used with multi-fideilty methods.
To setup the benchmark use the following

.. GENERATED FROM PYTHON SOURCE LINES 275-282

.. code-block:: default

    fwd_benchmark = setup_benchmark(
        "multi_index_advection_diffusion",
        kle_nvars=inv_benchmark.variable.num_vars(), kle_length_scale=0.5,
        time_scenario=True)
    model = WorkTrackingModel(
        TimerModel(fwd_benchmark.model_ensemble), num_config_vars=1)








.. GENERATED FROM PYTHON SOURCE LINES 283-288

Here we will use Multi-fidelity statistical estimation to compute the
mean value of the QoI to account for the uncertainty in the KLE cofficients.
So first we must compute the covariance between the QoI returned by
each of our models. We use samples from the posterior. But uncommenting
the code below will use samples from the prior.

.. GENERATED FROM PYTHON SOURCE LINES 288-295

.. code-block:: default

    npilot_samples = 10
    # generate_samples = inv_benchmark.variable.rvs # for sampling from prior
    generate_samples = post_samples 
    cov = multifidelity.estimate_model_ensemble_covariance(
        npilot_samples, generate_samples, model,
        fwd_benchmark.model_ensemble.nmodels)[0]








.. GENERATED FROM PYTHON SOURCE LINES 296-301

By using a WorkTrackingModel we can extract the median costs
of evaluatin each model which is needed to predict the
error of the multi-fidelity estimate of the mean which we can
compare to a prediction of the single fidelity estimate that only uses
the highest fidelity model.

.. GENERATED FROM PYTHON SOURCE LINES 301-306

.. code-block:: default

    model_costs = model.work_tracker(
        np.asarray([np.arange(fwd_benchmark.model_ensemble.nmodels)]))
    # make costs in terms of fraction of cost of high-fidelity evaluation
    model_costs /= model_costs[0]








.. GENERATED FROM PYTHON SOURCE LINES 307-309

Now visualize the correlation between the models and their computational
cost relative to the highest-fidelity model cost

.. GENERATED FROM PYTHON SOURCE LINES 309-316

.. code-block:: default

    fig, axs = plt.subplots(1, 2, figsize=(2*8, 6))
    multifidelity.plot_correlation_matrix(
        multifidelity.get_correlation_from_covariance(cov), ax=axs[0])
    multifidelity.plot_model_costs(model_costs, ax=axs[1])
    axs[0].set_title(mathrm_label("Model covariances"))
    axs[1].set_title(mathrm_label("Relative model costs"))




.. image-sg:: /auto_examples/images/sphx_glr_plot_paper_demo_006.png
   :alt: $\mathrm{Model\;covariances}$, $\mathrm{Relative\;model\;costs}$
   :srcset: /auto_examples/images/sphx_glr_plot_paper_demo_006.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Text(0.5, 1.0, '$\\mathrm{Relative\\;model\\;costs}$')



.. GENERATED FROM PYTHON SOURCE LINES 317-321

Now find the best multi-fidelity estimator among all avialable option
Note, he exact predicted variance will change from run to run even with the
same seed because the computational time measured will change slightly
for each run

.. GENERATED FROM PYTHON SOURCE LINES 321-329

.. code-block:: default

    best_est, best_model_indices = (
        multifidelity.get_best_models_for_acv_estimator(
            "acvgmfb", cov, model_costs, inv_benchmark.variable, 1e2, max_nmodels=3,
            tree_depth=4))
    target_cost = 1000
    best_est.allocate_samples(target_cost)
    print("Predicted variance", best_est.optimized_variance)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [86.79587866] nsample_ratios
    [  1 134] nsf1
    [  1 134] nsf2
    [134.] rounded nsample_ratios
    [38.65097744] nsample_ratios
    [  3 132] nsf1
    [  3 132] nsf2
    [44.] rounded nsample_ratios
    [26.87360743] nsample_ratios
    [  5 148] nsf1
    [  5 148] nsf2
    [29.6] rounded nsample_ratios
    [400.02117139] nsample_ratios
    [  1 400] nsf1
    [  1 400] nsf2
    [400.] rounded nsample_ratios
    [178.4364653] nsample_ratios
    [  2 517] nsf1
    [  2 517] nsf2
    [258.5] rounded nsample_ratios
    [78.07372016] nsample_ratios
    [  6 499] nsf1
    [  6 499] nsf2
    [83.16666667] rounded nsample_ratios
    [53.83290167] nsample_ratios
    [ 10 549] nsf1
    [ 10 549] nsf2
    [54.9] rounded nsample_ratios
    [ 1.39009201 50.84824312] nsample_ratios
    [  2   3 130] nsf1
    [  2   3 130] nsf2
    [ 1.5 65. ] rounded nsample_ratios
    [ 1.24410659 35.35949621] nsample_ratios
    [  4   5 144] nsf1
    [  4   5 144] nsf2
    [ 1.25 36.  ] rounded nsample_ratios
    [  2.         394.11976397] nsample_ratios
    [  1   2 394] nsf1
    [  1   2 394] nsf2
    [  2. 394.] rounded nsample_ratios
    [  1.46106696 234.70634192] nsample_ratios
    [  2   3 509] nsf1
    [  2   3 509] nsf2
    [  1.5 254.5] rounded nsample_ratios
    [  1.21116308 102.70488626] nsample_ratios
    [  4   5 486] nsf1
    [  4   5 486] nsf2
    [  1.25 121.5 ] rounded nsample_ratios
    [ 1.13398864 70.82091007] nsample_ratios
    [  7   8 528] nsf1
    [  7   8 528] nsf2
    [ 1.14285714 75.42857143] rounded nsample_ratios
    [ 1.24384568 35.32411583] nsample_ratios
    [  4   5 144] nsf1
    [  4   5 144] nsf2
    [ 1.25 36.  ] rounded nsample_ratios
    [  2.         394.14175859] nsample_ratios
    [  1   2 394] nsf1
    [  1   2 394] nsf2
    [  2. 394.] rounded nsample_ratios
    [  1.46071238 234.53993971] nsample_ratios
    [  2   3 509] nsf1
    [  2   3 509] nsf2
    [  1.5 254.5] rounded nsample_ratios
    [  1.2109642 102.6170491] nsample_ratios
    [  4   5 486] nsf1
    [  4   5 486] nsf2
    [  1.25 121.5 ] rounded nsample_ratios
    [ 1.13385259 70.75713423] nsample_ratios
    [  7   8 528] nsf1
    [  7   8 528] nsf2
    [ 1.14285714 75.42857143] rounded nsample_ratios
    [  2.         394.87842068] nsample_ratios
    [  1   2 394] nsf1
    [  1   2 394] nsf2
    [  2. 394.] rounded nsample_ratios
    [  1.44767714 228.346756  ] nsample_ratios
    [  2   3 510] nsf1
    [  2   3 510] nsf2
    [  1.5 255. ] rounded nsample_ratios
    [ 1.20469207 99.878338  ] nsample_ratios
    [  4   5 487] nsf1
    [  4   5 487] nsf2
    [  1.25 121.75] rounded nsample_ratios
    [ 1.12969767 68.86284924] nsample_ratios
    [  7   8 530] nsf1
    [  7   8 530] nsf2
    [ 1.14285714 75.71428571] rounded nsample_ratios
    [  1.38741704 199.31151917] nsample_ratios
    [  2   3 514] nsf1
    [  2   3 514] nsf2
    [  1.5 257. ] rounded nsample_ratios
    [ 1.17622457 87.215252  ] nsample_ratios
    [  5   6 494] nsf1
    [  5   6 494] nsf2
    [ 1.2 98.8] rounded nsample_ratios
    [ 1.11100821 60.14095862] nsample_ratios
    [  9  10 541] nsf1
    [  9  10 541] nsf2
    [ 1.11111111 60.11111111] rounded nsample_ratios
    [ 1.17153023 85.08895227] nsample_ratios
    [  5   6 496] nsf1
    [  5   6 496] nsf2
    [ 1.2 99.2] rounded nsample_ratios
    [ 1.10793829 58.67247151] nsample_ratios
    [  9  10 543] nsf1
    [  9  10 543] nsf2
    [ 1.11111111 60.33333333] rounded nsample_ratios
    [ 1.10790956 58.65752382] nsample_ratios
    [  9  10 543] nsf1
    [  9  10 543] nsf2
    [ 1.11111111 60.33333333] rounded nsample_ratios
    [178.4364653] nsample_ratios
    [  29 5174] nsf1
    [  29 5174] nsf2
    [178.4137931] rounded nsample_ratios
    Predicted variance 9.589916867071303e-11




.. GENERATED FROM PYTHON SOURCE LINES 330-332

Now we can plot the relative performance of the single and multi-fidelity
estimates of the mean before requiring any additional model evaluations

.. GENERATED FROM PYTHON SOURCE LINES 332-354

.. code-block:: default

    hf_cov, hf_cost = cov[:1, :1], model_costs[:1]
    estimators = [
        multifidelity.get_estimator(
            "mc", hf_cov, hf_cost, inv_benchmark.variable),
        best_est]
    target_costs = np.array([1e1, 1e2, 1e3, 1e4], dtype=int)
    optimized_estimators = multifidelity.compare_estimator_variances(
        target_costs, estimators)
    est_labels = mathrm_labels(["MC", "ACV"])
    fig, axs = plt.subplots(1, 2, figsize=(2*8, 6))
    multifidelity.plot_estimator_variances(
        optimized_estimators, est_labels, axs[0],
        ylabel=mathrm_label("Relative Estimator Variance"))
    axs[0].set_xlim(target_costs.min(), target_costs.max())
    nmodels = cov.shape[0]
    model_labels = [
        r"$f_{%d}$" % ii for ii in np.arange(nmodels)[best_model_indices]]
    multifidelity.plot_acv_sample_allocation_comparison(
        optimized_estimators[1], model_labels, axs[1])
    if savefig:
        plt.savefig("acv-variance-reduction.pdf")




.. image-sg:: /auto_examples/images/sphx_glr_plot_paper_demo_007.png
   :alt: plot paper demo
   :srcset: /auto_examples/images/sphx_glr_plot_paper_demo_007.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [47.96479826] nsample_ratios
    [ 1 47] nsf1
    [ 1 47] nsf2
    [47.] rounded nsample_ratios
    [178.4364653] nsample_ratios
    [  2 517] nsf1
    [  2 517] nsf2
    [258.5] rounded nsample_ratios
    [178.4364653] nsample_ratios
    [  29 5174] nsf1
    [  29 5174] nsf2
    [178.4137931] rounded nsample_ratios
    [178.4364653] nsample_ratios
    [  290 51748] nsf1
    [  290 51748] nsf2
    [178.44137931] rounded nsample_ratios




.. GENERATED FROM PYTHON SOURCE LINES 355-358

It is clear that the multi-fidelity estimator will be more computationally
efficient for multiple computational budgets (target costs). Once the user
is ready to actually estimate the mean QoI they can use

.. GENERATED FROM PYTHON SOURCE LINES 358-368

.. code-block:: default

    target_cost = 10
    best_est.allocate_samples(target_cost)
    best_model_ensemble = ModelEnsemble(
        [fwd_benchmark.model_ensemble.functions[ii] for ii in best_model_indices])
    samples, values = best_est.generate_data(best_model_ensemble)
    mean = best_est(values)
    print("Mean QoI", mean)

    plt.show()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [47.96479826] nsample_ratios
    [ 1 47] nsf1
    [ 1 47] nsf2
    [47.] rounded nsample_ratios
    Mean QoI 0.001136890542170262




.. GENERATED FROM PYTHON SOURCE LINES 369-372

References
^^^^^^^^^^
.. [PYAPPROX2022] `Jakeman J.D., PyApprox: Enabling efficient model analysis. (2022)`_


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  21.713 seconds)


.. _sphx_glr_download_auto_examples_plot_paper_demo.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_paper_demo.py <plot_paper_demo.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_paper_demo.ipynb <plot_paper_demo.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
