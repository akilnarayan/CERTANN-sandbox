{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\rvset{{\\mathcal{Z}}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\text{d}#1}\\newcommand{\\mat}[1]{{\\boldsymbol{\\mathrm{#1}}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Univariate Interpolation\n\nThis tutorial will present methods to approximate univariate functions $\\hat{f}_{\\alpha}:\\reals\\to\\reals$ using interpolation. An interpolant of a function $f$ is a weighted linear combination of basis functions\n\n\\begin{align}f_{\\alpha,\\beta}(\\rv_i)=\\sum_{j=1}^M f_{\\alpha}(\\rv_i^{(j)})\\phi_{i,j}(\\rv_i),\\end{align}\n\nwhere the weights are the evaluation on the function $f$ on a set of points $\\rv_i^{(j)}, j=1\\ldots,M_\\beta$. Here $\\beta$ is an index that controls the number of points in the interpolant and the indices $\\alpha$ and $i$ can be ingored in this tutorial.\n\nI included $\\alpha$ and $i$ because they will be useful in later tutotorials that build multi-fidelity multi-variate interpolants of functions $f_\\alpha:\\reals^D\\to\\reals$. In such cases i is used to denote the dimension $i=1\\ldots,D$ and $\\alpha$ is an index functions of different accuracy (fidelity).\n\n\nThe properties of interpolants depends on two factors. The way the interpolation points $\\rv_i^{(j)}$ are constructed and the form of the basis functions $\\phi$.\n\nIn this tutorial I will introduce two types on interpolation strategies based on local and global basis functions.\n\n\n## Lagrange Interpolation\nLagrange interpolation uses global polynomials as basis functions\n\n\\begin{align}\\phi_{i,j}(\\rv_i) = \\prod_{k=1,k\\neq j}^{m_{\\beta_i}}\\frac{\\rv_i-\\rv_i^{(k)}}{\\rv_i^{(j)}-\\rv_i^{(k)}}.\\end{align}\n\nThe error of a Lagrange interpolant on the interval $[a,b]$ is given by\n\n\\begin{align}e(\\rv_i) = f_\\alpha(\\rv_i)- f_{\\alpha,\\beta}(\\rv_i)\\leq\\frac{f_\\alpha^{(m_\\beta+1)}(c)}{(m_\\beta+1)!}\\prod_{j=1}^{m_{\\beta_i}}(\\rv_i-\\rv_i^{(j)})\\end{align}\n\nwhere $f_\\alpha^{(n+1)}(c)$ is the (n+1)-th derivative of the function at some point in $c\\in[a,b]$.\n\nThe error in the polynomial is bounded by\n\n\\begin{align}\\max_{\\rv_i\\in[a,b]}\\lvert e(\\rv_i)\\rvert = \\max_{\\rv_i\\in[a,b]}\\left\\lvert f_\\alpha(\\rv_i)- f_{\\alpha,\\beta}(\\rv_i)\\right\\rvert  \\leq \\frac{1}{(n+1)!} \\max_{\\rv_i\\in[a,b]}\\prod_{j=1}^{m_{\\beta_i}}\\left\\lvert (\\rv_i-\\rv_i^{(j)})\\right\\rvert\\max_{c\\in[a,b]} \\left\\lvert f_\\alpha^{(n+1)}(c)\\right\\rvert\\end{align}\n\nThis result shows that the choice of inteprolation points matter. Specificallly, we can not change the derivatives of a function but we can attempt to minimize\n\n\\begin{align}\\prod_{j=1}^{m_{\\beta_i}}\\left\\lvert(\\rv_i-\\rv_i^{(j)})\\right\\rvert\\end{align}\n\nChebyshev points are a very good choice of interpolation points that produce a small value of the quantity above. They are given by\n\n\\begin{align}z_{i}^{(j)}=\\cos\\left(\\frac{(j-1)\\pi}{m_{\\beta_i}-1}\\right),\\qquad j=1,\\ldots,m_{\\beta_i}\\end{align}\n\n\n## Piecewise polynomial interpolation\n\n\n\\begin{align}\\max_{\\rv\\in[\\rv_i^{(j)},\\rv_i^{(j+1)}]}\\lvert f_\\alpha(\\rv_i)-f_{\\alpha,\\beta}(\\rv_i)\\lvert \\leq \\frac{h^3}{72\\sqrt{3}} \\max_{\\rv_i\\in[\\rv_i^{(j)},\\rv_i^{(j+1)}]} f_\\alpha^{(3)}(\\rv_i)\\end{align}\n\nA proof of this lemma can be found [here](https://eng.libretexts.org/Workbench/Math%2C_Numerics%2C_and_Programming_(Ethan's)/01%3A_Unit_I_-_(Numerical)_Calculus_and_Elementary_Programming_Concepts/1.02%3A_Interpolation/1.2.01%3A_Interpolation_of_Univariate_Functions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pyapprox.surrogates.interp.tensorprod import (\n    UnivariatePiecewiseQuadraticBasis, UnivariateLagrangeBasis,\n    TensorProductInterpolant)\n\n\n#The following code compares polynomial and piecewise polynomial univariate basis functions.\nnnodes = 5\nsamples = np.linspace(-1, 1, 201)\nax = plt.subplots(1, 2, figsize=(2*8, 6), sharey=True)[1]\ncheby_nodes = np.cos(np.arange(nnodes)*np.pi/(nnodes-1))\nlagrange_basis = UnivariateLagrangeBasis()\nlagrange_basis_vals = lagrange_basis(cheby_nodes, samples)\nax[0].plot(samples, lagrange_basis_vals)\nax[0].plot(cheby_nodes, cheby_nodes*0, 'ko')\nequidistant_nodes = np.linspace(-1, 1, nnodes)\nquadratic_basis = UnivariatePiecewiseQuadraticBasis()\npiecewise_basis_vals = quadratic_basis(equidistant_nodes, samples)\n_ = ax[1].plot(samples, piecewise_basis_vals)\n_ = ax[1].plot(equidistant_nodes, equidistant_nodes*0, 'ko')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the unlike the lagrange basis the picewise polynomial basis is non-zero only on a local region of the input space.\n\nThe compares the accuracy of lagrange basis the picewise polynomial approximations for a piecewise continuous function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fun(samples):\n    yy = samples[0].copy()\n    yy[yy > 1/3] = 1.0\n    yy[yy <= 1/3] = 0.\n    return yy[:, None]\n\n\nlagrange_interpolant = TensorProductInterpolant([lagrange_basis])\nquadratic_interpolant = TensorProductInterpolant([quadratic_basis])\naxs = plt.subplots(1, 2, figsize=(2*8, 6))[1]\n[ax.plot(samples, fun(samples[None, :])) for ax in axs]\ncheby_nodes = -np.cos(np.arange(nnodes)*np.pi/(nnodes-1))\nvalues = fun(cheby_nodes[None, :])\nlagrange_interpolant.fit([cheby_nodes], values)\nequidistant_nodes = np.linspace(-1, 1, nnodes)\nvalues = fun(equidistant_nodes[None, :])\nquadratic_interpolant.fit([equidistant_nodes], values)\naxs[0].plot(samples, lagrange_interpolant(samples[None, :]), ':')\naxs[0].plot(cheby_nodes, values, 'o')\naxs[1].plot(samples, quadratic_interpolant(samples[None, :]), '--')\n_ = axs[1].plot(equidistant_nodes, values, 's')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Lagrange polynomials induce oscillations around the discontinuity, which significantly decreases the convergence rate of the approximation. The picewise quadratic also over and undershoots around the discontinuity, but the phenomena is localized.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets see how the error changes as we increase the number of nodes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "axs = plt.subplots(1, 2, figsize=(2*8, 6))[1]\n[ax.plot(samples, fun(samples[None, :])) for ax in axs]\nfor nnodes in [3, 5, 9, 17]:\n    nodes = np.cos(np.arange(nnodes)*np.pi/(nnodes-1))\n    values = fun(nodes[None, :])\n    lagrange_interpolant.fit([nodes], values)\n    nodes = np.linspace(-1, 1, nnodes)\n    values = fun(nodes[None, :])\n    quadratic_interpolant.fit([nodes], values)\n    axs[0].plot(samples, lagrange_interpolant(samples[None, :]), ':')\n    _ = axs[1].plot(samples, quadratic_interpolant(samples[None, :]), '--')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probability aware interpolation for UQ\nWhen interpolants are used for UQ we do not need the approximation to be accurate everywhere but rather only in regions of high-probability. First lets see what happens when we approximate a function using an interpolant that targets accuracy with respect to a dominating measure $\\nu$ when really needed an approximation that targets accuracy with respect to a different measure $w$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy import stats\nfrom pyapprox.variables.joint import IndependentMarginalsVariable\nfrom pyapprox.benchmarks import setup_benchmark\nfrom pyapprox.surrogates.orthopoly.quadrature import (\n    gauss_jacobi_pts_wts_1D)\nfrom pyapprox.util.utilities import cartesian_product\n\nnvars = 1\nc = np.array([20])\nw = np.array([0])\nbenchmark = setup_benchmark(\n    \"genz\", test_name=\"oscillatory\", nvars=nvars, coeff=[c, w])\n\nalpha_stat, beta_stat = 11, 11\ntrue_rv = IndependentMarginalsVariable(\n    [stats.beta(a=alpha_stat, b=beta_stat)]*nvars)\n\ninterp = TensorProductInterpolant([lagrange_basis]*nvars)\nopt_interp = TensorProductInterpolant([lagrange_basis]*nvars)\n\nalpha_poly, beta_poly = 0, 0\nntrain_samples = 7\nxx = gauss_jacobi_pts_wts_1D(ntrain_samples, alpha_poly, beta_poly)[0]\ntrain_samples = cartesian_product([(xx+1)/2]*nvars)\ntrain_values = benchmark.fun(train_samples)\ninterp.fit(train_samples, train_values)\n\nopt_xx = gauss_jacobi_pts_wts_1D(ntrain_samples, beta_stat-1, alpha_stat-1)[0]\nopt_train_samples = cartesian_product([(opt_xx+1)/2]*nvars)\nopt_train_values = benchmark.fun(opt_train_samples)\nopt_interp.fit(opt_train_samples, opt_train_values)\n\n\nax = plt.subplots(1, 1, figsize=(8, 6))[1]\nplot_xx = np.linspace(0, 1, 101)\ntrue_vals = benchmark.fun(plot_xx[None, :])\npbwt = r\"\\pi\"\nax.plot(plot_xx, true_vals, '-r', label=r'$f(z)$')\nax.plot(plot_xx, interp(plot_xx[None, :]), ':k', label=r'$f_M^\\nu$')\nax.plot(train_samples[0], train_values[:, 0], 'ko', ms=10,\n        label=r'$\\mathcal{Z}_{M}^{\\nu}$')\nax.plot(plot_xx, opt_interp(plot_xx[None, :]), '--b', label=r'$f_M^%s$' % pbwt)\nax.plot(opt_train_samples[0], opt_train_values[:, 0], 'bs',\n        ms=10, label=r'$\\mathcal{Z}_M^%s$' % pbwt)\n\npdf_vals = true_rv.pdf(plot_xx[None, :])[:, 0]\nax.set_ylim(true_vals.min()-0.1*abs(true_vals.min()),\n            max(true_vals.max(), pdf_vals.max())*1.1)\nax.fill_between(\n    plot_xx, ax.get_ylim()[0], pdf_vals+ax.get_ylim()[0],\n    alpha=0.3, visible=True,\n    label=r'$%s(z)$' % pbwt)\nax.set_xlabel(r'$M$', fontsize=24)\n_ = ax.legend(fontsize=18, loc=\"upper right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see the approximation that targets the uniform norm is \"more accurate\" on average over the domain, but the interpolant that directly targets accuracy with respect to the desired Beta distribution is more accurate in the regions of non-negligible probability.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets looks at how the accuracy changes with the \"distance\" between the dominating and target measures. This demonstrates the numerical impact of the main theorem in [XJD2013]_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_density_ratio_beta(num, true_rv, alpha_stat_2, beta_stat_2):\n    beta_rv2 = IndependentMarginalsVariable(\n        [stats.beta(a=alpha_stat_2, b=beta_stat_2)]*nvars)\n    print(beta_rv2, true_rv)\n    xx = np.random.uniform(0, 1, (nvars, 100000))\n    density_ratio = true_rv.pdf(xx)/beta_rv2.pdf(xx)\n    II = np.where(np.isnan(density_ratio))[0]\n    assert II.shape[0] == 0\n    return density_ratio.max()\n\n\ndef compute_L2_error(interp, validation_samples, validation_values):\n    nvalidation_samples = validation_values.shape[0]\n    approx_vals = interp(validation_samples)\n    l2_error = np.linalg.norm(validation_values-approx_vals)/np.sqrt(\n        nvalidation_samples)\n    return l2_error\n\n\nnvars = 3\nc = np.array([20, 20, 20])\nw = np.array([0, 0, 0])\nbenchmark = setup_benchmark(\n    \"genz\", test_name=\"oscillatory\", nvars=nvars, coeff=[c, w])\ntrue_rv = IndependentMarginalsVariable(\n    [stats.beta(a=alpha_stat, b=beta_stat)]*nvars)\n\nnvalidation_samples = 1000\nvalidation_samples = true_rv.rvs(nvalidation_samples)\nvalidation_values = benchmark.fun(validation_samples)\ninterp = TensorProductInterpolant([lagrange_basis]*nvars)\n\nalpha_polys = np.arange(0., 11., 2.)\nntrain_samples_list = np.arange(2, 20, 2)\nax = plt.subplots(1, 1, figsize=(8, 6))[1]\nfor alpha_poly in alpha_polys:\n    beta_poly = alpha_poly\n    density_ratio = compute_density_ratio_beta(\n        nvars, true_rv, beta_poly+1, alpha_poly+1)\n    results = []\n    for ntrain_samples in ntrain_samples_list:\n        xx = gauss_jacobi_pts_wts_1D(ntrain_samples, alpha_poly, beta_poly)[0]\n        nodes_1d = [(xx+1)/2]*nvars\n        train_samples = cartesian_product(nodes_1d)\n        train_values = benchmark.fun(train_samples)\n        interp.fit(nodes_1d, train_values)\n        l2_error = compute_L2_error(\n            interp, validation_samples, validation_values)\n        results.append(l2_error)\n    ax.semilogy(ntrain_samples_list, results,\n                label=\"{0:1.2f}\".format(density_ratio))\n\nax.set_xlabel(r'$M$', fontsize=24)\nax.set_ylabel(r'$\\| f-f_M^\\nu\\|_{L^2_%s}$' % pbwt, fontsize=24)\n_ = ax.legend(ncol=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [XJD2013] [Chen Xiaoxiao, Park Eun-Jae, Xiu Dongbin. A flexible numerical approach for quantification of epistemic uncertainty. J. Comput. Phys., 240 (2013), pp. 211-224](https://doi.org/10.1016/j.jcp.2013.01.018)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}