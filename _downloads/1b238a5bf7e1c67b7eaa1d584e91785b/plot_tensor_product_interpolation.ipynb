{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\rvset{{\\mathcal{Z}}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\text{d}#1}\\newcommand{\\mat}[1]{{\\boldsymbol{\\mathrm{#1}}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tensor-product Interpolation\nMany simulation models are extremely computationally expensive such that adequately understanding their behaviour and quantifying uncertainty can be computationally intractable for any of the aforementioned techniques. Various methods have been developed to produce surrogates of the model response to uncertain parameters, the most efficient are goal-oriented in nature and target very specific uncertainty measures.\n\nGenerally speaking surrogates are built using a \"small\" number of model simulations and are then substituted in place of the expensive simulation models in future analysis. Some of the most popular surrogate types include polynomial chaos expansions (PCE) [XKSISC2002]_, Gaussian processes (GP) [RWMIT2006]_, and sparse grids (SG) [BGAN2004]_.\n\nReduced order models (e.g. [SFIJNME2017]_) can also be used to construct surrogates and have been applied successfully for UQ on many applications. These methods do not construct response surface approximations, but rather solve the governing equations on a reduced basis. PyApprox does not currently implement reduced order modeling, however the modeling analyis tools found in PyApprox can easily be applied to assess or design systems based on reduced order models.\n\nThe use of surrogates for model analysis consists of two phases: (1) construction; and (2) post-processing.\n\n## Construction\nIn this section we show how to construct a surrogate using tensor-product Lagrange interpolation.\n\n### Tensor-product Lagrange interpolation\n\nLet $\\hat{f}_{\\boldsymbol{\\alpha},\\boldsymbol{\\beta}}(\\mathbf{z})$ be an M-point tensor-product interpolant of the function $\\hat{f}_{\\boldsymbol{\\alpha}}$. This interpolant is a weighted linear combination of tensor-product of univariate Lagrange polynomials\n\n\\begin{align}\\phi_{i,j}(z_i) = \\prod_{k=1,k\\neq j}^{m_{\\beta_i}}\\frac{z_i-z_i^{(k)}}{z_i^{(j)}-z_i^{(k)}}, \\quad i\\in[d],\\end{align}\n\n\ndefined on a set of univariate points $z_{i}^{(j)},j\\in[m_{\\beta_i}]$  Specifically the multivariate interpolant is given by\n\n\\begin{align}\\hat{f}_{\\boldsymbol{\\alpha},\\boldsymbol{\\beta}}(\\mathbf{z}) = \\sum_{\\boldsymbol{j}\\le\\boldsymbol{\\beta}} \\hat{f}_{\\boldsymbol{\\alpha}}(\\mathbf{z}^{(\\boldsymbol{j})})\\prod_{i\\in[d]}\\phi_{i,j_i}(z_i).\\end{align}\n\nThe partial ordering $\\boldsymbol{j}\\le\\boldsymbol{\\beta}$ is true if all the component wise conditions are true.\n\nConstructing the interpolant requires evaluating the function $\\hat{f}_{\\boldsymbol{\\alpha}}$ on the grid of points\n\n\\begin{align}\\mathcal{Z}_{\\boldsymbol{\\beta}} = \\bigotimes_{i=1}^d \\mathcal{Z}_{\\beta_i}^i=\\begin{bmatrix}\\mathbf{z}^{(1)} & \\cdots&\\mathbf{z}^{(M_{\\boldsymbol{\\beta}})}\\end{bmatrix}\\in\\mathbb{R}^{d\\times M_{\\boldsymbol{\\beta}}}\\end{align}\n\n\nWe denote the resulting function evaluations by\n\n\\begin{align}\\mathcal{F}_{\\boldsymbol{\\alpha},\\boldsymbol{\\beta}}=\\hat{f}_{\\boldsymbol{\\alpha}}(\\mathcal{Z}_{\\boldsymbol{\\beta}})=\\begin{bmatrix}\\hat{f}_{\\boldsymbol{\\alpha}}(\\mathbf{z}^{(1)}) \\quad \\cdots\\quad \\hat{f}_{\\boldsymbol{\\alpha}}(\\mathbf{z}^{(M_{\\boldsymbol{\\beta}})})\\end{bmatrix}^T\\in\\mathbb{R}^{M_{\\boldsymbol{\\beta}}\\times q},\\end{align}\n\nwhere the number of points in the grid is $M_{\\boldsymbol{\\beta}}=\\prod_{i\\in[d]} m_{\\beta_i}$\n\nIt is often reasonable to assume that, for any $\\mathbf{z}$, the cost of each simulation is constant for a given $\\boldsymbol{\\alpha}$. So letting $W_{\\boldsymbol{\\alpha}}$ denote the cost of a single simulation, we can write the total cost of evaluating the interpolant $W_{\\boldsymbol{\\alpha},\\boldsymbol{\\beta}}=W_{\\boldsymbol{\\alpha}} M_{\\boldsymbol{\\beta}}$. Here we have assumed that the computational effort to compute the interpolant once data has been obtained is negligible, which is true for sufficiently expensive models $\\hat{f}_{\\boldsymbol{\\alpha}}$.\nHere we will use the nested Clenshaw-Curtis points\n\n\\begin{align}z_{i}^{(j)}=\\cos\\left(\\frac{(j-1)\\pi}{m_{\\beta_i}-1}\\right),\\qquad j=1,\\ldots,m_{\\beta_i}\\end{align}\n\nto define the univariate Lagrange polynomials. The number of points $m(l)$ of this rule grows exponentially with the level $l$, specifically\n$m(0)=1$ and $m(l)=2^{l}+1$ for $l\\geq1$. The univariate Clenshaw-Curtis points, the tensor-product grid $\\mathcal{Z}_{\\boldsymbol{\\beta}}$, and two multivariate Lagrange polynomials with their corresponding univariate Lagrange polynomials are shown below for $\\boldsymbol{\\beta}=(2,2)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom pyapprox.util.utilities import cartesian_product\nfrom pyapprox.util.visualization import get_meshgrid_function_data, plt\nfrom pyapprox.util.utilities import get_tensor_product_quadrature_rule\nfrom pyapprox.surrogates.orthopoly.quadrature import clenshaw_curtis_pts_wts_1D\nfrom pyapprox.surrogates.approximate import adaptive_approximate\nfrom pyapprox.surrogates.interp.adaptive_sparse_grid import (\n    tensor_product_refinement_indicator)\nfrom functools import partial\nfrom pyapprox.surrogates.orthopoly.quadrature import (\n    clenshaw_curtis_in_polynomial_order, clenshaw_curtis_rule_growth)\nfrom pyapprox.surrogates.interp.tensorprod import (\n    canonical_univariate_piecewise_polynomial_quad_rule)\nfrom pyapprox.benchmarks.benchmarks import setup_benchmark\nfrom pyapprox.surrogates.interp.tensorprod import (\n    UnivariatePiecewiseQuadraticBasis, UnivariateLagrangeBasis,\n    TensorProductInterpolant, TensorProductBasis)\n\nnnodes_1d = [5, 9]\nnodes_1d = [-np.cos(np.arange(nnodes)*np.pi/(nnodes-1))\n            for nnodes in nnodes_1d]\nnodes = cartesian_product(nodes_1d)\nlagrange_basis_1d = UnivariateLagrangeBasis()\ntp_lagrange_basis = TensorProductBasis([lagrange_basis_1d]*2)\n\nfig = plt.figure(figsize=(2*8, 6))\nax = fig.add_subplot(1, 2, 1, projection='3d')\nii, jj = 1, 3\ntp_lagrange_basis.plot_single_basis(ax, nodes_1d, ii, jj, nodes)\n\nax = fig.add_subplot(1, 2, 2, projection='3d')\nlevel = 2\nii, jj = 2, 4\ntp_lagrange_basis.plot_single_basis(ax, nodes_1d, ii, jj, nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To construct a surrogate using tensor product interpolation we simply multiply all such basis functions by the value of the function $f_\\ai$ evaluated at the corresponding interpolation point. The following uses tensor product interpolation to approximate the simple function\n\n\\begin{align}f_\\ai(\\rv) = \\cos(2\\pi\\rv_1)\\cos(2\\pi\\rv_2), \\qquad \\rv\\in\\rvdom=[-1,1]^2\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fun(z): return (np.cos(2*np.pi*z[0, :]) *\n                    np.cos(2*np.pi*z[1, :]))[:, np.newaxis]\n\n\nlagrange_interpolant = TensorProductInterpolant([lagrange_basis_1d]*2)\nvalues = fun(nodes)\nlagrange_interpolant.fit(nodes_1d, values)\n\n\nmarker_color = 'k'\nalpha = 1.0\nfig, axs = plt.subplots(1, 1, figsize=(8, 6))\naxs.plot(nodes[0, :], nodes[1, :], 'o',\n         color=marker_color, ms=10, alpha=alpha)\n\nplot_limits = [-1, 1, -1, 1]\nnum_pts_1d = 101\nX, Y, Z = get_meshgrid_function_data(\n    lagrange_interpolant, plot_limits, num_pts_1d)\n\nnum_contour_levels = 10\nlevels = np.linspace(Z.min(), Z.max(), num_contour_levels)\ncset = axs.contourf(\n    X, Y, Z, levels=levels, cmap=\"coolwarm\", alpha=alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The error in the tensor product interpolant is given by\n\n\\begin{align}\\lVert f_\\ai-f_{\\ai,\\bi}\\rVert_{L^\\infty(\\rvdom)} \\le C_{d,s} N_{\\bi}^{-s/d}\\end{align}\n\nwhere $f_\\alpha$ has continuous mixed derivatives of order $s$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-processing\nOnce a surrogate has been constructed it can be used for many different purposes. For example one can use it to estimate moments, perform sensitivity analysis, or simply approximate the evaluation of the expensive model at new locations where expensive simulation model data is not available.\n\nTo use the surrogate for computing moments we simply draw realizations of the input random variables $\\rv$ and evaluate the surrogate at those samples. We can approximate the mean of the expensive simluation model as the average of the surrogate values at the random samples.\n\nWe know from `sphx_glr_auto_tutorials_multi_fidelity_plot_monte_carlo.py` that the error in the Monte carlo estimate of the mean using the surrogate is\n\n\\begin{align}\\mean{\\left(Q_{\\alpha}-\\mean{Q}\\right)^2}&=N^{-1}\\var{Q_\\alpha}+\\left(\\mean{Q_{\\alpha}}-\\mean{Q}\\right)^2\\\\\n  &\\le N^{-1}\\var{Q_\\alpha}+C_{d,s} N_{\\bi}^{-s/d}\\end{align}\n\nBecause a surrogate is inexpensive to evaluate the first term can be driven to zero so that only the bias remains. Thus the error in the Monte Carlo estimate of the mean using the surrogate is dominated by the error in the surrogate. If this error can be reduced more quickly than \\frac{N^{-1}} (as is the case for low-dimensional tensor-product interpolation) then using surrogates for computing moments is very effective.\n\nNote that moments can be estimated without using Monte-Carlo sampling by levaraging properties of the univariate interpolation rules used to build the multi-variate interpolant. Specifically, the expectation of a tensor product interpolant can be computed without explicitly forming the interpolant and is given by\n\n\\begin{align}\\mu_{\\bi}=\\int_{\\rvdom} \\sum_{\\V{j}\\le\\bi}f_\\ai(\\rv^{(\\V{j})})\\prod_{i=1}^d\\phi_{i,j_i}(\\rv_i) w(\\rv)\\,d\\rv=\\sum_{\\V{j}\\le\\bi} f_\\ai(\\rv^{(\\V{j})}) v_{\\V{j}}.\\end{align}\n\nThe expectation is simply the weighted sum of the Cartesian-product of the univariate quadrature weights\n\n\\begin{align}v_{\\V{j}}=\\prod_{i=1}^d\\int_{\\rvdom_i}{\\phi_{i,j_i}(\\rv_i)}\\,dw(\\rv_i),\\end{align}\n\nwhich can be computed analytically.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x, w = get_tensor_product_quadrature_rule(level, 2, clenshaw_curtis_pts_wts_1D)\nsurrogate_mean = fun(x)[:, 0].dot(w)\nprint('Quadrature mean', surrogate_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we have recomptued the values of $f$ at the interpolation samples, but in practice we sould just re-use the values collected when building the interpolant.\n\nNow let us compare the quadrature mean with the MC mean computed using the surrogate\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_samples = int(1e6)\nsamples = np.random.uniform(-1, 1, (2, num_samples))\nvalues = lagrange_interpolant(samples)\nmc_mean = values.mean()\nprint('Monte Carlo surrogate mean', mc_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Piecewise-polynomial approximation\nPolynomial interpolation accurately approximates smooth functions, however its accuracy degrades as the regularity of the target function decreases. For piecewise continuous functions, or functions with only a limited number of continuous derivaties, piecewise-polynomial approximation may be more appropriate.\n\nThe following plots two piecewise-quadratic basis functions in 2D\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(2*8, 6))\nnnodes_1d = [5, 5]\nnodes_1d = [np.linspace(-1, 1, nnodes) for nnodes in nnodes_1d]\nnodes = cartesian_product(nodes_1d)\ntp_quadratic_basis = TensorProductBasis(\n    [UnivariatePiecewiseQuadraticBasis()]*2)\nax = fig.add_subplot(1, 2, 1, projection='3d')\ntp_quadratic_basis.plot_single_basis(ax, nodes_1d, 2, 2, nodes)\nax = fig.add_subplot(1, 2, 2, projection='3d')\ntp_quadratic_basis.plot_single_basis(ax, nodes_1d, 0, 1, nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following compares the convergence of Lagrange and picewise polynomial tensor product interpolants. Change the benchmark to see the effect of smoothness on the approximation accuracy.\n\nFirst define wrappers to build the tensor product interpolants\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def build_lagrange_tp(max_level_1d):\n    univariate_quad_rule_info = [\n        clenshaw_curtis_in_polynomial_order, clenshaw_curtis_rule_growth,\n        None, None]\n    return adaptive_approximate(\n        benchmark.fun, benchmark.variable, \"sparse_grid\",\n        {\"refinement_indicator\": tensor_product_refinement_indicator,\n         \"max_level_1d\": max_level_1d,\n         \"univariate_quad_rule_info\": univariate_quad_rule_info,\n         \"max_nsamples\": np.inf}).approx\n\n\ndef build_piecewise_tp(max_level_1d):\n    basis_type = \"quadratic\"\n    # basis_type = \"linear\"\n    univariate_quad_rule_info = [\n        partial(canonical_univariate_piecewise_polynomial_quad_rule,\n                basis_type),\n        clenshaw_curtis_rule_growth, None, None]\n    return adaptive_approximate(\n        benchmark.fun, benchmark.variable, \"sparse_grid\",\n        {\"refinement_indicator\": tensor_product_refinement_indicator,\n         \"max_level_1d\": max_level_1d,\n         \"univariate_quad_rule_info\": univariate_quad_rule_info,\n         \"basis_type\": basis_type, \"max_nsamples\": np.inf}).approx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nvars = 2\nbenchmark = setup_benchmark(\"genz\", nvars=nvars, test_name=\"oscillatory\")\n# benchmark = setup_benchmark(\"genz\", nvars=nvars, test_name=\"c0continuous\",\n#                            c_factor=0.5, w=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a convergence study\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation_samples = benchmark.variable.rvs(1000)\nvalidation_values = benchmark.fun(validation_samples)\n\npiecewise_data = []\nlagrange_data = []\nfor level in range(1, 7):   # nvars = 2\n# for level in range(1, 4):  # nvars = 3\n    ltp = build_lagrange_tp(level)\n    lvalues = ltp(validation_samples)\n    lerror = np.linalg.norm(validation_values-lvalues)/np.linalg.norm(\n        validation_values)\n    lagrange_data.append([ltp.samples.shape[1], lerror])\n    ptp = build_piecewise_tp(level)\n    pvalues = ptp(validation_samples)\n    perror = np.linalg.norm(validation_values-pvalues)/np.linalg.norm(\n        validation_values)\n    piecewise_data.append([ptp.samples.shape[1], perror])\nlagrange_data = np.array(lagrange_data).T\npiecewise_data = np.array(piecewise_data).T\n\nax = plt.subplots()[1]\nax.loglog(*lagrange_data, '-o', label='Lagrange')\nax.loglog(*piecewise_data, '--o', label='Piecewise')\nwork = piecewise_data[0][1:3]\nax.loglog(work, work**(-1.0), ':', label='linear rate')\nax.loglog(work, work**(-2.0), ':', label='quadratic rate')\n_ = ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar behavior occurs when using quadrature.\n\nLoad in the benchmark.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nvars = 2\nbenchmark = setup_benchmark(\"genz\", nvars=nvars, test_name=\"oscillatory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a convergence study\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "piecewise_data = []\nlagrange_data = []\nfor level in range(1, 7):   # nvars = 2\n    ltp = build_lagrange_tp(level)\n    lvalues = ltp.moments()[0, 0]\n    lerror = np.linalg.norm(benchmark.mean-lvalues)/np.linalg.norm(\n        benchmark.mean)\n    lagrange_data.append([ltp.samples.shape[1], lerror])\n    ptp = build_piecewise_tp(level)\n    pvalues = ptp.moments()[0, 0]\n    perror = np.linalg.norm(benchmark.mean-pvalues)/np.linalg.norm(\n        benchmark.mean)\n    piecewise_data.append([ptp.samples.shape[1], perror])\nlagrange_data = np.array(lagrange_data).T\npiecewise_data = np.array(piecewise_data).T\n\nax = plt.subplots()[1]\nax.loglog(*lagrange_data, '-o', label='Lagrange')\nax.loglog(*piecewise_data, '--o', label='Piecewise')\nwork = piecewise_data[0][1:3]\nax.loglog(work, work**(-1.0), ':', label='linear rate')\nax.loglog(work, work**(-2.0), ':', label='quadratic rate')\n_ = ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n\n.. [XKSISC2002] [D. Xiu and G.E. Karniadakis. The Wiener-Askey Polynomial Chaos for stochastic differential equations. SIAM J. Sci. Comput., 24(2), 619-644, 2002.](http://dx.doi.org/10.1137/S1064827501387826)\n\n.. [RWMIT2006] [C.E Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.](http://www.gaussianprocess.org/gpml/chapters/)\n\n.. [BGAN2004] [H. Bungartz and M. Griebel. Sparse Grids. Acta Numerica, 13, 147-269, 2004.](http://dx.doi.org/10.1017/S0962492904000182)\n\n.. [SFIJNME2017] [C Soize and C. Farhat. A nonparametric probabilistic approach for quantifying uncertainties in low-dimensional and high-dimensional nonlinear models. International Journal for Numerical Methods in Engineering, 109(6), 837-888, 2017.](https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.5312)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}