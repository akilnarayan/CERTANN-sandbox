{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-level and Multi-index Collocation\nThsi tutorial introduces multi-level  [TJWGSIAMUQ2015]_ and multi-index collocation [HNTTCMAME2016]_. It assumes knowledge of the material covered in `sphx_glr_auto_tutorials_surrogates_plot_tensor_product_interpolation.py` and `sphx_glr_auto_tutorials_surrogates_plot_sparse_grids.py`.\n\nModels often utilize numerical discretizations to solve the equations governing the system dynamics. For example, finite-elements, spectral collocation, etc are often used to solve partial differential equations. `sphx_glr_auto_examples_plot_pde_convergence.py` demonstrates how the numerical discretization, specifically the spatial mesh discretization and time-step, of a spectral collocation model of transient advection diffusion effects the accuracy of the model output.\n\n.. list-table::\n\n   * - .. _multilevel_hierarchy:\n\n       .. figure:: ../../figures/multilevel-hierarchy.png\n          :width: 50%\n          :align: center\n\n          A multi-level hierarchy formed by increasing mesh discretizations.\n\n## An observation\nMultilevel collocation was introduced to reduce the cost of building surrogates of models when a one-dimensional hierarchy of numerical discretizations of a model  $f_\\alpha(\\rv), \\alpha=0,1,\\ldots$ are available such that\n\n\\begin{align}\\lVert f-f_\\alpha\\rVert \\le \\lVert f-f_{\\alpha^\\prime}\\rVert\\end{align}\n\nif $\\alpha^\\prime < \\alpha.$ and the work $W_\\alpha$ increases with fidelity.\n\nMultilevel collocation can be implemented by modifying sparse grid interplation developed for a single model fidelity. The modification is based on the observation that the discrepancy between two consecutive models and the lower-fidelity model will be computationally cheaper to approximate than higher-fidelity model.\n\nThe following code demonstrates this observation for a simple 1D model with two numerical discretizations\n\n\\begin{align}f_\\alpha = \\cos(\\pi (\\rv+1)/2+\\epsilon_\\alpha)\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom functools import partial\nfrom scipy import stats\nfrom pyapprox.variables.joint import IndependentMarginalsVariable\nimport matplotlib.pyplot as plt\nfrom pyapprox.surrogates.approximate import adaptive_approximate\nfrom pyapprox.surrogates.interp.adaptive_sparse_grid import (\n    tensor_product_refinement_indicator, isotropic_refinement_indicator,\n    variance_refinement_indicator)\nfrom pyapprox.variables.transforms import ConfigureVariableTransformation\nfrom pyapprox.interface.wrappers import MultiIndexModel\n\ndef fun(eps, zz):\n    return np.cos(np.pi*(zz[0]+1)/2+eps)[:, None]\n\n\nfuns = [partial(fun, 0.25), partial(fun, 0)]\nvariable = IndependentMarginalsVariable([stats.uniform(-1, 2)])\nranges = variable.get_statistics(\"interval\", 1.0).flatten()\nzz = np.linspace(*ranges, 101)\naxs = plt.subplots(1, 3, figsize=(3*8, 6), sharey=True)[1]\n\ndef build_tp(fun, max_level_1d):\n    tp = adaptive_approximate(\n        fun, variable, \"sparse_grid\",\n        {\"refinement_indicator\": tensor_product_refinement_indicator,\n         \"max_level_1d\": max_level_1d,\n         \"univariate_quad_rule_info\": None}).approx\n    return tp\n\nlf_approx = build_tp(funs[0], 2)\naxs[0].plot(zz, funs[0](zz[None, :]), 'k', label=r\"$f_0$\")\naxs[0].plot(zz, funs[1](zz[None, :]), 'r', label=r\"$f_1$\")\naxs[0].plot(lf_approx.samples[0], lf_approx.values[:, 0], 'ko')\naxs[0].plot(zz, lf_approx(zz[None])[:, 0], 'g:', label=r\"$f_{0,\\mathcal{I}_0}$\")\naxs[0].legend()\n\nhf_approx = build_tp(funs[1], 1)\naxs[1].plot(zz, funs[1](zz[None, :]), 'r', label=r\"$f_1$\")\naxs[1].plot(hf_approx.samples[0], hf_approx.values[:, 0], 'ro')\naxs[1].plot(zz, hf_approx(zz[None])[:, 0], ':', color='gray',\n            label=r\"$f_{1,\\mathcal{I}_1}$\")\naxs[1].legend()\n\ndef discrepancy_fun(fun1, fun0, zz):\n    return fun1(zz)-fun0(zz)\n\ndiscp_approx = build_tp(partial(discrepancy_fun, funs[1], lf_approx), 1)\naxs[2].plot(zz, funs[1](zz[None, :]), 'r', label=r\"$f_1$\")\naxs[2].plot(zz, funs[1](zz[None, :])-funs[0](zz[None, :]), 'k',\n            label=r\"$f_1-f_0$\")\naxs[2].plot(discp_approx.samples[0], discp_approx.values[:, 0], 'ko')\naxs[2].plot(zz, discp_approx(zz[None, :]),\n            'g:', label=r\"$f_{0,\\mathcal{I}_0}+\\delta_{\\mathcal{I}_1}$\")\naxs[2].plot(zz, lf_approx(zz[None])+discp_approx(zz[None, :]),\n            'b:', label=r\"$f_{0,\\mathcal{I}_1}+\\delta_{\\mathcal{I}_1}$\")\n[ax.set_xlabel(r\"$z$\") for ax in axs]\n_ = axs[2].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The left plot shows that using 5 samples of the low-fidelity model produces an accurate approximation of the low-fidelity model, but it will be a poor approximation of the high fidelity model in the limit of infinite low-fidelity data. The middle plot shows three samples of the high-fidelity model also produces a poor approximation, but if more samples were added the approximation would coverge to the high-fidelity model. In contrast the right plot shows that 5 samples of the low-fideliy model plus three samples of the high-fidelity model produces a good approximation of the high-fidelity model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code plots different interpolations $f_{\\alpha,\\beta}$ of $f_\\alpha$ for various $\\alpha$ and number of interpolation points (controled by $\\beta$). Instead of building each interpolant with a custom function, we just build a multi-index sparse grid that uses a tensor-product refinement criterion to define the set $\\mathcal{I}=\\{[\\alpha,\\beta]:\\alpha \\le l_0, \\; \\beta\\le l_1\\}$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_level = 2\nnvars = 1\nconfig_values = [np.asarray([0.25, 0])]\n# config_values = [np.asarray([0.25, 0.125, 0])]\nconfig_var_trans = ConfigureVariableTransformation(config_values)\n\n\ndef setup_model(config_values):\n    eps = config_values[0]\n    return partial(fun, eps)\n\nmi_model = MultiIndexModel(setup_model, config_values)\n\n# cannot let max_level_1d for configure variables be larger\n# than number of configure variables\nmax_level_1d = [max_level, len(config_values[0])-1]  # [l_0, l_1]\n\nfig, axs = plt.subplots(\n    max_level_1d[1]+1, max_level_1d[0]+1,\n    figsize=((max_level_1d[0]+1)*8, (max_level_1d[1]+1)*6),\n    sharey=True)\n\ntp_approx = adaptive_approximate(\n    mi_model, variable, \"sparse_grid\",\n    {\"refinement_indicator\": tensor_product_refinement_indicator,\n     \"max_level_1d\": max_level_1d,\n     \"config_variables_idx\": nvars,\n     \"config_var_trans\": config_var_trans,\n     \"univariate_quad_rule_info\": None}).approx\n\nfrom pyapprox.surrogates.interp.sparse_grid import (\n    get_subspace_values, evaluate_sparse_grid_subspace)\nfun_colors = ['r', 'k', 'cyan']\napprox_colors = ['b', 'g', 'pink']\nfor ii, subspace_index in enumerate(tp_approx.subspace_indices.T):\n    subspace_values = get_subspace_values(\n        tp_approx.values, tp_approx.subspace_values_indices_list[ii])\n    jj, kk = subspace_index\n    subspace_samples = tp_approx.samples_1d[0][jj]\n    ax = axs[max_level_1d[1]-kk, jj]\n    ax.plot(\n        subspace_samples, subspace_values, 'o', color=fun_colors[kk])\n    for ll in range(max_level_1d[1]+1):\n        ax.plot(zz, mi_model._model_ensemble.functions[ll](zz[None, :]),\n                '-', color=fun_colors[ll], label=r\"$f_{%d}$\" % (jj))\n    subspace_approx_vals = evaluate_sparse_grid_subspace(\n        zz[None, :], subspace_index, subspace_values,\n        tp_approx.samples_1d, tp_approx.config_variables_idx)\n    ax.plot(zz, subspace_approx_vals, '--', color=approx_colors[kk],\n            label=r\"$f_{%d,%d}$\" % (kk, jj))\n    ax.legend()\n_ = [[ax.set_ylim([-1, 1]), ax.set_xlabel(r\"$z$\")] for ax in axs.flatten()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-level Collocation\nSimilar to sparse grids, multi-index collocation is a weighted combination of\nlow-resolution tensor products, like those shown in the last plot\n\n\\begin{align}f_{\\mathcal{I}}(z) = \\sum_{[\\alpha,\\beta]\\in \\mathcal{I}} c_{[\\alpha,\\beta]} f_{\\alpha,\\beta}(z),\\end{align}\n\nwhere the Smolay coefficients can be computed using the same formula used for traditional sparse grids. However, unlike sparse grids, we now have introduced configuration variables that change what model discretization is being evaluated.\n\nA level-one isotropic collocation algorithm uses top-left, bottom-left and bottom middle interpolants in the previous plot, i.e. $f_{1, 0}, f_{0, 0}, f_{0, 1}$, respectively. The level 2 approximation is plotted below. Note it is not a true isotropic grid because it cannot reach level 2 in the configuration variable which only uses two models. This is not true if more than 2 models are provided.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mi_approx = adaptive_approximate(\n    mi_model, variable, \"sparse_grid\",\n    {\"refinement_indicator\": isotropic_refinement_indicator,\n     \"max_level_1d\": max_level_1d,\n     \"config_variables_idx\": nvars,\n     \"max_level\": 2,\n     \"config_var_trans\": config_var_trans,\n     \"univariate_quad_rule_info\": None}).approx\n\nax = plt.subplots(figsize=(8, 6))[1]\nax.plot(zz, mi_approx(zz[None]), '--', label=r\"$f_\\mathcal{I}$\")\nfor ll in range(max_level_1d[1]+1):\n    ax.plot(zz, mi_model._model_ensemble.functions[ll](zz[None, :]),\n            '-', color=fun_colors[ll], label=r\"$f_{%d}$\" % (jj))\nax.legend()\n_ = ax.set_xlabel(r\"$z$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The approximation is close to the accuracy of $f_{1, 2}$ without needing as many evaluations of $f_{1}$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adaptivity\nThe the algorithm that adapts the sparse grid index set $\\mathcal{I}$ to the importance of each variable be modified for use with multi-index collocation [JEGG2019]_. The algorithm is highly effective as it balances the interpolation error due to using a finite number of training points with the cost of evaluating the models of varying accuracy.\n\nLets build a multi-level sparse grid \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nfrom pyapprox.surrogates.interp.adaptive_sparse_grid import (\n    plot_adaptive_sparse_grid_2d)\nfrom pyapprox.util.visualization import get_meshgrid_function_data\n\nconfig_values = [np.asarray([0.25, 0.125, 0])]\nconfig_var_trans = ConfigureVariableTransformation(config_values)\nmi_model = MultiIndexModel(setup_model, config_values)\n\n#The sparse grid uses the wall time of the model execution as a cost function by default, but here we will use a custom cost function because all models are trivial to evaluate.\n\ndef cost_function(config_sample):\n    canonical_config_sample = config_var_trans.map_to_canonical(\n        config_sample)\n    return (1+canonical_config_sample[0])**2\n\nclass AdaptiveCallback():\n    def __init__(self, validation_samples, validation_values):\n        self.validation_samples = validation_samples\n        self.validation_values = validation_values\n\n        self.nsamples = []\n        self.errors = []\n        self.sparse_grids = []\n\n    def __call__(self, approx):\n        self.nsamples.append(approx.samples.shape[1])\n        approx_values = approx.evaluate_using_all_data(\n            self.validation_samples)\n        error = (np.linalg.norm(\n            self.validation_values-approx_values) /\n                     self.validation_samples.shape[1])\n        self.errors.append(error)\n        self.sparse_grids.append(copy.deepcopy(approx))\n\n\nvalidation_samples = variable.rvs(100)\nvalidation_values = mi_model._model_ensemble.functions[-1](validation_samples)\nadaptive_callback = AdaptiveCallback(validation_samples, validation_values)\nsg = adaptive_approximate(\n    mi_model, variable, \"sparse_grid\",\n    {\"refinement_indicator\": variance_refinement_indicator,\n     \"max_level_1d\": [10,  len(config_values[0])-1],\n     \"univariate_quad_rule_info\": None,\n     \"max_level\": np.inf, \"max_nsamples\": 50,\n     \"config_variables_idx\": nvars,\n     \"config_var_trans\": config_var_trans,\n     \"cost_function\": cost_function,\n     \"callback\": adaptive_callback}).approx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now plot the adaptive algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, sharey=False, figsize=(16, 6))\ndef animate(ii):\n    [ax.clear() for ax in axs]\n    sg = adaptive_callback.sparse_grids[ii]\n    plot_adaptive_sparse_grid_2d(sg, axs=axs[:2])\n    axs[0].set_xlim([0, 10])\n    axs[0].set_ylim([0, len(config_values[0])-1])\n\nimport matplotlib.animation as animation\nani = animation.FuncAnimation(\n    fig, animate, interval=500,\n    frames=len(adaptive_callback.sparse_grids), repeat_delay=1000)\nani.save(\"adaptive_misc.gif\", dpi=100,\n         writer=animation.ImageMagickFileWriter())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The lower fidelity models are evaluated more until they can no longer reduce the error in the sparse grid. At this point the interpolation error of the low-fidelity models is dominated by the bias in the exact low-fidelity models. Changing the cost_function will change how many samples are used to evaluate each model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Three or more models\nThis tutorial only demonstrates the use of multi-level collocation with two models, but it can easily be used with three or more models. Try setting config_values = [np.asarray([0.25, 0.125, 0])]\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-index collocation\nMulti-index collocation is an extension of mulit-level collocation that can be used with models that have multiple parameters controlling the numerical discretization, for example, the spatial and temporal resoutions of a finite element solver. While this tutorial does not demonstrate multi-index collocation it is supported by PyApprox.\n\n.. list-table::\n\n   * - .. _multilevel_hierarchy:\n\n       .. figure:: ../../figures/multiindex-hierarchy.png\n          :width: 50%\n          :align: center\n\n          A multi-index hierarchy formed by increasing mesh discretizations in two different spatial directions.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n.. [HNTTCMAME2016] [A. Haji-Ali, F. Nobile, L. Tamellini, and R. Tempone. Multi-index stochastic collocation for random pdes. Computer Methods in Applied Mechanics and Engineering, 306:95 \u2013 122, 2016.](http://www. sciencedirect.com/science/article/pii/S0045782516301141, doi:10.1016/j.cma.2016.03.029)\n\n.. [TJWGSIAMUQ2015] [A. Teckentrup, P. Jantsch, C. Webster, and M. Gunzburger. A multilevel stochastic collocation method for partial differential equations with random input data. SIAM/ASA Journal on Uncertainty Quantification, 3(1):1046-1074, 2015.](https://doi.org/10.1137/140969002)\n\n.. [JEGG2019] [J.D. Jakeman, M.S. Eldred, G. Geraci, and A. Gorodetsky. Adaptive Multi-index Collocation for Uncertainty Quantification and Sensitivity Analysis. International Journal for Numerical Methods in Engineering (2019).](https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.6268)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}