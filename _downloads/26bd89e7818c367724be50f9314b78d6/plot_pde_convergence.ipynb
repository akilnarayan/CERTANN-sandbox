{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Convergence studies\nThis tutorial demonstrates how to investigate the convergence of parameterized numerical approximations, for example tensor product quadrature or numerical models used to solve partial differential equations with unknown inputs.\n\nFirst lets define a Integrator class which can be used to integrate multivariate functions with tensor product quadrature, that is compute\n\n\\begin{align}I(\\rv) = \\int_D f(x, \\rv)dx\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom pyapprox.analysis.convergence_studies import \\\n    run_convergence_study, plot_convergence_data\nfrom pyapprox.util.configure_plots import plt\nfrom pyapprox.surrogates import (\n    get_tensor_product_piecewise_polynomial_quadrature_rule,\n)\nfrom pyapprox.interface import (\n    evaluate_1darray_function_on_2d_array, WorkTrackingModel,\n    TimerModel\n)\nfrom scipy import stats\nfrom pyapprox.variables import (\n    IndependentMarginalsVariable, ConfigureVariableTransformation\n)\n\n\nclass Integrator(object):\n    def __init__(self, integrand):\n        self.integrand = integrand\n\n    def set_quad_rule(self, nsamples_1d):\n        self.xquad, self.wquad = \\\n            get_tensor_product_piecewise_polynomial_quadrature_rule(\n                nsamples_1d, [0, 1, 0, 1], degree=1)\n\n    def integrate(self, sample):\n        self.set_quad_rule(sample[-2:].astype(int))\n        val = self.integrand(sample[:-2], self.xquad)[:, 0].dot(self.wquad)\n        return val\n\n    def __call__(self, samples):\n        return evaluate_1darray_function_on_2d_array(\n            self.integrate, samples, None)\n\n    @staticmethod\n    def get_num_degrees_of_freedom(config_sample):\n        return np.prod(config_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To assess convergence we will use the function `run_convergence_study`. This routine requires a function that takes samples that consist of realizations of the random variables concatenated with any configuration variables which define the numerical resolution of the quadrature rule, in this case the number of quadrature points used in the first and second dimension.\n\nTo demonstrate its usage lets integrate the function\n\n\\begin{align}f(x, \\rv)=\\rv_1(x_1^2+x_2^2)\\end{align}\n\nwere $z$ is a uniform variable on $[0, 1]$.\nNow define this integrand and the true value as a function of the random samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "variable = IndependentMarginalsVariable([stats.uniform(0, 1)])\n\n\ndef integrand(sample, x):\n    return np.sum(sample[0]*x**2, axis=0)[:, None]\n\n\ndef true_value(samples):\n    return 2/3*samples.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We must also define the permissible values of the configuration variables that define the number of points $n_1, n_2$ in the quadrature rule. Here set $n_1=2^{j+1}+1$ and $n_2=2^{k+1}+1$ where $j,k=0\\ldots,9$. Now construct a `ConfigureVariableTransformation` that can map $j,k$ to  $n_1, n_2$ and back. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "config_values = [2**np.arange(1, 11)+1, 2**np.arange(1, 11)+1]\nconfig_var_trans = ConfigureVariableTransformation(config_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then define the values of j and k we wish to use to assess convergence. `validation_levels` $v_1,v_2$ specifies the values used to compute a reference solution if an exact solution is not known. `coarsest_levels` specifies the mininimum values $c_1,c_2$ of j, k to be used to integrate. Integrator will be used to integrate the integrand for all combinatinos of j,k in the tensor product of $\\{c_1,\\ldots v_1-1\\},$ and $\\{c_2,\\ldots v_2-1\\}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation_levels = [5, 5]\ncoarsest_levels = [0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convergence can be assessed with respect to the CPU time used to compute the integral. To return the time taken we must wrap `Integrator` in a WorkTrackingModel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Integrator(integrand)\ntimer_model = TimerModel(model, model)\nwork_model = WorkTrackingModel(timer_model, model,\n                               config_var_trans.num_vars())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The routine `run_convergence_study` also requires a function `get_num_degrees_of_freedom` which returns the number of degrees of freedom (DoF) for each realization of the configuration variables. In this case the number of DoF is $n_1n_2$. Finally we must specify the number of samples of $z$ used to evaluate the integral. Errors are reported as the average error over these samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "convergence_data = run_convergence_study(\n    work_model, variable, validation_levels,\n    model.get_num_degrees_of_freedom, config_var_trans,\n    num_samples=10, coarsest_levels=coarsest_levels,\n    reference_model=true_value)\n_ = plot_convergence_data(convergence_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The left plots depicts the convergence of the estimated integral as $n_1$ is increased for varying values of $n_1$ and vice-versa for the right plot. These plots confirm that the Integrator converges as the expected linear rate. Until the error introduced by fixing the other configuration variables dominates.\n\nWe can also generate similar plots for methods used to solve parameterized partial differential equations. I the following we will assess convergence of a spectral collocation method used to solve the transient advection diffusion equation on a rectangle (see :func:`~pyapprox.benchmarks.setup_multi_index_advection_diffusion_benchmark`).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pyapprox.benchmarks import setup_benchmark\nnp.random.seed(1)\nfinal_time = 0.5\ntime_scenario = {\n    \"final_time\": final_time,\n    # \"butcher_tableau\": \"im_beuler1\",\n    \"butcher_tableau\": \"im_crank2\",\n    \"deltat\": final_time/100,  # will be overwritten\n    \"init_sol_fun\": None,\n    # \"init_sol_fun\": partial(full_fun_axis_1, 0),\n    \"sink\": None  # [50, 0.1, [0.75, 0.75]]\n}\n\nN = 8\nconfig_values = [2*np.arange(1, N+2)+5, 2*np.arange(1, N+2)+5,\n                 final_time/((2**np.arange(1, N+2)+40))]\n# values of kle stdev and mean before exponential is taken\nlog_kle_mean_field = np.log(0.1)\nlog_kle_stdev = 1\nbenchmark = setup_benchmark(\n    \"multi_index_advection_diffusion\", kle_nvars=3, kle_length_scale=1,\n    kle_stdev=log_kle_stdev,\n    time_scenario=time_scenario, config_values=config_values,\n    vel_vec=[0.2, -0.2], source_loc=[0.5, 0.5], source_amp=1,\n    kle_mean_field=log_kle_mean_field)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First plot the evolution of the PDE for a realization of the model inputs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom pyapprox.pde.autopde.mesh import generate_animation\nmodel = benchmark.fun.base_model._model_ensemble.functions[0]\nsample = torch.as_tensor(benchmark.variable.rvs(1)[:, 0])\nmodel._set_random_sample(sample)\ninit_sol = model._get_init_sol(sample)\nsols, times = model._fwd_solver.solve(\n    init_sol, 0, model._final_time,\n    newton_kwargs=model._newton_kwargs, verbosity=0)\nani = generate_animation(\n    model._fwd_solver.physics.mesh, sols, times,\n    filename=None, maxn_frames=100, duration=2)\nimport matplotlib.animation as animation\nani.save('ad-sol.gif', writer=animation.ImageMagickFileWriter(),\n         dpi=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now perform a convgernce study\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation_levels = np.full(3, N)\ncoarsest_levels = np.full(3, 0)\nfinest_levels = np.full(3, N-4)\nif final_time is None:\n    validation_levels = validation_levels[:2]\n    coarsest_levels = coarsest_levels[:2]\nconvergence_data = run_convergence_study(\n    benchmark.fun, benchmark.variable, validation_levels,\n    benchmark.get_num_degrees_of_freedom, benchmark.config_var_trans,\n    num_samples=1, coarsest_levels=coarsest_levels,\n    finest_levels=finest_levels)\n_ = plot_convergence_data(convergence_data, cost_type=\"ndof\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note when because the benchmark fun is run using multiprocessing.Pool\nThe .py script of this tutorial cannot be run with max_eval_concurrency > 1\nvia the shell command using python plot_pde_convergence.py because Pool\nmust be called inside\n\n`if __name__ == \"__main__\":\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}