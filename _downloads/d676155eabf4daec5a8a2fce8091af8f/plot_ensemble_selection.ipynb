{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\rvset{{\\mathcal{Z}}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\text{d}#1}\\newcommand{\\mat}[1]{{\\boldsymbol{\\mathrm{#1}}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Model Ensemble Selection\nFor many applications a large number of model fidelities are available. However, it may not be beneficial to use all the lower-fidelity models because if a lower-fidelity model is poorly correlated with the other models it can increase the covariance of the ACV estimator. In some extreme cases, the covariance can be worse than the variance of a single fidelity MC estimator that solely uses the high-fidelity data. Consequently, in practice it is important to choose the subset of models that produces the smallest estimator covariance.\n\nThe following tutorial shows how to choose the best subset of models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pyapprox import multifidelity\nfrom pyapprox.benchmarks import setup_benchmark\nfrom pyapprox.interface.wrappers import WorkTrackingModel, TimerModel\nfrom pyapprox.util.visualization import mathrm_label\n\nnp.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure the benchmark\nLets configure the benchmark.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time_scenario = {\n    \"final_time\": 1.0,\n    \"butcher_tableau\": \"im_crank2\",\n    \"deltat\": 0.1,  # default will be overwritten\n    \"init_sol_fun\": None,\n    \"sink\": None\n}\n\nnlevels = 2\nconfig_values = [\n    [11, 21, 31],\n    [11, 31],\n    [0.125, 0.0625]]\n\nbenchmark = setup_benchmark(\n    \"multi_index_advection_diffusion\",\n    kle_nvars=3, kle_length_scale=0.5,\n    time_scenario=time_scenario, config_values=config_values)\n\n# Add wraper to compute the time each model takes to run\nfuns = [WorkTrackingModel(\n    TimerModel(fun), base_model=fun) for fun in benchmark.funs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the pilot study\nThe following code runs a pilot study to compute the necessary\npilot quantities needed to predict the variance of any estimator\nof the mean of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "npilot_samples = 20\npilot_samples = benchmark.variable.rvs(npilot_samples)\npilot_values_per_model = [fun(pilot_samples) for fun in funs]\n\nnmodels = len(benchmark.funs)\nstat = multifidelity.multioutput_stats[\"mean\"](1)\nstat.set_pilot_quantities(\n    *stat.compute_pilot_quantities(pilot_values_per_model))\n\n# Extract median run times of each model\nmodel_ids = np.asarray([np.arange(nmodels)])\nmodel_costs = [fun.cost_function()[0] for fun in funs]\n\nax = plt.subplots(1, 1, figsize=(8, 6))[1]\nmultifidelity.plot_model_costs(model_costs, ax=ax)\n\nax = plt.subplots(1, 1, figsize=(16, 12))[1]\n_ = multifidelity.plot_correlation_matrix(\n    multifidelity.get_correlation_from_covariance(stat._cov.numpy()), ax=ax,\n    format_string=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find the best subset of of low-fidelity models\n The following code finds the subset of models that minimizes the variance of a single estimator by iterating over all possible subsets of models and computing the optimal sample allocation and assocated estimator variance. Here we restrict our attention to model subsets that contain at most 4 models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some MFMC estimators will fail because the models\n# do not satisfy its hierarchical condition so set\n# allow_failures=True\nbest_est = multifidelity.get_estimator(\n    \"mfmc\", stat, model_costs, allow_failures=True,\n    max_nmodels=5, save_candidate_estimators=True)\n\ntarget_cost = 1e2\nbest_est.allocate_samples(target_cost)\nprint(\"Predicted variance\",\n      best_est._covariance_from_npartition_samples(\n          best_est._rounded_npartition_samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the variance reduction relative to single-fidelity MC of the best estimator for increasing total number of allowed low fidelity models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Sort candidate estimators into lists with the same numbers of models\nfrom collections import defaultdict\nest_dict = defaultdict(list)\nfor result in best_est._candidate_estimators:\n    if result[0] is None:\n        # skip failures\n        continue\n    est_dict[result[0]._nmodels].append(result)\n\nnmodels_list = np.sort(list(est_dict.keys())).astype(int)\nbest_est_indices = [\n    np.argmin([result[0]._optimized_criteria for result in est_dict[nmodels]])\n    for nmodels in nmodels_list]\nbest_ests = [est_dict[nmodels_list[ii]][best_est_indices[ii]][0]\n             for ii in range(len(nmodels_list))]\nest_labels = [\"{0}\".format(est_dict[nmodels_list[ii]][best_est_indices[ii]][1])\n              for ii in range(len(nmodels_list))]\n\nax = plt.subplots(1, 1, figsize=(8, 6))[1]\n_ = multifidelity.plot_estimator_variance_reductions(\n    best_ests, est_labels, ax)\nax.set_xlabel(mathrm_label(\"Low fidelity models\"))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Find the best estimator\nWe can also find the best estimator from a list of estimator types while still determining the best model subset. This code chooses the best estimator from two possible parameterized ACV estimator classes. Specifically it chooses from all possible generalized recursive difference (GRD) estimators and genearlized multifidelity estimators that use at most 3 models and have a maximum tree depth of 3.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_est = multifidelity.get_estimator(\n    [\"grd\", \"gmf\"], stat, model_costs, allow_failures=True,\n    max_nmodels=3, tree_depth=3,\n    save_candidate_estimators=True)\n\ntarget_cost = 1e2\nbest_est.allocate_samples(target_cost)\nprint(\"Predicted variance\",\n      best_est._covariance_from_npartition_samples(\n          best_est._rounded_npartition_samples))\n\n# Sort candidate estimators into lists with the same estimator type\nfrom collections import defaultdict\nest_dict = defaultdict(list)\nfor result in best_est._candidate_estimators:\n    if result[0] is None:\n        # skip failures\n        continue\n    est_dict[result[0].__class__.__name__].append(result)\n\n\nest_name_list = list(est_dict.keys())\nest_name_list.sort()\nbest_est_indices = [\n    np.argmin([result[0]._optimized_criteria for result in est_dict[name]])\n    for name in est_name_list]\nbest_ests = [est_dict[est_name_list[ii]][best_est_indices[ii]][0]\n             for ii in range(len(est_name_list))]\nest_labels = [\n    \"{0}({1}, {2})\".format(\n        est_name_list[ii],\n        est_dict[est_name_list[ii]][best_est_indices[ii]][1],\n        est_dict[est_name_list[ii]][best_est_indices[ii]][0]._recursion_index)\n    for ii in range(len(est_name_list))]\n\nax = plt.subplots(1, 1, figsize=(8, 6))[1]\n_ = multifidelity.plot_estimator_variance_reductions(\n    best_ests, est_labels, ax)\nax.set_xlabel(mathrm_label(\"Estimator types\"))\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}