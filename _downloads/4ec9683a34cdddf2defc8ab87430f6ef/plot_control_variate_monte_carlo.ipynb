{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Control Variate Monte Carlo\nThis tutorial describes how to implement and deploy control variate Monte Carlo sampling to compute the expectations of the output of a high-fidelity model using a lower-fidelity model with a known mean. The information presented here builds upon the tutorial `sphx_glr_auto_tutorials_multi_fidelity_plot_monte_carlo.py`.\n\nLet us introduce a model $Q_\\V{\\kappa}$ with known mean $\\mu_{\\V{\\kappa}}$. We can use this model to estimate the mean of $Q_{\\V{\\alpha}}$ via [LMWOR1982]_\n\n\\begin{align}Q_{\\V{\\alpha},N}^{\\text{CV}} &= Q_{\\V{\\alpha},N} + \\eta \\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}} \\right) \\\\\\end{align}\n\nHere $\\eta$ is a free parameter which can be optimized to the reduce the variance of this so called control variate estimator, which is given by\n\n\\begin{align}\\var{Q_{\\V{\\alpha},N}^{\\text{CV}}} &= \\var{Q_{\\V{\\alpha},N} + \\eta \\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}\\\\\n   &=\\var{Q_{\\V{\\alpha},N}} + \\eta^2\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}+ 2\\eta\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}\\\\\n   &=\\var{Q_{\\V{\\alpha},N}}\\left(1+\\eta^2\\frac{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{Q_{\\V{\\alpha},N}}}+ 2\\eta\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{Q_{\\V{\\alpha},N}}}\\right).\\end{align}\n\nThe first line follows from the variance of sums of random variables.\n\nWe can measure the change in MSE bys using the control variate estimator, by looking at the ratio of the CVMC and MC estimator variances. The variance reduction ratio is\n\n\\begin{align}\\gamma=\\frac{\\var{Q_{\\V{\\alpha},N}^{\\text{CV}}}}{\\var{Q_{\\V{\\alpha},N}}}=\\left(1+\\eta^2\\frac{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{Q_{\\V{\\alpha},N}}}+ 2\\eta\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{Q_{\\V{\\alpha},N}}}\\right)\\end{align}\n\nThe variance reduction can be minimized by setting its gradient to zero and solving for $\\eta$, i.e.\n\n\\begin{align}\\frac{d}{d\\eta}\\gamma  &= 2\\eta\\frac{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{Q_{\\V{\\alpha},N}}}+ 2\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{Q_{\\V{\\alpha},N}}} = 0\\\\\n  &\\implies \\eta\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}+ \\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)} = 0\\\\\n  &\\implies \\eta=-\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}\\\\\n  &=-\\frac{\\covar{Q_{\\V{\\alpha},N}}{Q_{\\V{\\kappa},N}}}{\\var{Q_{\\V{\\kappa},N}}}\\end{align}\n\nWith this choice\n\n\\begin{align}\\gamma &= 1+\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}^2}{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}^2}\\frac{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{Q_{\\V{\\alpha},N}}}-2\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}}{\\var{Q_{\\V{\\alpha},N}}}\\\\\n  &= 1+\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}^2}{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}\\var{Q_{\\V{\\alpha},N}}}-2\\frac{\\covar{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}^2}{\\var{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}\\var{Q_{\\V{\\alpha},N}}}\\\\\n   &= 1-\\corr{Q_{\\V{\\alpha},N}}{\\left( Q_{\\V{\\kappa},N} - \\mu_{\\V{\\kappa}}\\right)}^2\\\\\n   &= 1-\\corr{Q_{\\V{\\alpha},N}}{Q_{\\V{\\kappa},N}}^2\\end{align}\n\n\nThus if a two highly correlated models (one with a known mean) are available then we can drastically reduce the MSE of our estimate of the unknown mean.\n\nAgain consider the tunable model ensemble. The correlation between the models $f_0$ and $f_1$ can be tuned by varying $\\theta_1$. For a given choice of theta lets compute a single relization of the CVMC estimate of $\\mean{f_0}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First let us setup the problem and compute a single estimate using CVMC\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom pyapprox.benchmarks import setup_benchmark\n\nnp.random.seed(1)\nshifts = [.1, .2]\nbenchmark = setup_benchmark(\n    \"tunable_model_ensemble\", theta1=np.pi/2*.95, shifts=shifts)\nmodel = benchmark.fun\nprint(benchmark.variable)\n\nnsamples = int(1e2)\nsamples = benchmark.variable.rvs(nsamples)\nvalues0 = model.m0(samples)\nvalues1 = model.m1(samples)\ncov = model.get_covariance_matrix()\neta = -cov[0, 1]/cov[0, 0]\n#cov_mc = np.cov(values0,values1)\n#eta_mc = -cov_mc[0,1]/cov_mc[0,0]\nexact_integral_f0, exact_integral_f1 = 0, shifts[0]\ncv_mean = values0.mean()+eta*(values1.mean()-exact_integral_f1)\nprint('MC difference squared =', (values0.mean()-exact_integral_f0)**2)\nprint('CVMC difference squared =', (cv_mean-exact_integral_f0)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets look at the statistical properties of the CVMC estimator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntrials = 1000\nmeans = np.empty((ntrials, 2))\nfor ii in range(ntrials):\n    samples = benchmark.variable.rvs(nsamples)\n    values0 = model.m0(samples)\n    values1 = model.m1(samples)\n    means[ii, 0] = values0.mean()\n    means[ii, 1] = values0.mean()+eta*(values1.mean()-exact_integral_f1)\n\nprint(\"Theoretical variance reduction\",\n      1-cov[0, 1]**2/(cov[0, 0]*cov[1, 1]))\nprint(\"Achieved variance reduction\",\n      means[:, 1].var(axis=0)/means[:, 0].var(axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following plot shows that unlike the `Monte Carlo estimator <estimator-histogram>`. $\\mean{f_1}$ the CVMC estimator is unbiased and has a smaller variance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots()\ntextstr = '\\n'.join(\n    [r'$E[Q_{0,N}]=\\mathrm{%.2e}$' % means[:, 0].mean(),\n     r'$V[Q_{0,N}]=\\mathrm{%.2e}$' % means[:, 0].var(),\n     r'$E[Q_{0,N}^\\mathrm{CV}]=\\mathrm{%.2e}$' % means[:, 1].mean(),\n     r'$V[Q_{0,N}^\\mathrm{CV}]=\\mathrm{%.2e}$' % means[:, 1].var()])\nax.hist(means[:, 0], bins=ntrials//100, density=True, alpha=0.5,\n        label=r'$Q_{0,N}$')\nax.hist(means[:, 1], bins=ntrials//100, density=True, alpha=0.5,\n        label=r'$Q_{0,N}^\\mathrm{CV}$')\nax.axvline(x=0,c='k',label=r'$E[Q_0]$')\nprops = {'boxstyle': 'round', 'facecolor': 'white', 'alpha': 1}\nax.text(0.6, 0.75, textstr,transform=ax.transAxes, bbox=props)\n_ = ax.legend(loc='upper left')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change ``eta`` to ``eta_mc`` to see how the variance reduction changes when the covariance between models is approximated\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [LMWOR1982] [S.S. Lavenberg, T.L. Moeller, P.D. Welch, Statistical results on control variables with application to queueing network simulation, Oper. Res., 30, 45, 182-202, 1982.](https://doi.org/10.1287/opre.30.1.182)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}