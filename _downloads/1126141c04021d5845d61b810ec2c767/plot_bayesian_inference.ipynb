{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Bayesian Inference\nThis tutorial describes how to use Bayesian inference condition estimates of uncertainty on observational data.\n\n## Background\nWhen observational data are available, that data should be used to inform prior assumptions of model uncertainties. This so-called inverse problem that seeks to estimate uncertain parameters from measurements or observations is usually ill-posed. Many different realizations of parameter values may be consistent with the data. The lack of a unique solution can be due to the non-convexity of the parameter-to-QoI map, lack of data, and model structure and measurement errors.\n\nDeterministic model calibration is an inverse problem that seeks to find a single parameter set that minimizes the misfit between the measurements and model predictions. A unique solution is found by simultaneously minimising the misfit and a regularization term which penalises certain characteristics of the model parameters.\n\nIn the presence of uncertainty we typically do not want a single optimal solution, but rather a probabilistic description of the extent to which different realizations of parameters are consistent with the observations.\nBayesian inference [KAIPO2005]_ can be used to define a posterior density for the model parameters $\\rv$ given\nobservational data $\\V{y}=(y_1,\\ldots,y_{n_y})$:\n\n### Bayes Rule\nGiven a model $\\mathcal{M}(\\rv)$ parameterized by a set of parameters $\\rv$, our goal is to infer the parameter $\\rv$ from data $d$.\n\nBayes Theorem describes the probability of the parameters $\\rv$ conditioned on the data $d$ is proportional to the conditional probability of observing the data given the parameters multiplied by the probability of observing the data, that is\n\n\\begin{align}\\pi (\\rv\\mid d)&=\\frac{\\pi (d\\mid \\rv)\\,\\pi (\\rv)}{\\pi(d)}\\\\\n  &=\\frac{\\pi (d\\mid \\rv)\\,\\pi (\\rv)}{\\int_{\\mathbb{R}^d} \\pi (d\\mid \\rv)\\,\\pi (\\rv)\\,d\\rv}\\end{align}\n\nThe density $\\pi (\\rv\\mid d)$ is referred to as the posterior density.\n\n\\begin{align}\\pi_{\\text{post}}(\\rv)=\\pi_\\text(\\rv\\mid\\V{y})=\\frac{\\pi(\\V{y}|\\rv)\\pi(\\rv)}{\\int_{\\rvdom}\n   \\pi(\\V{y}|\\rv)\\pi(\\rv)d\\rv}\\end{align}\n\n### Prior\nTo find the posterior density we must first quantify our prior belief of the possible values\nof the parameter that can give the data. We do this by specifying the probability of \nobserving the parameter independently of observing the data. \n\nHere we specify the prior distribution to be Normally distributed, e.g\n\n\\begin{align}\\pi\\sim N(m_\\text{prior},\\Sigma_\\text{prior})\\end{align}\n\n### Likelihood\nNext we must specify the likelihood $\\pi(d\\mid \\rv)$ of observing the data given a realizations of the parameter $\\rv$\nThe likelihood answers the question: what is the distribution of the data assuming that $\\rv$ are the exact parameters?\n\nThe form of the likelihood is derived from an assumed relationship between the model and the\ndata.\n\nIt is often assumed that\n\n.. math :: d=\\mathcal{M}(\\rv)+\\eta\n\nwhere $\\eta\\sim N(0,\\Sigma_\\text{noise})$ is normally distributed noise with zero mean and covariance $\\Sigma_\\text{noise}$.\n\nIn this case the likelihood is\n\n\\begin{align}\\pi(d|\\rv)=\\frac{1}{\\sqrt {(2\\pi )^{k}|{\\boldsymbol {\\Sigma_\\text{noise} }}}|}\\exp \\left(-{\\frac {1}{2}}(\\mathcal{M}(\\rv)-d)^{\\mathrm {T} }{\\boldsymbol {\\Sigma_\\text{noise} }}^{-1}(\\mathcal{M}(\\rv)-d)\\right)\\end{align}\n\nwhere $|\\Sigma_\\text{noise}|=\\det \\Sigma_\\text{noise}$ is the determinant of $\\Sigma_\\text{noise}$\n\n## Exact Linear-Gaussian Inference\nIn the following we will generate data at a truth parameter $\\rv_\\text{truth}$ and use Bayesian inference\nto estimate the probability of any model parameter $\\rv$ conditioned on the observations we generated.\nFirstly assume  $\\mathcal{M}$ is a linear model, i.e.\n\n\\begin{align}\\mathcal{M}(\\rv)=A\\rv+b,\\end{align}\n\nand as above assume that\n\n\\begin{align}d=\\mathcal{M}(\\rv)+\\eta\\end{align}\n\nNow define the prior probability of the parameters to be\n\n\\begin{align}\\pi(\\rv)\\sim N(m_\\text{prior},\\Sigma_\\text{prior})\\end{align}\n\nUnder these assumptions, the marginal density (integrating over the prior of $\\rv$) of the data and parameters is\n\n\\begin{align}\\pi(d)\\sim N(m_\\text{noise}+Am_\\text{prior},\\Sigma_\\text{noise}+ A\\Sigma_\\text{prior} A^T)=N(m_\\text{data},\\Sigma_\\text{data})\\end{align}\n\nand the joint density of the parameters and data is\n\n\\begin{align}\\pi(\\rv,d)\\sim N(m_\\text{joint},\\Sigma_\\text{joint})\\end{align}\n\nwhere\n\n\\begin{align}\\boldsymbol m_\\text{joint}=\\begin{bmatrix} \\boldsymbol m_\\text{prior} \\\\ \\boldsymbol m_\\text{data}\\end{bmatrix},\\quad\n   \\boldsymbol \\Sigma_\\text{joint}=\\begin{bmatrix} \\boldsymbol\\Sigma_\\text{prior} & \\boldsymbol\\Sigma_\\text{prior,data} \\\\ \\boldsymbol\\Sigma_\\text{prior,data} & \\boldsymbol\\Sigma_\\text{data}\\end{bmatrix}\\end{align}\n\nand\n \n\\begin{align}\\Sigma_\\text{prior,data}=A\\Sigma_\\text{prior}\\end{align}\n\nis the covariance between the parameters and data.\n\nNow let us setup this problem in Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pyapprox.bayes.markov_chain_monte_carlo import (\n    run_bayesian_inference_gaussian_error_model, PYMC3LogLikeWrapper\n)\nfrom pyapprox.bayes.tests.test_markov_chain_monte_carlo import (\n    ExponentialQuarticLogLikelihoodModel)\nfrom pyapprox.variables.density import NormalDensity\nfrom scipy import stats\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport matplotlib as mpl\nnp.random.seed(1)\n\nA = np.array([[.5]])\nb = 0.0\n# define the prior\nprior_covariance = np.atleast_2d(1.0)\nprior = NormalDensity(0., prior_covariance)\n# define the noise\nnoise_covariance = np.atleast_2d(0.1)\nnoise = NormalDensity(0, noise_covariance)\n# compute the covariance between the prior and data\nC_12 = np.dot(A, prior_covariance)\n# define the data marginal distribution\ndata_covariance = noise_covariance+np.dot(C_12, A.T)\ndata = NormalDensity(np.dot(A, prior.mean)+b,\n                         data_covariance)\n# define the covariance of the joint distribution of the prior and data\n\n\ndef form_normal_joint_covariance(C_11, C_22, C_12):\n    num_vars1 = C_11.shape[0]\n    num_vars2 = C_22.shape[0]\n    joint_covariance = np.empty((num_vars1+num_vars2, num_vars1+num_vars2))\n    joint_covariance[:num_vars1, :num_vars1] = C_11\n    joint_covariance[num_vars1:, num_vars1:] = C_22\n    joint_covariance[:num_vars1, num_vars1:] = C_12\n    joint_covariance[num_vars1:, :num_vars1] = C_12\n    return joint_covariance\n\n\njoint_mean = np.hstack((prior.mean, data.mean))\njoint_covariance = form_normal_joint_covariance(\n    prior_covariance, data_covariance, C_12)\njoint = NormalDensity(joint_mean, joint_covariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can plot the joint distribution and some samples from that distribution\nand print the sample covariance of the joint distribution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_samples = 10000\ntheta_samples = prior.generate_samples(num_samples)\nnoise_samples = noise.generate_samples(num_samples)\ndata_samples = np.dot(A, theta_samples) + b + noise_samples\nplot_limits = [theta_samples.min(), theta_samples.max(),\n               data_samples.min(), data_samples.max()]\nfig, ax = plt.subplots(1, 1)\njoint.plot_density(plot_limits=plot_limits, ax=ax)\n_ = plt.plot(theta_samples[0, :100], data_samples[0, :100], 'o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conditional probability of multivariate Gaussians\nFor multivariate Gaussians the dsitribution of $x$ conditional on observing the data $d^\\star=d_\\text{truth}+\\eta^\\star$, $\\pi(x\\mid d=d^\\star)\\sim N(m_\\text{post},\\Sigma_\\text{post})$ is a multivariate Gaussian with mean and covariance\n\n\\begin{align}\\boldsymbol{m}_\\text{post}&=\\boldsymbol m_\\text{prior} + \\boldsymbol\\Sigma_\\text{prior,data} \\boldsymbol\\Sigma_{data}^{-1}\\left( \\mathbf{d}^\\star - \\boldsymbol m_\\text{data}\\right),\\\\\n  \\boldsymbol \\Sigma_\\text{post}&=\\boldsymbol \\Sigma_\\text{prior}-\\boldsymbol \\Sigma_\\text{prior,data}\\boldsymbol\\Sigma_\\text{data}^{-1}\\boldsymbol\\Sigma_\\text{data,prior}^T.\\end{align}\n\nwhere $\\eta^\\star$ is a random sample from the noise variable $\\eta$. In the case of one parameter and one QoI we have\n\n\\begin{align}\\pi(x\\mid d=d^\\star) \\sim\\ N\\left(m_\\text{prior}+\\frac{\\sigma_\\text{prior}}{\\sigma_\\text{data}}\\rho( d^\\star - m_\\text{data}),\\, (1-\\rho^2)\\sigma_\\text{prior}^2\\right).\\end{align}\n\nwhere the correlation coefficient between the parameter and data is\n\n.. math: \\rho=\\frac{\\Sigma_\\text{prior,data}}{\\sigma_\\text{prior}\\sigma_\\text{data}}\n\nLets use this formula to update the prior when one data $d^\\star$ becomes available.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def condition_normal_on_data(mean, covariance, fixed_indices, values):\n    indices = set(np.arange(mean.shape[0]))\n    ind_remain = list(indices.difference(set(fixed_indices)))\n    new_mean = mean[ind_remain]\n    diff = values - mean[fixed_indices]\n    sigma_11 = np.array(covariance[ind_remain, ind_remain], ndmin=2)\n    sigma_12 = np.array(covariance[ind_remain, fixed_indices], ndmin=2)\n    sigma_22 = np.array(covariance[fixed_indices, fixed_indices], ndmin=2)\n    update = np.dot(sigma_12, np.linalg.solve(sigma_22, diff)).flatten()\n    new_mean = new_mean + update\n    new_cov = sigma_11-np.dot(sigma_12, np.linalg.solve(sigma_22, sigma_12.T))\n    return new_mean, new_cov\n\n\nx_truth = 0.2\ndata_obs = np.dot(A, x_truth)+b+noise.generate_samples(1)\n\nnew_mean, new_cov = condition_normal_on_data(\n    joint_mean, joint_covariance, [1], data_obs)\nposterior = NormalDensity(new_mean, new_cov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets plot the prior and posterior of the parameters as well as the joint distribution and the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f, axs = plt.subplots(1, 2, figsize=(16, 6))\nprior.plot_density(label='prior', ax=axs[0])\naxs[0].axvline(x=x_truth, lw=3, label=r'$x_\\text{truth}$')\nposterior.plot_density(plot_limits=prior.plot_limits,\n                       ls='-', label='posterior', ax=axs[0])\ncolorbar_lims = None  # [0,1.5]\njoint.plot_density(ax=axs[1], colorbar_lims=colorbar_lims)\naxhline = axs[1].axhline(y=data_obs, color='k')\naxplot = axs[1].plot(x_truth, data_obs, 'ok', ms=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets also plot the joint distribution and marginals in a 3d plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def data_obs_limit_state(samples, vals, data_obs):\n    vals = np.ones((samples.shape[1]), float)\n    I = np.where(samples[1, :] <= data_obs[0, 0])[0]\n    return I, 0.\n\n\nfrom pyapprox.util.visualization import (\n    get_meshgrid_function_data, create_3d_axis, plot_surface, plot_contours)\nnum_pts_1d = 100\nlimit_state = partial(data_obs_limit_state, data_obs=data_obs)\nX, Y, Z = get_meshgrid_function_data(\n    joint.pdf, joint.plot_limits, num_pts_1d, qoi=0)\noffset = -(Z.max()-Z.min())\nsamples = np.array([[x_truth, data_obs[0, 0], offset]]).T\nax = create_3d_axis()\nplot_surface(X, Y, Z, ax, axis_labels=[r'$x_1$', r'$d$', ''],\n             limit_state=limit_state,\n             alpha=0.3, cmap=mpl.cm.coolwarm, zorder=3, plot_axes=False)\nnum_contour_levels = 30\ncset = plot_contours(X, Y, Z, ax, num_contour_levels=num_contour_levels,\n                     offset=offset, cmap=mpl.cm.coolwarm, zorder=-1)\nZ_prior = prior.pdf(X.flatten()).reshape(X.shape[0], X.shape[1])\nax.contour(X, Y, Z_prior, zdir='y', offset=Y.max(), cmap=mpl.cm.coolwarm)\nZ_data = data.pdf(Y.flatten()).reshape(Y.shape[0], Y.shape[1])\nax.contour(X, Y, Z_data, zdir='x', offset=X.min(), cmap=mpl.cm.coolwarm)\nax.set_zlim(Z.min()+offset, max(Z_prior.max(), Z_data.max()))\nx = np.linspace(X.min(), X.max(), num_pts_1d)\ny = data_obs[0, 0]*np.ones(num_pts_1d)\nz = offset*np.ones(num_pts_1d)\nax.plot(x, y, z, zorder=100, color='k')\n_ = ax.plot([x_truth], [data_obs[0, 0]], [offset],\n            zorder=100, color='k', marker='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets assume another piece of observational data becomes available we can use the posterior as a new prior.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_obs = 5\nposteriors = [None]*num_obs\nposteriors[0] = posterior\nfor ii in range(1, num_obs):\n    new_prior = posteriors[ii-1]\n    data_obs = np.dot(A, x_truth)+b+noise.generate_samples(1)\n    C_12 = np.dot(A, new_prior.covariance)\n    new_joint_covariance = form_normal_joint_covariance(\n        new_prior.covariance, data.covariance, C_12)\n    new_joint = NormalDensity(np.hstack((new_prior.mean, data.mean)),\n                                  new_joint_covariance)\n    new_mean, new_cov = condition_normal_on_data(\n        new_joint.mean, new_joint.covariance,\n        np.arange(new_prior.num_vars(), new_prior.num_vars()+data.num_vars()),\n        data_obs)\n    posteriors[ii] = NormalDensity(new_mean, new_cov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now lets again plot the joint density before the last data was added and final posterior and the intermediate priors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f, axs = plt.subplots(1, 2, figsize=(16, 6))\nprior.plot_density(label='prior', ax=axs[0])\naxs[0].axvline(x=x_truth, lw=3, label=r'$x_\\text{truth}$')\nfor ii in range(num_obs):\n    posteriors[ii].plot_density(\n        plot_limits=prior.plot_limits, ls='-', label='posterior', ax=axs[0])\n\ncolorbar_lims = None\nnew_joint.plot_density(ax=axs[1], colorbar_lims=colorbar_lims,\n                       plot_limits=joint.plot_limits)\naxhline = axs[1].axhline(y=data_obs, color='k')\naxplot = axs[1].plot(x_truth, data_obs, 'ok', ms=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see the variance of the joint density decreases as more data is added. The posterior variance also decreases and the posterior will converge to a Dirac-delta function as the number of observations tends to infinity. Currently the mean of the posterior is not near the true parameter value (the horizontal line). Try increasing ``num_obs1`` to see what happens.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inexact Inference using Markov Chain Monte Carlo\n\nWhen using non-linear or non-Gaussian priors, a functional representation of the posterior distribution $\\pi_\\text{post}$ cannot be computed analytically. Instead the the posterior is characterized by samples drawn from the posterior using Markov-chain Monte Carlo (MCMC) sampling methods.\n\nLets consider non-linear model with two uncertain parameters with independent uniform priors on [-2,2] and the negative log likelihood function\n\n\\begin{align}-\\log\\left(\\pi(d\\mid\\rv)\\right)=\\frac{1}{10}\\rv_1^4 + \\frac{1}{2}(2\\rv_2-\\rv_1^2)^2\\end{align}\n\nWe can sample the posterior using Sequential Markov Chain Monte Carlo using the following code.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n\nfrom pyapprox.variables.joint import IndependentMarginalsVariable\nunivariate_variables = [stats.uniform(-2, 4), stats.uniform(-2, 4)]\nplot_range = np.asarray([-1, 1, -1, 1])*2\nvariables = IndependentMarginalsVariable(univariate_variables)\n\nloglike = ExponentialQuarticLogLikelihoodModel()\nloglike = PYMC3LogLikeWrapper(loglike)\n\n# number of draws from the distribution\nndraws = 500\n# number of \"burn-in points\" (which we'll discard)\nnburn = min(1000, int(ndraws*0.1))\n# number of parallel chains\nnjobs = 1\nsamples, effective_sample_size, map_sample = \\\n    run_bayesian_inference_gaussian_error_model(\n        loglike, variables, ndraws, nburn, njobs,\n        algorithm='smc', get_map=True, print_summary=True)\n\nprint('MAP sample', map_sample.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The NUTS sampler offerred by PyMC3 can also be used by specifying `algorithm='nuts'`. This sampler requires gradients of the likelihood function which if not provided will be computed using finite difference.\n\nLets plot the posterior distribution and the MCMC samples. First we must compute the evidence\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pyapprox.surrogates.orthopoly.quadrature import gauss_jacobi_pts_wts_1D\nfrom pyapprox.surrogates.interp.tensorprod import (\n    get_tensor_product_quadrature_rule\n)\n\n\ndef unnormalized_posterior(x):\n    vals = np.exp(loglike.loglike(x))\n    rvs = variables.marginals()\n    for ii in range(variables.num_vars()):\n        vals[:, 0] *= rvs[ii].pdf(x[ii, :])\n    return vals\n\n\ndef univariate_quadrature_rule(n):\n    x, w = gauss_jacobi_pts_wts_1D(n, 0, 0)\n    x *= 2\n    return x, w\n\n\nx, w = get_tensor_product_quadrature_rule(\n    100, variables.num_vars(), univariate_quadrature_rule)\nevidence = unnormalized_posterior(x)[:, 0].dot(w)\nprint('evidence', evidence)\n\nplt.figure()\nX, Y, Z = get_meshgrid_function_data(\n    lambda x: unnormalized_posterior(x)/evidence, plot_range, 50)\nplt.contourf(\n    X, Y, Z, levels=np.linspace(Z.min(), Z.max(), 30),\n    cmap=matplotlib.cm.coolwarm)\nplt.plot(samples[0, :], samples[1, :], 'ko')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets compute the mean of the posterior using a highly accurate quadrature rule and compars this to the mean estimated using MCMC samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exact_mean = ((x*unnormalized_posterior(x)[:, 0]).dot(w)/evidence)\nprint('mcmc mean', samples.mean(axis=1))\nprint('exact mean', exact_mean.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n.. [KAIPO2005] `J. Kaipio and E. Somersalo. Statistical and Computational Inverse Problems. 2005 <https://link.springer.com/book/10.1007/b138659>`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}