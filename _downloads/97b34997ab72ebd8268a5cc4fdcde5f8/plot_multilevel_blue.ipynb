{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multilevel Best Linear Unbiased estimators (MLBLUE)\nThis tutorial introduces Multilevel Best Linear Unbiased estimators (MLBLUE) and compares its characteristics and performance with the previously introduced\nmulti-fidelity estimators.\n\n.. list-table::\n\n   * - .. _MLBLUE-sample-allocation:\n\n       .. figure:: ../../figures/MLBLUE-sets.png\n          :width: 50%\n          :align: center\n\n          MLBLUE sample allocations.\n\nMLBLUE assumes that each model $f_1,\\ldots,f_M$ is linearly dependent on the means of each model $q=[Q_1,\\ldots,Q_M]^\\top$, where $M$ is the number of low-fidelity models. Specifically, MLBLUE assumes that\n\n\\begin{align}Y=Hq+\\epsilon,\\end{align}\n\nwhere $Y=[Y_1,\\ldots,Y_{K}]$ are evaluations of models within all $K=2^{M}-1$ nonempty subset $S^k\\in2^{\\{1,\\ldots,M\\}}\\setminus\\emptyset$ , that is $Y_k=[f_{S_k, 1}^{(1)},\\ldots,f_{S_k, |S_k|}^{(1)},\\ldots,f_{S_k, 1}^{(m_k)},\\ldots,f_{S_k, |S_k|}^{(m_k)}]$. An example of such sets is shown in the figure above where $m_k$ denotes the number of samples of each model in the subset $S_k$. \n\nHere $H$ is a restriction opeator that specifies the means that produce the data in each set, that is\n\n\\begin{align}H=[R_1^\\top,\\ldots,R_K^\\top]^\\top, \\qquad R_kq = [Q_{S_k,1},\\ldots,Q_{S_k,|S_k|}]^\\top\\end{align}\n\nMLBLUE then finds the means of all models bys solving the generalized least squares problem\n\n\\begin{align}\\min_{q\\in\\reals^M} \\lVert Y-Hq\\rVert^2_{{\\covar{\\epsilon}{\\epsilon}}^{-1}}\\end{align}\n\nHere ${\\covar{\\epsilon}{\\epsilon}$ is a matrix that is dependent on the covariance between the models and is given by\n\n\\begin{align}\\covar{\\epsilon}{\\epsilon}&=\\mathrm{BlockDiag}(G_1,\\ldots, G_K),\\quad G_k = \\mathrm{BlockDiag}((C_k)_{i=1}^{m_k}), \\\\ C_k &= \\covar{Y-H_kq}{Y-H_kq}.\\end{align}\n\nThe figure below gives an example of the construction of the least squares system when only three subsets are active, $S_2, S_3, S_5$; one, one and two samples of the models in each subset are taken, respectively. $S_2$ only contributes on equation because it consists one model that is only sampled once. $S_3$ contributes two equations, because it consists of two models sampled once each. Finally, $S_5$ contributes four equations because it consists of two models sampled twice.\n\n.. list-table::\n\n   * - .. _MLBLUE-sample-example:\n\n       .. figure:: ../../figures/MLBLUE_sample_example.png\n          :width: 100%\n          :align: center\n\n          Example of subset data construction.\n\n\nThe figure below depicts the structure of the $\\covar{\\epsilon}{\\epsilon}$ for the same example, where $\\sigma_{ij}^2$ denotes the covariance between the models $f_i,f_j$ and must be computed from a pilot study.\n\n.. list-table::\n\n   * - .. _MLBLUE-covariance-example:\n\n       .. figure:: ../../figures/MLBLUE_covariance_example.png\n          :width: 100%\n          :align: center\n\n          Example of the covariance structure.\n\nThe solution to the generalized least-squares problem can be found by solving the sustem of linear equations\n\n\\begin{align}\\Psi q^B=y,\\end{align}\n\nwhere q^B denotes the MLBLUE estimate of the model means $q$ and\n\n\\begin{align}\\Psi = \\sum_{k=1}^K m_k R_k^\\top C_k^{-1} R_k, \\qquad y = \\sum_{k=1}^K R^\\top_K C_k^{-1} \\sum_{i=1}^{m_k} Y_{k}^{(i)}\\end{align}\n\nThe vector $q^B$ is an estimate of all model means, however one is often only interested in a linear combination of means, i.e.\n\n\\begin{align}q^B_\\beta = \\beta^\\top q^B.\\end{align}\n\nFor example, $\\beta=[1, 0, \\ldots, 0]^\\top$ can be used to estimate only the high-fidelity mean.\n\nGiven $\\beta$ the variance of the MLBLU estimator is\n\n\\begin{align}\\var{Q^B_\\beta}=\\beta^\\top\\Psi^{-1}\\beta\\end{align}\n\nThe following code compares MLBLUE to other multif-fidelity esimators when the number of high-fidelity samples is fixed and the number of low-fidelity samples increases. This example shows that unlike MLMC and MFMC, MLBLUE like ACV obtains the optimal variance reduction obtained by control variate MC, using known means,  as the number of low-fidelity samples becomes very large.\n\nFirst setup the polynomial benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom functools import partial\nfrom pyapprox.util.configure_plots import plt, mathrm_labels, mathrm_label\nfrom pyapprox import multifidelity\nfrom pyapprox.multifidelity.control_variate_monte_carlo import (\n    get_control_variate_rsquared)\nfrom pyapprox.benchmarks import setup_benchmark\n\nplt.figure()\nbenchmark = setup_benchmark(\"polynomial_ensemble\")\npoly_model = benchmark.fun\ncov = poly_model.get_covariance_matrix()\nmodel_costs = np.asarray([10**-ii for ii in range(cov.shape[0])])\nnhf_samples = 10\nnsample_ratios_base = np.array([2, 4, 8, 16])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, plot the variance reduction of the optimal control variates using known low-fidelity means.\n\nSecond, plot the variance reduction of multi-fidelity estimators that do not assume known low-fidelity means. The code below repeatedly doubles the number of low-fidelity samples according to the initial allocation defined by nsample_ratios_base=[2,4,8,16].\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# plot optimal control variate variance reduction\ncv_labels = mathrm_labels([\"OCV-1\", \"OCV-2\", \"OCV-4\"])\ncv_rsquared_funcs = [\n    lambda cov: get_control_variate_rsquared(cov[:2, :2]),\n    lambda cov: get_control_variate_rsquared(cov[:3, :3]),\n    lambda cov: get_control_variate_rsquared(cov)]\ncv_gammas = [1-f(cov) for f in cv_rsquared_funcs]\nxloc = -.35\nfor ii in range(len(cv_gammas)):\n    plt.axhline(y=cv_gammas[ii], linestyle='--', c='k')\n    plt.text(xloc, cv_gammas[ii]*1.1, cv_labels[ii], fontsize=16)\nplt.axhline(y=1, linestyle='--', c='k')\nplt.text(xloc, 1, mathrm_label(\"MC\"), fontsize=16)\n\n# plot multi-fidelity estimator variance reduction\nest_labels = mathrm_labels([\"MLMC\", \"MFMC\", \"ACVMF\", \"ACVGMFB\", \"MLBLUE\"])\nestimator_types = [\"mlmc\", \"mfmc\", \"acvmf\", \"acvgmfb\", \"mlblue\"]\nestimators = [\n    multifidelity.get_estimator(t, cov, model_costs, poly_model.variable)\n    for t in estimator_types]\n# acvgmfb and mlblue require nhf_samples so create wrappers of that does not\nestimators[-2]._get_rsquared = partial(\n    estimators[-2]._get_rsquared_from_nhf_samples, nhf_samples)\nestimators[-1]._get_rsquared = partial(\n    estimators[-1]._get_rsquared_from_nhf_samples, nhf_samples)\nnplot_points = 20\nacv_gammas = np.empty((nplot_points, len(estimators)))\nfor ii in range(nplot_points):\n    nsample_ratios = np.array([r*(2**ii) for r in nsample_ratios_base])\n    acv_gammas[ii, :] = [1-est._get_rsquared(cov, nsample_ratios)\n                         for est in estimators]\nfor ii in range(len(est_labels)):\n    plt.semilogy(np.arange(nplot_points), acv_gammas[:, ii],\n                 label=est_labels[ii])\nplt.legend()\nplt.xlabel(r'$\\log_2(r_i)-i$')\n_ = plt.ylabel(mathrm_label('Variance reduction ratio ')+ r'$\\gamma$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimal sample allocation\nIn the following we show how to optimize the sample allocation of MLBLUE and compare the optimized estimator to other alternatives.\n\nThe optimal sample allocation is obtained by solving\n\n\\begin{align}\\min_{m\\in\\mathbb{N}_0^K}\\var{Q^B_\\beta(m)}\\quad\\text{such that}\\quad\\sum_{k=1}^K m_k \\sum_{j=1}^{|S_k|} W_j \\le W_{\\max},\\end{align}\n\nwhere $W_j$ denotes the cost of evaluating the jth model and $W_{\\max}$ is the total budget.\n\n We will again use the polynomail model and to be consistent with previous tutoriuals, we will only use the first 4 models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cov = cov[:4, :4]\nmodel_costs = model_costs[:4]\n\n# Add MC estimator to comparison\nestimator_types = [\"mc\", \"mlmc\", \"mfmc\", \"acvmf\", \"acvgmfb\", \"mlblue\"]\nestimators = [\n    multifidelity.get_estimator(t, cov, model_costs, benchmark.variable)\n    for t in estimator_types]\nest_labels = mathrm_labels(\n    [\"MC\", \"MLMC\", \"MFMC\", \"ACVMF\", \"ACVGMFB\", \"MLBLUE\"])\n\ntarget_costs = np.array([1e1, 1e2, 1e3, 1e4], dtype=int)\noptimized_estimators = multifidelity.compare_estimator_variances(\n    target_costs, estimators)\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nmultifidelity.plot_estimator_variances(\n    optimized_estimators, est_labels, ax,\n    ylabel=mathrm_label(\"Relative Estimator Variance\"))\nax.set_xlim(target_costs.min(), target_costs.max())\n\nocv_variances = np.empty_like(target_costs, dtype=float)\nfor ii, target_cost in enumerate(target_costs):\n    # ocv uses one sample for each model (but does not need additional\n    # values for estimating low fidelity means\n    nocv_samples_per_model = int(target_cost//model_costs.sum())\n    ocv_variances[ii] = cov[0, 0]/nocv_samples_per_model*(\n        1-get_control_variate_rsquared(cov))\nrel_ocv_variances = (\n    ocv_variances/optimized_estimators[0][0].optimized_variance)\nax.loglog(target_costs, rel_ocv_variances, ':o',\n          label=mathrm_label(\"OCV\"))\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now plot the number of samples allocated for each target cost\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_labels = [r\"$M_{0}$\".format(ii) for ii in range(cov.shape[0])]\nmultifidelity.plot_acv_sample_allocation_comparison(\n    optimized_estimators[-1], model_labels, plt.figure().gca())\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n.. [SUSIAMUQ2020] [D Schaden, E Ullmann. On multilevel best linear unbiased estimators, SIAM/ASA J. Uncertainty Quantification 8 (2), 601 - 635, 2020.](https://doi.org/10.1137/19M1263534)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}