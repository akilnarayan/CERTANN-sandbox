{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sparse Grids\nThe number of model evaluations required by tensor product interpolation grows exponentitally with the number of model inputs. This tutorial introduces sparse grids [BNR2000]_, [BG2004]_ which can be used to overcome the so called curse of dimensionality faced by tensor-product methods.\n\nSparse grids approximate a model (function) $f_\\alpha$ with $D$ inputs $z=[z_1,\\ldots,z_D]^\\top$ as a linear combination of low-resolution tensor product interpolantsm that is\n\n\\begin{align}f_{\\alpha, \\mathcal{I}}(z) = \\sum_{\\beta\\in \\mathcal{I}} c_\\beta f_{\\alpha,\\beta}(z),\\end{align}\n\nwhere $\\beta=[\\beta_1,\\ldots,\\beta_D]$ is a multi-index controlling the number of samples in each dimension of the tensor-product interpolants, and the index set $\\mathcal{I}$ controls the approximation accuracy and data-efficiency of the sparse grid. If the set $\\mathcal{I}$ is downward closed, that is\n\n\\begin{align}\\gamma \\le \\beta \\text{ and } \\beta \\in \\mathcal{I} \\implies \\gamma \\in \\mathcal{I},\\end{align}\n\nwhere the $\\le$ is applied per entry, then the (Smolyak) coefficients of the sparse grid are given by\n\n\\begin{align}\\sum_{i\\in [0,1]^D, \\alpha+i\\in \\mathcal{I}} (-1)^{\\lVert i \\rVert_1}.\\end{align}\n\n\nWhile any tensor-product approximation can be used with sparse grids, e.g. based on piecewise-polynomials or splines, in this tutorial we will build sparse grids with Lagrange polynomials (see `sphx_glr_auto_tutorials_surrogates_plot_tensor_product_interpolation.py`).\n\nThe following code compares a tensor-product interpolant with a level-$l$ isotropic sparse grid which sets\n\n\\begin{align}\\mathcal{I}(l)=\\{\\beta \\mid (\\max(0,l-1)\\le \\lVert\\beta\\rVert_1\\le l+D-2\\}, \\quad l\\ge 0\\end{align}\n\nwhich leads to a simpler expression for the coefficients\n\n\\begin{align}c_\\beta = (-1)^{l-\\lvert\\beta\\rvert_1} {D-1\\choose l-\\lvert\\beta\\rvert_1}.\\end{align}\n\nFirst import the necessary modules and define the function we will approximate and its variable $\\rv$.\n\n\\begin{align}f(\\rv) = \\cos(\\pi\\rv_1)\\cos(\\pi\\rv_2/2)\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import copy\nimport numpy as np\nfrom scipy import stats\nfrom pyapprox.util.visualization import get_meshgrid_function_data, plt\nfrom pyapprox.variables.joint import IndependentMarginalsVariable\nfrom pyapprox.surrogates.approximate import adaptive_approximate\nfrom pyapprox.surrogates.interp.adaptive_sparse_grid import (\n    tensor_product_refinement_indicator, isotropic_refinement_indicator,\n    variance_refinement_indicator)\nfrom pyapprox.surrogates.orthopoly.quadrature import (\n    clenshaw_curtis_in_polynomial_order, clenshaw_curtis_rule_growth)\nfrom pyapprox.util.utilities import nchoosek\nfrom pyapprox.benchmarks import setup_benchmark\n\nvariable = IndependentMarginalsVariable([stats.uniform(-1, 2)]*2)\n\n\ndef fun(zz):\n    return (np.cos(np.pi*zz[0])*np.cos(np.pi*zz[1]/2))[:, None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now plot the tensor product interpolants and the Smolyak coefficients that make up the sparse grid. The coefficients are in the upper left corner of each subplot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_level = 2\nfig, axs = plt.subplots(\n    max_level+1, max_level+1, figsize=((max_level+1)*8, (max_level+1)*6))\nranges = variable.get_statistics(\"interval\", 1.0).flatten()\nunivariate_quad_rule_info = [\n    clenshaw_curtis_in_polynomial_order, clenshaw_curtis_rule_growth,\n    None, None]\n\n\ndef build_tp(max_level_1d):\n    tp = adaptive_approximate(\n        fun, variable, \"sparse_grid\",\n        {\"refinement_indicator\": tensor_product_refinement_indicator,\n         \"max_level_1d\": max_level_1d,\n         \"univariate_quad_rule_info\": univariate_quad_rule_info}).approx\n    return tp\n\n\nlevels = np.linspace(-1.1, 1.1, 31)\ntext_props = dict(boxstyle='round', facecolor='white', alpha=0.5)\nfor ii in range(max_level+1):\n    for jj in range(max_level+1):\n        tp = build_tp([ii, jj])\n        X, Y, Z_tp = get_meshgrid_function_data(tp, ranges, 71)\n        ax = axs[max_level-jj][ii]\n        ax.contourf(X, Y, Z_tp, levels=levels, cmap=\"coolwarm\")\n        ax.plot(*tp.samples, 'ko')\n        coef = int((-1)**(max_level-(ii+jj))*nchoosek(\n            variable.num_vars()-1, max_level-(ii+jj)))\n        ax.text(0.05, 0.95, r\"${:d}$\".format(coef),\n                transform=ax.transAxes, fontsize=24,\n                verticalalignment='top', bbox=text_props)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now compare the sparse grid with a tensor product interpolant of the same level.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(3*8, 6))\nX, Y, Z_fun = get_meshgrid_function_data(fun, ranges, 51)\ntp = adaptive_approximate(\n    fun, variable, \"sparse_grid\",\n    {\"refinement_indicator\": tensor_product_refinement_indicator,\n     \"max_level_1d\": np.full(2, max_level),\n     \"univariate_quad_rule_info\": univariate_quad_rule_info}).approx\nsg = adaptive_approximate(\n    fun, variable, \"sparse_grid\",\n    {\"refinement_indicator\": isotropic_refinement_indicator,\n     \"max_level_1d\": np.full(2, max_level),\n     \"univariate_quad_rule_info\": univariate_quad_rule_info,\n     \"max_level\": max_level}).approx\nX, Y, Z_tp = get_meshgrid_function_data(tp, ranges, 51)\nX, Y, Z_sg = get_meshgrid_function_data(sg, ranges, 51)\nlb = np.min([Z_fun.min(), Z_tp.min(), Z_sg.min()])\nub = np.max([Z_fun.max(), Z_tp.max(), Z_sg.max()])\nlevels = np.linspace(lb, ub, 21)\nim = axs[0].contourf(X, Y, Z_fun, levels=levels, cmap=\"coolwarm\")\naxs[1].contourf(X, Y, Z_tp, levels=levels, cmap=\"coolwarm\")\naxs[1].plot(*tp.samples, 'ko')\naxs[2].contourf(X, Y, Z_sg, levels=levels, cmap=\"coolwarm\")\naxs[2].plot(*sg.samples, 'ko')\nfig.subplots_adjust(right=0.9)\ncbar_ax = fig.add_axes([0.9125, 0.125, 0.025, 0.75])\n_ = fig.colorbar(im, cax=cbar_ax)\n# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sparse grid is slightly less accurate than the tensor product interpolant, but uses fewer points. There is no exact formula for the number of points in an isotropic sparse grid. The following code can be used to determine the number of points in a sparse grid of any dimension or level. The number of points is much smaller than the number of points in a tensor-product grid, for a given level $l$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_isotropic_sparse_grid_num_samples(\n        nvars, max_level, univariate_quad_rule_info):\n    \"\"\"\n    Get the number of points in an isotropic sparse grid\n    \"\"\"\n    variable = IndependentMarginalsVariable([stats.uniform(-1, 2)]*nvars)\n\n    def fun(xx):\n        # a dummy function\n        return np.ones((xx.shape[1], 1))\n    sg = adaptive_approximate(\n        fun, variable, \"sparse_grid\",\n        {\"refinement_indicator\": isotropic_refinement_indicator,\n         \"max_level_1d\": np.full(nvars, max_level),\n         \"univariate_quad_rule_info\": univariate_quad_rule_info,\n         \"max_level\": max_level, \"verbose\": 0, \"max_nsamples\": np.inf}).approx\n    return sg.samples.shape[1]\n\n\nsg_num_samples = [\n    [get_isotropic_sparse_grid_num_samples(\n        nvars, level, univariate_quad_rule_info) for level in range(5)]\n    for nvars in [2, 3, 5]]\ntp_num_samples = [\n    [univariate_quad_rule_info[1](level)**nvars for level in range(5)]\n    for nvars in [2, 3, 5]]\nprint(\"Growth of number of sparse grid points\")\nprint(sg_num_samples)\nprint(\"Growth of number of tensor-product points\")\nprint(tp_num_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a function with $r$ continous mixed-derivatives, the isotropic level-$l$ sparse grid, based on 1D Clenshaw Curtis abscissa, with $M_{\\mathcal{I}(l)}$ points satisfies\n\n\\begin{align}\\lVert f-f_{\\mathcal{I}(l)}\\rVert_{L^\\infty}\\le C_{D,r} M^{-r}_{\\mathcal{I}(l)}(\\log M_{\\mathcal{I}(l)})^{(r+2)(D-1)+1}.\\end{align}\n\nIn contrast the tensor-product interpolant with $M_l$ points satifies\n\n\\begin{align}\\lVert f-f_{\\mathcal{I}(l)}\\rVert_{L^\\infty}\\le K_{D,r} M_l^{-r/D}.\\end{align}\n\nThe following code compares the convergence of sparse grids and tensor-product lagrange interpolants. A callback is used to compute the error as the level of the approximations increases\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class IsotropicCallback():\n    def __init__(self, validation_samples, validation_values, istp):\n        self.level = -1\n        self.errors = []\n        self.nsamples = []\n        self.validation_samples = validation_samples\n        self.validation_values = validation_values\n        self.istp = istp\n\n    def __call__(self, approx):\n        if self.istp:\n            approx_level = approx.subspace_indices.max()\n        else:\n            approx_level = approx.subspace_indices.sum(axis=0).max()\n        if self.level != approx_level:\n            # only compute error when all subspaces of the current\n            # approximation level are added to the sparse grid.\n            # The number of sparse grid points will be slightly larger\n            # than an isotoropic grid of level=approx_level because\n            # points associated with active indies will be included here.\n            self.level = approx_level\n            self.nsamples.append(approx.samples.shape[1])\n            approx_values = approx.evaluate_using_all_data(\n                self.validation_samples)\n            error = (np.linalg.norm(\n                self.validation_values-approx_values) /\n                     self.validation_samples.shape[1])\n            self.errors.append(error)\n\n\ndef fun(xx):\n    return np.exp(-0.05*(((xx+1)/2-0.5)**2).sum(axis=0))[:, None]\n\n\n# do not go passed nvars,level = (4, 4) with clenshaw curtis rules\n# do not go passed nvars,level = (5, 4) with Leja rules\n\nnvars = 4\nvariable = IndependentMarginalsVariable([stats.uniform(-1, 2)]*nvars)\nvalidation_samples = variable.rvs(1000)\nvalidation_values = fun(validation_samples)\n\n# switch to Leja quadrature rules with linear growth\n# univariate_quad_rule_info = None\n\ntp_max_level = 4\ntp_callback = IsotropicCallback(validation_samples, validation_values, True)\ntp = adaptive_approximate(\n    fun, variable, \"sparse_grid\",\n    {\"refinement_indicator\": tensor_product_refinement_indicator,\n     \"max_level_1d\": np.full(nvars, tp_max_level),\n     \"univariate_quad_rule_info\": univariate_quad_rule_info,\n     \"max_nsamples\": np.inf, \"callback\": tp_callback,\n     \"verbose\": 0}).approx\n# compute error at final level\ntp_callback.level = tp_max_level  # set so callback computes error\ntp_callback(tp)\n\nsg_max_level = 6\nsg_callback = IsotropicCallback(validation_samples, validation_values, False)\nsg = adaptive_approximate(\n    fun, variable, \"sparse_grid\",\n    {\"refinement_indicator\": isotropic_refinement_indicator,\n     \"max_level_1d\": np.full(nvars, sg_max_level),\n     \"univariate_quad_rule_info\": univariate_quad_rule_info,\n     \"max_level\": sg_max_level, \"max_nsamples\": np.inf,\n     \"callback\": sg_callback}).approx\n# compute error at final level\nsg_callback.level = sg_max_level  # set so callback computes error\nsg_callback(sg)\n\nax = plt.subplots(1, 1, figsize=(8, 6))[1]\nax.loglog(tp_callback.nsamples, tp_callback.errors, '-o', label=\"TP\")\nax.loglog(sg_callback.nsamples, sg_callback.errors, '--s', label=\"SG\")\n_ = ax.legend()\n# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment with changing nvars, e.g. try nvars = 2,3,4. Sparse grids become more effective as nvars increases.\n\nSo far we have used sparse grids based on Clenshaw-Curtis 1D quadrature rules. However other types of rules can be used. PyApprox also supports 1D Leja sequences  [NJ2014]_ (see `sphx_glr_auto_tutorials_surrogates_plot_adaptive_leja_interpolation.py`). Change univariate_quad_rule=None to use Leja rules and observe the difference in convergence.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimension adaptivity\nThe efficiency of sparse grids can be improved using methods [GG2003]_, [H2003]_ that construct the index set $\\mathcal{I}$ adaptively. This is the default behavior when using Pyapprox. The following applies the adaptive algorithm to an anisotropic function, where one variable impacts the function much more than the other.\n\nFinding an efficient index set can be cast as an optimization problem. With this goal, let the difference in sparse grid error before and after the interpolant $f_{\\alpha,\\beta}$ and the work from adding the new interpolant respectively be\n\n\\begin{align}\\begin{align*}\\Delta E_\\beta = \\lVert f_{\\alpha, \\mathcal{I}\\cup\\beta}-f_{\\alpha, \\mathcal{I}}\\rVert && \\Delta W_\\beta = \\lVert W_{\\alpha, \\mathcal{I}\\cup\\beta}-W_{\\alpha, \\mathcal{I}}\\rVert\\end{align*}\\end{align}\n\nThen noting that the error in the sparse grid satisfies, we can formulate finding a quasi-optimal index set as a binary knapsack problem\n\n\\begin{align}\\max \\sum_{\\beta}\\Delta E_\\beta\\delta_\\beta \\text{ such that }\\sum_{\\beta}\\Delta W_\\beta\\delta_\\beta \\le W_{\\max},\\end{align}\n\nfor a total work budget $W_{\\max}$. The solution to this problem balances the computational work of adding a specific interpolant with the reduction in error that would be achieved.\n\nThe isotropic index set represents a solution to this knapsack problem under certain conditions on the smoothness of the function being approximated. However, for weaker conditions, finding optimal index sets by solving the knapsack problem is typically intractable. Consequently, we use a greedy adaptive procedure.\n\nThe algorithm begins with the index set $\\mathcal{I}=\\{\\beta\\mid \\beta=[0, \\ldots, 0]\\}$ then identifies, so called active indices, which are candidates for refinement. The active indices satisfy the downward closed admissibility criteria above. The function is evaluated at the points assoicated with the active indices and error indicators, similar to $\\Delta W_\\beta$, are computed. The active index with the largest error indicator is then added to $\\mathcal{I}$ and the active set is updated. This procedure is repeated until and error threshold is met or a computational budget reached.\n\nFirst set up the benchmark\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark = setup_benchmark(\n    \"genz\", nvars=2, test_name='oscillatory', coeff=([2, 0.2], [0, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now build a sparse grid using a callback that tracks important properties\nof the sparse grid and its adaptation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class AdaptiveCallback():\n    def __init__(self, validation_samples, validation_values):\n        self.validation_samples = validation_samples\n        self.validation_values = validation_values\n\n        self.nsamples = []\n        self.errors = []\n        self.sparse_grids = []\n\n    def __call__(self, approx):\n        self.nsamples.append(approx.samples.shape[1])\n        approx_values = approx.evaluate_using_all_data(\n            self.validation_samples)\n        error = (np.linalg.norm(\n            self.validation_values-approx_values) /\n                     self.validation_samples.shape[1])\n        self.errors.append(error)\n        self.sparse_grids.append(copy.deepcopy(approx))\n\n\nvalidation_samples = benchmark.variable.rvs(100)\nvalidation_values = benchmark.fun(validation_samples)\nadaptive_callback = AdaptiveCallback(validation_samples, validation_values)\nsg = adaptive_approximate(\n    benchmark.fun, benchmark.variable, \"sparse_grid\",\n    {\"refinement_indicator\": variance_refinement_indicator,\n     \"max_level_1d\": np.inf,\n     \"univariate_quad_rule_info\": None,\n     \"max_level\": np.inf, \"max_nsamples\": 100,\n     \"callback\": adaptive_callback}).approx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following visualizes the adaptive algorithm.\n\nThe left plot depicts the multi-index of each tensor product interpolant. They gray boxes represent indices added to the sparse grid and the red boxes represent the active indices. The numbers in the boxes represent the Smolyak coefficients.\n\nThe middle plot shows the grid points associated with the gray boxes (black dots) and the grid points associated with the active indices (red dots).\n\nThe left plot depicts the sparse grid approximation at each iteration.\n\nThe sparse grid spends more points resolving the function variation in the horizontal direction, associated with the most sensitive function input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pyapprox.surrogates.interp.adaptive_sparse_grid import (\n    plot_adaptive_sparse_grid_2d)\n\nfig, axs = plt.subplots(1, 3, sharey=False, figsize=(3*8, 6))\nranges = benchmark.variable.get_statistics(\"interval\", 1.0).flatten()\ndata = [get_meshgrid_function_data(sg, ranges, 51)\n        for sg in  adaptive_callback.sparse_grids]\nZ_min = np.min([d[2] for d in data])\nZ_max = np.max([d[2] for d in data])\nlevels = np.linspace(Z_min, Z_max, 21)\ndef animate(ii):\n    [ax.clear() for ax in axs]\n    sg = adaptive_callback.sparse_grids[ii]\n    plot_adaptive_sparse_grid_2d(sg, axs=axs[:2])\n    axs[2].contourf(*data[ii], levels=levels)\n    axs[0].set_xlim([0, 10])\n    axs[0].set_ylim([0, 10])\n\nimport matplotlib.animation as animation\nani = animation.FuncAnimation(\n    fig, animate, interval=500,\n    frames=len(adaptive_callback.sparse_grids), repeat_delay=1000)\nani.save(\"adaptive_sparse_grid.gif\", dpi=100,\n         writer=animation.ImageMagickFileWriter())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n.. [BNR2000] [V. Barthelmann, E. Novak and K. Ritter. High dimensional polynomial interpolation on sparse grid. Advances in Computational Mathematics (2000).](https://doi.org/10.1023/A:1018977404843)\n.. [BG2004] [H. Bungartz and  M. Griebel. Sparse grids. Acta Numerica (2004).](https://doi.org/10.1017/S0962492904000182)\n.. [GG2003] [T. Gerstner and M. Griebel. Dimension-adaptive tensor-product quadrature. Computing (2003).](https://doi.org/10.1007/s00607-003-0015-5)\n.. [H2003] [M. Hegland. Adaptive sparse grids. Proc. of 10th Computational Techniques and Applications Conference (2003).](https://doi.org/10.21914/anziamj.v44i0.685)\n.. [NJ2014] [A. Narayan and J.D. Jakeman. Adaptive Leja Sparse Grid Constructions for Stochastic Collocation and High-Dimensional Approximation. SIAM Journal on Scientific Computing (2014).](http://dx.doi.org/10.1137/140966368)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}