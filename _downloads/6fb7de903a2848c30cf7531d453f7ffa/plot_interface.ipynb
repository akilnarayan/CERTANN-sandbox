{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\rvset{{\\mathcal{Z}}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\text{d}#1}\\newcommand{\\mat}[1]{{\\boldsymbol{\\mathrm{#1}}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Model interfacing\nThis tutorial demonstrats how to use model wrappers to time function calls and evaluate a function at multiple samples in parallel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Timing function evaluations\nIt is often useful to be able to track the time needed to evaluate a function. We can track this using the :class:`~pyapprox.interface.wrappers.TimerModel` and :class:`~pyapprox.interface.wrappers.WorkTrackingModel` objects which are designed to work together. The former times each evaluation of a function that returns output of shape (nsamples,nqoi) and appends the time to the quantities of interest returned by the function, i.e returns a 2D np.ndarray with shape (nsamples,nqoi+1). The second extracts the time and removes it from the quantities of interest and returns output with the original shape  (nsamples,nqoi) of the user function.\n\nLets use the class with a function that takes a random amount of time. We will use the previous function but add a random pause between 0 and .1 seconds. Lets import some functions and define a multi-variate random variable\n\n.. literalinclude:: ../../../examples/__util.py\n  :language: python\n  :start-at: def fun_pause_1\n  :end-before: def fun_pause_2\n\n.. Note for some reason text like this is needed after the literalinclude\n.. Also note that path above is relative to source/auto_examples\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport time\nimport tempfile\nimport numpy as np\nfrom scipy import stats\n\nfrom pyapprox.variables import IndependentMarginalsVariable\nfrom pyapprox.interface.wrappers import (\n    TimerModel, WorkTrackingModel, PoolModel, ModelEnsemble\n)\nfrom pyapprox.interface.async_model import AynchModel\nfrom pyapprox.interface.tests.test_async_model import get_file_io_model\n\nunivariate_variables = [stats.uniform(-2, 4), stats.uniform(-2, 4)]\nvariable = IndependentMarginalsVariable(univariate_variables)\n\nfrom __util import pyapprox_fun_1, fun_pause_1\ntimer_fun = TimerModel(pyapprox_fun_1)\nworktracking_fun = WorkTrackingModel(timer_fun)\n\nnsamples = 10\nsamples = variable.rvs(nsamples)\nvalues = worktracking_fun(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~pyapprox.interface.wrappers.WorkTrackingModel` has an attribute :class:`~pyapprox.interface.wrappers.WorkTracker` which stores the execution time of each function evaluation as a dictionary. The key corresponds is the model id. For this example the id will always be the same, but the id can vary and this is useful when evaluating mutiple models, e.g. when using multi-fidelity methods. To print the dictionary use\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "costs = worktracking_fun.work_tracker.costs\nprint(costs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also call the work tracker to query the median cost for a model with a given id. The default id is 0.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fun_id = np.atleast_2d([0])\nprint(worktracking_fun.work_tracker(fun_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating multiple models\nNow let apply this two an ensemble of models to explore the use of model ids. First create a second function which we import.\n\n.. literalinclude:: ../../../examples/__util.py\n  :language: python\n  :start-at: def fun_pause_2\n\n.. Note for some reason text like this is needed after the literalinclude\n.. Also note that path above is relative to source/auto_examples\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __util import pyapprox_fun_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now using :class:`~pyapprox.interface.ModelEnsemble` we can create a function which takes the random samples plus an additional configure variable which defines which model to evaluate. Lets use half the samples to evaluate the first model and evaluate the second model at the remaining samples\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_ensemble = ModelEnsemble([pyapprox_fun_1, pyapprox_fun_2])\ntimer_fun_ensemble = TimerModel(model_ensemble)\nworktracking_fun_ensemble = WorkTrackingModel(\n    timer_fun_ensemble, num_config_vars=1)\n\nfun_ids = np.ones(nsamples)\nfun_ids[:nsamples//2] = 0\nensemble_samples = np.vstack([samples, fun_ids])\nvalues = worktracking_fun_ensemble(ensemble_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we had to pass the number (1) of configure variables to the\nWorkTrackingModel. PyApprox assumes that the configure variables are the last rows of the samples 2D array\n\nNow check that the new values are the same as when using the individual functions directly\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert np.allclose(values[:nsamples//2],\n                   pyapprox_fun_1(samples[:, :nsamples//2]))\nassert np.allclose(values[nsamples//2:],\n                   pyapprox_fun_2(samples[:, nsamples//2:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we can query the execution times of each model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "costs = worktracking_fun_ensemble.work_tracker.costs\nprint(costs)\n\nquery_fun_ids = np.atleast_2d([0, 1])\nprint(worktracking_fun_ensemble.work_tracker(query_fun_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected there are 5 samples tracked for each model and the median evaluation time of the second function is about twice as large as for the first function.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating non-python models\nThe :class:`~pyapprox.interface.async_model.AynchModel` can be used to run models that may not be written in Python, but can be evaluated from the command line via a shell script. The :class:`~pyapprox.interface.async_model.AynchModel` creates a file named params.in (the name can be changed by the user) and assumes that the shell script reads in that file and returns the output of the model in a file called results.out (this name can also be changed). Each evaluation of the model is performed in a separate work directory to ensure that no results are overwritten, which is especially important when running the model in parallel. If a list of filenames needed to run the bash script is provided a soft link to each file is created in each work directory. This is extremely useful when running large finite element simulations that may have input files, such as mesh and topography data, with large memory footprints.\n\nThe following creates a model with two inputs and two quantities of interest and evaluates it at three samples. Temporary work directories are created to run the model at each sample. The directories are automatically deleted, however the user can choose to keep each directory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "file_io_model = get_file_io_model(0.02)[0]\ntmp_dir = tempfile.TemporaryDirectory()\nasynch_model = AynchModel(\n    file_io_model.shell_command, workdir_basename=tmp_dir.name,\n    save_workdirs=\"no\")\nasynch_variable = IndependentMarginalsVariable([stats.uniform(0, 1)]*2)\nsamples = asynch_variable.rvs(3)\nvalues = asynch_model(samples)\nprint(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating functions at multiple samples in parallel\nFor expensive models it is often useful to be able to evaluate each model concurrently. This can be achieved using :class:`~pyapprox.interface.wrappers.PoolModel`. Note this function is not intended for use with distributed memory systems, but rather is intended to use all the threads of a personal computer or compute node. See the documention of the , max_eval_concurrency keyword argument to the initization function for :class:`~pyapprox.interface.async_model.AynchModel` if you are interested in running multiple simulations in parallel on a distributed memory system.\n\nPoolModel cannot be used to wrap WorkTrackingModel. However it can still\nbe used with WorkTrackingModel using the sequence of wrappers below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_eval_concurrency = 1  # set higher\n# clear WorkTracker counters\npool_model = PoolModel(\n    timer_fun_ensemble, max_eval_concurrency, assert_omp=False)\nworktracking_fun_ensemble.work_tracker.costs = dict()\nworktracking_fun_ensemble = WorkTrackingModel(\n    pool_model, num_config_vars=1, enforce_timer_model=False)\n\n# create more samples to notice improvement in wall time\nnsamples = 10\nsamples = variable.rvs(nsamples)\nfun_ids = np.ones(nsamples)\nfun_ids[:nsamples//2] = 0\nensemble_samples = np.vstack([samples, fun_ids])\n\nt0 = time.time()\nvalues = worktracking_fun_ensemble(ensemble_samples)\nt1 = time.time()\nprint(f'With {max_eval_concurrency} threads that took {t1-t0} seconds')\n\nif ('OMP_NUM_THREADS' not in os.environ or\n    int(os.environ['OMP_NUM_THREADS']) != 1):\n    # make sure to set OMP_NUM_THREADS=1 to maximize benefit of pool model\n    print('Warning set OMP_NUM_THREADS=1 for best performance')\nmax_eval_concurrency = 4\npool_model.set_max_eval_concurrency(max_eval_concurrency)\nt0 = time.time()\nvalues = worktracking_fun_ensemble(ensemble_samples)\nt1 = time.time()\nprint(f'With {max_eval_concurrency} threads that took {t1-t0} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets print a summary of the costs to make sure individual function evaluation\ncosts are still being recorded correctly\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(worktracking_fun_ensemble.work_tracker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note\nPoolModel cannot be used with lambda functions. You will get error similar to pickle.PicklingError: Can't pickle <function <lambda> at 0x12b4e6440>: attribute lookup <lambda> on __main__ failed\n\nWhen because the benchmark fun is run using multiprocessing.Pool\nThe .py script of this tutorial cannot be run with max_eval_concurrency > 1\nvia the shell command using python plot_pde_convergence.py because Pool\nmust be called inside\n\n```python\nif __name__ == '__main__':\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sphinx_gallery_thumbnail_path = './figures/cantilever-beam.png'\n.. gallery thumbnail will say broken if no plots are made in this file so\n.. specify a default file as above. Must start with a #\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}