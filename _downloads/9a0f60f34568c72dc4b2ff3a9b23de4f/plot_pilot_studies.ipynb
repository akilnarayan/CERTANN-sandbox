{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\rvset{{\\mathcal{Z}}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\text{d}#1}\\newcommand{\\mat}[1]{{\\boldsymbol{\\mathrm{#1}}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Pilot Studies\nThe covariance of an ACV estimator depends on the covariance between the different model fidelities and other statistics, such as variance, depend on additional statistics properties of the model. In previous tutorials, we have assumed that thse statistics are available. However, in practice they must be estimated, because if we knew them we would not have to construct an MC estimator in the first place.\n\nThis tutorial presents how to use a pilot-study to compute the statistics needed to compute a MC-based estimator and compute its estimator covariance. We will focus on estimating the mean of a scalar model, but the procedures we describe here can easily be extended to estimation of other statistics such as variance. One simlpy must use a pilot study to compute the relevant quantities defined in `sphx_glr_auto_tutorials_multi_fidelity_acv_covariances.py`\n\nComputing an ACV estimator of a statistic requires computing  $\\covar{\\mat{Q}_0}{\\mat{\\Delta}}\\text{ and} \\covar{\\mat{\\Delta}}{\\mat{\\Delta}}$. These quantities in turn depend on the quantities in `sphx_glr_auto_tutorials_multi_fidelity_acv_covariances.py`. For example, when estimating the mean we must compute\n\n\\begin{align}\\covar{f_\\alpha}{f_\\beta}\\qquad \\forall \\alpha,\\beta\\end{align}\n\nTypically, this is not available so we compute it with a pilot study. A pilot study evaluates all the available models at a small set of samples $\\rvset_\\text{pilot}$ and computes the quantities necessary to construct an ACV estimator. For example when computing the mean we estimate\n\n\\begin{align}\\covar{f_\\alpha}{f_\\beta}\\approx{N_\\text{pilot}^{-1}}\\sum_{n=1}^{N_\\text{pilot}} \\left(f_\\alpha(\\rv^{(n)})-Q_\\alpha(\\rvset_\\text{pilot})\\right)\\left(f_\\beta(\\rv^{(n)})-Q_\\beta(\\rvset_\\text{pilot})\\right)\\end{align}\n\nwhere\n\n\\begin{align}Q_\\alpha={N_\\text{pilot}^{-1}}\\sum_{n=1}^{N_\\text{pilot}} f_\\alpha(\\rv^{(n)})\\approx \\mean{f_\\alpha}\\end{align}\n\nWith an unlimited computational budget, we would drive $N_\\text{pilot}\\to\\infty$, however in practice we must use a finite number of samples which introduces an error in to an ACV estimator.\n\nThe following code quantities the impact of the size of the pilot sample on the accuracy of an ACV estimator. If the pilot size is too small the accuracy of the control variate coefficieints will be poor, however it is made larger, constructing the pilot study will eat into the computational budget used to construct the estimator which also degrades accuracy. These two concerns need to be balanced.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First setup the example\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pyapprox.benchmarks import setup_benchmark\nfrom pyapprox.multifidelity.factory import get_estimator, multioutput_stats\nfrom pyapprox.util.utilities import get_correlation_from_covariance\nfrom pyapprox.util.visualization import mathrm_label\n\nnp.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now choose an estimator and optimally allocate it with oracle information,\nthat is the exact model covariance. We will use MFMC because the optimal sample allocation can be obtained analytically which speeds up this tutorial. However other estimators can be used. Also note that if using MFMC to estimate variance or other stats its allocation it still uses the allocation that is only guaranteed to be optimal when estimating the mean. This is not true of any other estimator except MLMC.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_cost = 100\nest_name = \"mfmc\"\nbenchmark = setup_benchmark(\"polynomial_ensemble\", nmodels=3)\nnmodels = len(benchmark.funs)\ncosts = np.array([1, 0.1, 0.05])\n\nstat_type = \"mean\"\n# stat_type = \"variance\"\n\ncov = benchmark.covariance\nif stat_type == \"mean\":\n    oracle_stats = benchmark.mean[0]\n    oracle_stat_args = [cov]\nelse:\n    oracle_stats = benchmark.covariance[0, 0]\n    oracle_stat_args = [\n        cov, benchmark.fun.covariance_of_centered_values_kronker_product()]\n\nprint(get_correlation_from_covariance(benchmark.covariance))\n\noracle_stat = multioutput_stats[stat_type](benchmark.nqoi)\noracle_stat.set_pilot_quantities(*oracle_stat_args)\n\noracle_est = get_estimator(est_name, oracle_stat, costs)\noracle_est.allocate_samples(target_cost)\nprint(oracle_est)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets look at the MSE of the ACV estimator when pilot samples of different sizes are used. First, create a function that computes an acv estimator for a single pilot study.  We will then repeatedly call this function to compute the MSE.\n\nNote, the function we define below can be replicated for most practical application of ACV estimation, but in such situations it sill only be called once.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pyapprox.multifidelity.stats import MultiOutputMean\nfrom pyapprox.multifidelity.factory import multioutput_stats\nfrom functools import partial\n\ndef build_acv(funs, variable, target_cost, npilot_samples, adjust_cost=True,\n              seed=1, stat_type=\"mean\"):\n    # run pilot study\n\n    # Note must set random state if running with nprocs > 1\n    random_states = [\n        np.random.RandomState(seed*2*variable.num_vars()+ii)\n        for ii in range(variable.num_vars())]\n    # instead of np.random.seed(seed)\n    pilot_samples = variable.rvs(npilot_samples, random_states=random_states)\n    pilot_values_per_model = [fun(pilot_samples) for fun in funs]\n    stat_class = multioutput_stats[stat_type]\n    pilot_quantities = stat_class.compute_pilot_quantities(\n        pilot_values_per_model)\n    # print(get_correlation_from_covariance(pilot_cov))\n\n    # optimize the ACV estimator\n    stat = multioutput_stats[stat_type](benchmark.nqoi)\n    stat.set_pilot_quantities(*pilot_quantities)\n    est = get_estimator(est_name, stat, costs)\n    # remaining_budget_after_pilot\n    if adjust_cost:\n        adjusted_target_cost = target_cost - (costs*npilot_samples).sum()\n    else:\n        adjusted_target_cost = target_cost\n    # print(adjusted_target_cost)\n    try:\n        est.allocate_samples(adjusted_target_cost)\n        # compute the ACV estimator\n        random_states = [\n            np.random.RandomState(\n                seed*2*variable.num_vars()+variable.num_vars()+ii)\n            for ii in range(variable.num_vars())]\n        samples_per_model = est.generate_samples_per_model(\n            partial(variable.rvs, random_states=random_states))\n        values_per_model = [\n            fun(samples) for fun, samples in zip(funs, samples_per_model)]\n        est_stats = est(values_per_model)\n        return est_stats\n    except (RuntimeError, RuntimeWarning) as e:\n        # sometimes MFMC will fail when used to find optimal solution\n        # for variance because we cannot bound from below the number of\n        # high-fidelity samples to be at least 2.\n        # This causes the error\n        # Degrees of freedom <= 0 for slice error occurs because\n        # or\n        # Rounding will cause nhf samples to be zero tensor\n        # when nhf samples is below 1.\n        print(e)\n        return [np.inf]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now define a function to compute the MSE. Note nprocs cannot be set > 1 unless all of this code is placed inside a function, e.g. called main,  which is then\nrun inside the following conditional\n\n```python\nif __name__ == '__main__':\n    main()\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from multiprocessing import Pool\ndef compute_mse(build_acv, funs, variable, target_cost, npilot_samples,\n                adjust_cost, ntrials, nprocs, exact_stats, stat_type):\n    build = partial(\n        build_acv, funs, variable, target_cost, npilot_samples, adjust_cost,\n        stat_type=stat_type)\n    if nprocs > 1:\n        pool = Pool(nprocs)\n        est_vals = pool.map(build, list(range(ntrials)))\n        pool.close()\n    else:\n        est_vals = np.asarray([build(ii) for ii in range(ntrials)])\n\n    # exclude failed MCMC runs\n    est_vals = np.array(est_vals)[np.isfinite(est_vals)]\n    # make sure random seed is getting set correctly on each processor\n    assert np.unique(est_vals).shape[0] == len(est_vals)\n    mse = ((est_vals-exact_stats)**2).mean()\n    return mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ignoring the computational cost of the pilot study\nNow we will build many realiaztions of the ACV estimator for each different samples size. We must ensure that the cost of the pilot study and the construction of the ACV estimator do not exceed the target cost.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\nntrials = int(1e3)\nmse_list = []\nnpilot_samples_list = [5, 10, 20, 40, 80, 160]\nfor npilot_samples in npilot_samples_list:\n    print(npilot_samples)\n    mse_list.append(compute_mse(\n        build_acv, benchmark.funs, benchmark.variable, target_cost,\n        npilot_samples, False, ntrials, 1, oracle_stats,\n        stat_type))\n    print(mse_list[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now compare the MSE of the oracle ACV estimator which is just equal to its estimator variance, with the estimators constructed for different pilot samples sizes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mc_est = get_estimator(\"mc\", oracle_stat, costs)\nmc_est.allocate_samples(target_cost)\nmc_mse = mc_est._optimized_covariance[0, 0].item()\n\noracle_mse = oracle_est._optimized_covariance[0, 0].item()\nax = plt.subplots(1, 1, figsize=(8, 6))[1]\nax.axhline(y=oracle_mse, ls='--', color='k', label=mathrm_label(\"Oracle MSE\"))\nax.axhline(y=mc_mse, ls=':', color='r', label=mathrm_label(\"Oracle MC MSE\"))\nax.plot(npilot_samples_list, mse_list, '-o', label=mathrm_label(\"Pilot MSE\"))\nax.set_xlabel(mathrm_label(\"Number of pilot samples\"))\n_ = ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accounting for the computational cost of the pilot study\nThe previous study asssumed that the target cost of the estimator was not impacted by the computational cost of running the pilot study but this is not practical. Now lets plot the MSE vs the number of pilot samples keeping while ensuring that the cost of computing the pilot and the cost of constructing the estimator are always equal to the target cost\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\nntrials = int(1e3)\nmse_list = []\ntarget_cost = 100\n# we must keep pilot cost below target cost\nnpilot_samples_list = [5, 10, 20, 40, 60, 70, 80]\nfor npilot_samples in npilot_samples_list:\n    # The cost of evaluating all models once is np.sum(costs)\n    pilot_cost = np.sum(costs)*npilot_samples\n    print(npilot_samples)\n    mse_list.append(compute_mse(\n        build_acv, benchmark.funs, benchmark.variable, target_cost-pilot_cost,\n        npilot_samples, False, ntrials, 1, oracle_stats,\n        stat_type))\n    print(mse_list[-1])\n\nax = plt.subplots(1, 1, figsize=(8, 6))[1]\nax.axhline(y=oracle_mse, ls='--', color='k', label=mathrm_label(\"Oracle MSE\"))\nax.axhline(y=mc_mse, ls=':', color='r', label=mathrm_label(\"Oracle MC MSE\"))\nax.plot(npilot_samples_list, mse_list, '-o', label=mathrm_label(\"Pilot MSE\"))\nax.set_xlabel(mathrm_label(\"Number of pilot samples\"))\n_ = ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see if the computational cost of the pilot study is a large fraction of the target cost then the MSE becomes very bad.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating model costs\nThe pilot study is also used to approximate the model costs. Typically, we take the median run time of each model over the pilot samples\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating statistics other than the mean\n Try setting stat_type=\"variance\". What impact does it have on the MSE as a function of the number of pilot samples?\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}