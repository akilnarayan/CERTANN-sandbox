{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Push Forward Based Inference\nThis tutorial describes push forward based inference (PFI) [BJWSISC2018]_.\n\nPFI solves the inverse problem of inferring parameters $\\rv$ of a deterministic model $f(\\rv)$ from stochastic observational data on quantities of interest.  The solution is a posterior probability measure that when propagated through the deterministic model produces a push-forward measure that exactly matches a given observed probability measure on available data.  \n\nThe solution to the PFI inverse problem is given by\n\n\\begin{align}\\pi_\\text{post}(\\rv)=\\pi_\\text{pr}(\\rv)\\frac{\\pi_\\text{obs}(f(\\rv))}{\\pi_\\text{model}(f(\\rv))}\\end{align}\n\nwhere $\\pi_\\text{pr}(\\rv)$ is a prior density which captures any initial knowledge, $\\pi_\\text{obs}(f(\\rv))$ is the density on the observations, and $\\pi_\\text{model}(f(\\rv))$ is the push-forward of the prior density trough the forward model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we must define the forward model. We will use a functional of solution to the following system of non-linear equations\n\n\\begin{align}\\rv_1 x_1^2+x_2^2&=1\\\\\n  x_1^2-\\rv_2x_2^2&=1\\end{align}\n\nSpecifically we choose $f(\\rv)=x_2(\\rv)$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pyapprox.benchmarks import setup_benchmark\nbenchmark = setup_benchmark(\"parameterized_nonlinear_model\")\nmodel = benchmark.fun"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the prior density and the observational density\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prior_variable = benchmark.variable\nprior_pdf = prior_variable.pdf\n\n\nmean = 0.3\nvariance = 0.025**2\nobs_variable = stats.norm(mean, np.sqrt(variance))\ndef obs_pdf(y): return obs_variable.pdf(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PFI requires the push forward of the prior $\\pi_\\text{model}(f(\\rv))$. Lets approximate this PDF using a Gaussian kernel density estimate built on a large number model outputs evaluated at random samples of the prior.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#Define samples used to evaluate the push forward of the prior\nnum_prior_samples = 10000\nprior_samples = prior_variable.rvs(num_prior_samples)\nresponse_vals_at_prior_samples = model(prior_samples)\n\n#Construct a KDE of the push forward of the prior through the model\npush_forward_kde = stats.gaussian_kde(response_vals_at_prior_samples.T)\ndef push_forward_pdf(y): return push_forward_kde(y.T)[:, None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now simply evaluate\n\n\\begin{align}\\pi_\\text{post}(\\rv)=\\pi_\\text{pr}(\\rv)\\frac{\\pi_\\text{obs}(f(\\rv))}{\\hat{\\pi}_\\text{model}(f(\\rv))}\\end{align}\n\nusing the approximate push forward PDF $\\hat{\\pi}_\\text{model}(f(\\rv))$. Lets use this fact to plot the posterior density.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#Define the samples at which to evaluate the posterior density\nnum_pts_1d = 50\nfrom pyapprox.analysis import visualize\nX, Y, samples_for_posterior_eval = \\\n    visualize.get_meshgrid_samples_from_variable(prior_variable, 30)\n\n#Evaluate the density of the prior at the samples used to evaluate\n#the posterior\nprior_prob_at_samples_for_posterior_eval = prior_pdf(\n    samples_for_posterior_eval)\n\n#Evaluate the model at the samples used to evaluate\n#the posterior\nresponse_vals_at_samples_for_posterior_eval = model(samples_for_posterior_eval)\n\n#Evaluate the distribution on the observable data\nobs_prob_at_samples_for_posterior_eval = obs_pdf(\n    response_vals_at_samples_for_posterior_eval)\n\n#Evaluate the probability of the responses at the desired points\nresponse_prob_at_samples_for_posterior_eval = push_forward_pdf(\n    response_vals_at_samples_for_posterior_eval)\n\n#Evaluate the posterior probability\nposterior_prob = (prior_prob_at_samples_for_posterior_eval*(\n    obs_prob_at_samples_for_posterior_eval /\n    response_prob_at_samples_for_posterior_eval))\n\n#Plot the posterior density\np = plt.contourf(\n    X, Y, np.reshape(posterior_prob, X.shape),\n    levels=np.linspace(posterior_prob.min(), posterior_prob.max(), 30),\n    cmap=mpl.cm.coolwarm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that to plot the posterior we had to evaluate the model at each plot points. This can be expensive. To avoid this surrogate models can be used to replace the expensive model [BJWSISC2018b]_. But ignoring such approaches we can still obtain useful information without additional model evaluations as is done above. For example, we can easily and with no additional model evaluations evaluate the posterior density at the prior samples\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prior_prob_at_prior_samples = prior_pdf(prior_samples)\nobs_prob_at_prior_samples = obs_pdf(response_vals_at_prior_samples)\nresponse_prob_at_prior_samples = push_forward_pdf(\n    response_vals_at_prior_samples)\nposterior_prob_at_prior_samples = prior_prob_at_prior_samples * (\n    obs_prob_at_prior_samples / response_prob_at_prior_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use rejection sampling to draw samples from the posterior\nGiven a random number $u\\sim U[0,1]$, and the prior samples $x$. We accept a prior sample as a draw from the posterior if\n\n\\begin{align}u\\le\\frac{\\pi_\\text{post}(\\rv)}{M\\pi_\\text{prop}(\\rv)}\\end{align}\n\nfor some proposal density $\\pi_\\text{prop}(\\rv)$ such that $M\\pi_\\text{prop}(\\rv)$ is an upper bound on the density of the posterior. Here we set $M=1.1\\,\\max_{\\rv\\in\\rvdom}\\pi_\\text{post}(\\rv)$ to be a constant sligthly bigger than the maximum of the posterior over all the prior samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_posterior_prob = posterior_prob_at_prior_samples.max()\naccepted_samples_idx = np.where(\n    posterior_prob_at_prior_samples/(1.1*max_posterior_prob) >\n    np.random.uniform(0., 1., (num_prior_samples, 1)))[0]\nposterior_samples = prior_samples[:, accepted_samples_idx]\nacceptance_ratio = posterior_samples.shape[1]/num_prior_samples\nprint(\"Acceptance ratio\", acceptance_ratio)\n\n#plot the accepted samples\nplt.plot(posterior_samples[0, :], posterior_samples[1, :], 'ok')\nplt.xlim(model.ranges[:2])\n_ = plt.ylim(model.ranges[2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of inverse problem was to define the posterior density that when push forward through the forward model exactly matches the observed PDF. Lets check that by pushing forward our samples from the posterior.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#compute the posterior push forward\nposterior_push_forward_kde = stats.gaussian_kde(\n    response_vals_at_prior_samples[accepted_samples_idx].T)\n\n\ndef posterior_push_forward_pdf(\n    y): return posterior_push_forward_kde(y.T).squeeze()\n\n\nplt.figure()\nlb, ub = obs_variable.interval(1-1e-10)\ny = np.linspace(lb, ub, 101)\nplt.plot(y, obs_pdf(y), 'r-')\nplt.plot(y, posterior_push_forward_pdf(y), 'k--')\nplt.plot(y, push_forward_pdf(y), 'b:')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explore the number effect of changing the number of prior samples on the posterior and its push-forward.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [BJWSISC2018] `T Butler, J Jakeman, T Wildey. Combining push-forward measures and Bayes' rule to construct consistent solutions to stochastic inverse problems. SIAM Journal on Scientific Computing, 40 (2), A984-A1011 (2018). <https://doi.org/10.1137/16M1087229>`_\n\n.. [BJWSISC2018b] `T Butler, J Jakeman, T Wildey. Convergence of probability densities using approximate models for forward and inverse problems in uncertainty quantification. SIAM Journal on Scientific Computing, 40 (5), A3523-A3548 (2018). <https://doi.org/10.1137/18M1181675>`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}