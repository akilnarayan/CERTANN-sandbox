{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Monte Carlo Quadrature\nThis tutorial describes how to use Monte Carlo sampling to compute the expectations of the output of a model. Specifically, given a function $f_\\alpha(\\rv):\\reals^{d}\\to\\reals$ parameterized by a set of variables $\\rv=[\\rv_1,\\ldots,\\rv_d]^T$ with joint density given by $\\rho(\\rv):\\reals^{d}\\to\\reals$, our goal is to approximate the integral\n\n\\begin{align}Q_\\alpha=\\int_\\rvdom f_\\alpha(\\rv)\\pdf(\\rv)\\dx{\\rv}\\end{align}\n\nWe can approximate the integral $Q_\\alpha$ using Monte Carlo quadrature by drawing $N$ random samples of $\\rv$ from $\\pdf$ and evaluating the function at each of these samples to obtain the data pairs $\\{(\\rv^{(n)},f^{(n)}_\\alpha)\\}_{n=1}^N$, where $f^{(n)}_\\alpha=f_\\alpha(\\rv^{(n)})$ and computing\n\n\\begin{align}Q_{\\alpha,N}=N^{-1}\\sum_{n=1}^N f^{(n)}_\\alpha\\end{align}\n\nThe mean squared error (MSE) of this estimator can be expressed as\n\n\\begin{align}\\mean{\\left(Q_{\\alpha,N}-\\mean{Q}\\right)^2}&=\\mean{\\left(Q_{\\alpha,N}-\\mean{Q_{\\alpha,N}}+\\mean{Q_{\\alpha,N}}-\\mean{Q}\\right)^2}\\\\\n   &=\\mean{\\left(Q_{\\alpha,N}-\\mean{Q_{\\alpha,N}}\\right)^2}+\\mean{\\left(\\mean{Q_{\\alpha,N}}-\\mean{Q}\\right)^2}\\\\\n   &\\qquad\\qquad+\\mean{2\\left(Q_{\\alpha,N}-\\mean{Q_{\\alpha,N}}\\right)\\left(\\mean{Q_{\\alpha,N}}-\\mean{Q}\\right)}\\\\\n   &=\\var{Q_{\\alpha,N}}+\\left(\\mean{Q_{\\alpha,N}}-\\mean{Q}\\right)^2\\end{align}\nHere we used that $Q_{\\alpha,N}$ is an unbiased estimator, i.e. $\\mean{Q_{\\alpha,N}}=\\mean{Q}$ so the third term on the second line is zero. Now using\n\n\\begin{align}\\var{Q_{\\alpha,N}}=\\var{N^{-1}\\sum_{n=1}^N f^{(n)}_\\alpha}=N^{-1}\\sum_{n=1}^N \\var{f^{(n)}_\\alpha}=N^{-1}\\var{Q_\\alpha}\\end{align}\n\nyields\n\n\\begin{align}\\mean{\\left(Q_{\\alpha}-\\mean{Q}\\right)^2}=\\underbrace{N^{-1}\\var{Q_\\alpha}}_{I}+\\underbrace{\\left(\\mean{Q_{\\alpha}}-\\mean{Q}\\right)^2}_{II}\\end{align}\n\nFrom this expression we can see that the MSE can be decomposed into two terms;\na so called stochastic error (I) and a deterministic bias (II). The first term is the variance of the Monte Carlo estimator which comes from using a finite number of samples. The second term is due to using an approximation of $f$. These two errors should be balanced, however in the vast majority of all MC analyses a single model $f_\\alpha$ is used and the choice of $\\alpha$, e.g. mesh resolution, is made a priori without much concern for the balancing bias and variance. \n\nGiven a fixed $\\alpha$ the modelers only recourse to reducing the MSE is to reduce the variance of the estimator. In the following we plot the variance of the MC estimate of a simple algebraic function $f_1$ which belongs to an ensemble of models\n\n\\begin{align}f_0(\\rv) &= A_0 \\left(\\rv_1^5\\cos\\theta_0 +\\rv_2^5\\sin\\theta_0\\right), \\\\\n   f_1(\\rv) &= A_1 \\left(\\rv_1^3\\cos\\theta_1 +\\rv_2^3\\sin\\theta_1\\right)+s_1,\\\\\n   f_2(\\rv) &= A_2 \\left(\\rv_1  \\cos\\theta_2 +\\rv_2  \\sin\\theta_2\\right)+s_2\\end{align}\n\n\nwhere $\\rv_1,\\rv_2\\sim\\mathcal{U}(-1,1)$ and all $A$ and $\\theta$ coefficients are real. We choose to set $A=\\sqrt{11}$, $A_1=\\sqrt{7}$ and $A_2=\\sqrt{3}$ to obtain unitary variance for each model. The parameters $s_1,s_2$ control the bias between the models. Here we set $s_1=1/10,s_2=1/5$. Similarly we can change the correlation between the models in a systematic way (by varying $\\theta_1$. We will levarage this later in the tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets setup the problem\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sympy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom pyapprox import variables\nfrom pyapprox.analysis import visualize\nfrom pyapprox.benchmarks import setup_benchmark\n\nnp.random.seed(1)\nshifts = [.1, .2]\nbenchmark = setup_benchmark(\n    \"tunable_model_ensemble\", theta1=np.pi/2*.95, shifts=shifts)\nprint(benchmark.variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let us compute the mean of $f_1$ using Monte Carlo\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nsamples = int(1e3)\nsamples = benchmark.variable.rvs(nsamples)\n\nmodel = benchmark.fun\nvalues = model.m1(samples)\nvariables.print_statistics(samples, values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute the exact mean using sympy and compute the MC MSE\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "z1, z2 = sp.Symbol('z1'), sp.Symbol('z2')\nranges = [-1, 1, -1, 1]\nexact_integral_f1 = benchmark.means[1]\nprint('MC difference squared =', (values.mean()-exact_integral_f1)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNow let us compute the MSE for different sample sets of the same size for $N=100,1000$ and plot the distribution of the MC estimator $Q_{\\alpha,N}$\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntrials = 1000\nmeans = np.empty((ntrials, 2))\nfor ii in range(ntrials):\n    samples = benchmark.variable.rvs(nsamples)\n    values = model.m1(samples)\n    means[ii] = values[:100].mean(), values.mean()\nfig, ax = plt.subplots()\ntextstr = '\\n'.join(\n    [r'$\\mathbb{E}[Q_{1,100}]=\\mathrm{%.2e}$' % means[:, 0].mean(),\n     r'$\\mathbb{V}[Q_{1,100}]=\\mathrm{%.2e}$' % means[:, 0].var(),\n     r'$\\mathbb{E}[Q_{1,1000}]=\\mathrm{%.2e}$' % means[:, 1].mean(),\n     r'$\\mathbb{V}[Q_{1,1000}]=\\mathrm{%.2e}$' % means[:, 1].var()])\nax.hist(means[:, 0], bins=ntrials//100, density=True)\nax.hist(means[:, 1], bins=ntrials//100, density=True, alpha=0.5)\nax.axvline(x=shifts[0], c='r', label=r'$\\mathbb{E}[Q_1]$')\nax.axvline(x=0, c='k', label=r'$\\mathbb{E}[Q_0]$')\nprops = {'boxstyle': 'round', 'facecolor': 'white', 'alpha': 1}\nax.text(0.65, 0.8, textstr, transform=ax.transAxes, bbox=props)\nax.set_xlabel(r'$\\mathbb{E}[Q_N]$')\nax.set_ylabel(r'$\\mathbb{P}(\\mathbb{E}[Q_N])$')\n_ = ax.legend(loc='upper left')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The numerical results match our theory. Specifically the estimator is unbiased( i.e. mean zero, and the variance of the estimator is $\\var{Q_{0,N}}=\\var{Q_{0}}/N=1/N$.\n\nThe variance of the estimator can be driven to zero by increasing the number of samples $N$. However when the variance becomes less than the bias, i.e. $\\left(\\mean{Q_{\\alpha}-Q}\\right)^2>\\var{Q_{\\alpha}}/N$, then the MSE will not decrease and any further samples used to reduce the variance are wasted.\n\nLet our true model be $f_0$ above. The following code compues the bias induced by using $f_\\alpha=f_1$ and also plots the contours of $f_0(\\rv)-f_1(\\rv)$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "integrand_f0 = model.A0*(sp.cos(model.theta0)*z1**5 +\n                         sp.sin(model.theta0)*z2**5)*0.25\nexact_integral_f0 = float(\n    sp.integrate(integrand_f0, (z1, ranges[0], ranges[1]), (z2, ranges[2], ranges[3])))\nbias = (exact_integral_f0-exact_integral_f1)**2\nprint('MC f1 bias =', bias)\nprint('MC f1 variance =', means.var())\nprint('MC f1 MSE =', bias+means.var())\n\nfig, ax = plt.subplots()\nX, Y, Z = visualize.get_meshgrid_function_data_from_variable(\n    lambda z: model.m0(z)-model.m1(z), benchmark.variable, 50)\ncset = ax.contourf(X, Y, Z, levels=np.linspace(Z.min(), Z.max(), 20))\n_ = plt.colorbar(cset, ax=ax)\n# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As $N\\to\\infty$ the MSE will only converge to the bias ($s_1$). Try this by increasing $\\texttt{nsamples}$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can produced unbiased estimators using the high fidelity model. However if this high-fidelity model is more expensive then this comes at the cost of the estimator having larger variance. To see this the following plots the distribution of the MC estimators using 100 samples of the $f_1$ and 10 samples of $f_0$. The cost of constructing these estimators would be equivalent if the high-fidelity model is 10 times more expensive than the low-fidelity model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntrials = 1000\nm0_means = np.empty((ntrials, 1))\nfor ii in range(ntrials):\n    samples = benchmark.variable.rvs(nsamples)\n    values = model.m0(samples)\n    m0_means[ii] = values[:10].mean()\n\nfig, ax = plt.subplots()\ntextstr = '\\n'.join(\n    [r'$\\mathbb{E}[Q_{1,100}]=\\mathrm{%.2e}$' % means[:, 0].mean(),\n     r'$\\mathbb{V}[Q_{1,100}]=\\mathrm{%.2e}$' % means[:, 0].var(),\n     r'$\\mathbb{E}[Q_{0,10}]=\\mathrm{%.2e}$' % m0_means[:, 0].mean(),\n     r'$\\mathbb{V}[Q_{0,10}]=\\mathrm{%.2e}$' % m0_means[:, 0].var()])\nax.hist(means[:, 0], bins=ntrials//100, density=True)\nax.hist(m0_means[:, 0], bins=ntrials//100, density=True, alpha=0.5)\nax.axvline(x=shifts[0], c='r', label=r'$\\mathbb{E}[Q_1]$')\nax.axvline(x=0, c='k', label=r'$\\mathbb{E}[Q_0]$')\nprops = {'boxstyle': 'round', 'facecolor': 'white', 'alpha': 1}\nax.text(0.65, 0.8, textstr, transform=ax.transAxes, bbox=props)\nax.set_xlabel(r'$\\mathbb{E}[Q_N]$')\nax.set_ylabel(r'$\\mathbb{P}(\\mathbb{E}[Q_N])$')\n_ = ax.legend(loc='upper left')\n# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a series of tutorials starting with `sphx_glr_auto_tutorials_multi_fidelity_plot_control_variate_monte_carlo.py` we show how to produce an unbiased estimator with small variance using both these models.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}