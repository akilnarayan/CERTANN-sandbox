{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-fidelity Monte Carlo\nThis tutorial builds on from `sphx_glr_auto_tutorials_multi_fidelity_plot_multi_level_monte_carlo.py` and `sphx_glr_auto_tutorials_multi_fidelity_plot_approximate_control_variate_monte_carlo.py` and introduces an approximate control variate estimator called Multi-fidelity Monte Carlo (MFMC) [PWGSIAM2016]_. Unlike MLMC this method does not assume a strict ordering of models.\n\n## Many Model MFMC\n\nTo derive the MFMC estimator first recall the two model ACV estimator\n\n\\begin{align}Q_{0,\\mathcal{Z}}^\\mathrm{MF}=Q_{0,\\mathcal{Z}_{0}} + \\eta\\left(Q_{1,\\mathcal{Z}_{0}}-\\mu_{1,\\mathcal{Z}_{1}}\\right)\\end{align}\n\nThe MFMC estimator can be derived with the following recursive argument. Partition the samples assigned to each model such that\n$\\mathcal{Z}_\\alpha=\\mathcal{Z}_{\\alpha,1}\\cup\\mathcal{Z}_{\\alpha,2}$ and $\\mathcal{Z}_{\\alpha,1}\\cap\\mathcal{Z}_{\\alpha,2}=\\emptyset$. That is the samples at the next lowest fidelity model are the samples used at all previous levels plus an additional independent set, i.e. $\\mathcal{Z}_{\\alpha,1}=\\mathcal{Z}_{\\alpha-1}$. See `mfmc-sample-allocation`. Note the differences between this scheme and the MLMC scheme.\n\n.. list-table::\n\n   * - \n\n       .. figure:: ../../figures/mfmc.png\n          :width: 100%\n          :align: center\n\n          MFMC sampling strategy\n\n     -\n\n       .. figure:: ../../figures/mlmc.png\n          :width: 100%\n          :align: center\n\n          MLMC sampling strategy\n\nStarting from two models we introduce the next low fidelity model in a way that reduces the variance of the estimate $\\mu_{\\alpha}$, i.e.\n\n\\begin{align}Q_{0,\\mathcal{Z}}^\\mathrm{MF}&=Q_{0,\\mathcal{Z}_{0}} + \\eta_1\\left(Q_{1,\\mathcal{Z}_{1}}-\\left(\\mu_{1,\\mathcal{Z}_{1}}+\\eta_2\\left(Q_{2,\\mathcal{Z}_1}-\\mu_{2,\\mathcal{Z}_2}\\right)\\right)\\right)\\\\\n   &=Q_{0,\\mathcal{Z}_{0}} + \\eta_1\\left(Q_{1,\\mathcal{Z}_{1}}-\\mu_{1,\\mathcal{Z}_{1}}\\right)+\\eta_1\\eta_2\\left(Q_{2,\\mathcal{Z}_1}-\\mu_{2,\\mathcal{Z}_2}\\right)\\end{align}\n\nWe repeat this process for all low fidelity models to obtain\n\n\\begin{align}Q_{0,\\mathcal{Z}}^\\mathrm{MF}=Q_{0,\\mathcal{Z}_{0}} + \\sum_{\\alpha=1}^M\\eta_\\alpha\\left(Q_{\\alpha,\\mathcal{Z}_{\\alpha,1}}-\\mu_{\\alpha,\\mathcal{Z}_{\\alpha,2}}\\right)\\end{align}\n\nThe optimal control variate weights for the MFMC estimator, which minimize the variance of the estimator, are $\\eta=(\\eta_1,\\ldots,\\eta_M)^T$, where for $\\alpha=1\\ldots,M$\n\n\\begin{align}\\eta_\\alpha = -\\frac{\\covar{Q_0}{Q_\\alpha}}{\\var{Q_\\alpha}}\\end{align}\n\nWith this choice of weights the variance reduction obtained is given by\n\n\\begin{align}\\gamma = 1-\\rho_{0,1}^2\\left(\\frac{r_1-1}{r_1}+\\sum_{\\alpha=2}^M \\frac{r_\\alpha-r_{\\alpha-1}}{r_\\alpha r_{\\alpha-1}}\\frac{\\rho_{0,\\alpha}^2}{\\rho_{0,1}^2}\\right)\\end{align}\n\nLet us use MFMC to estimate the mean of our high-fidelity model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import partial\nfrom pyapprox.benchmarks import setup_benchmark\nfrom pyapprox import interface\nfrom pyapprox import multifidelity\n\nbenchmark = setup_benchmark(\"short_column_ensemble\")\nshort_column_model = benchmark.fun\nmodel_costs = np.asarray([100, 50, 5, 1, 0.2])\n\ntarget_cost = int(1e4)\nidx = [0, 1, 2]\ncov = short_column_model.get_covariance_matrix()[np.ix_(idx, idx)]\nmodel_ensemble = interface.ModelEnsemble(\n    [short_column_model.models[ii] for ii in idx])\ncosts = model_costs[idx]\n\n# define the sample allocation\nest = multifidelity.get_estimator(\"mfmc\", cov, costs, benchmark.variable)\nnsample_ratios, variance, rounded_target_cost = est.allocate_samples(\n    target_cost)\nacv_samples, acv_values = est.generate_data(model_ensemble)\nmlmc_mean = est(acv_values)\nhf_mean = acv_values[0][1].mean()\n\n# get the true mean of the high-fidelity model\ntrue_mean = short_column_model.get_means()[0]\nprint('MC error', abs(hf_mean-true_mean))\nprint('MFMC error', abs(mlmc_mean-true_mean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimal Sample Allocation\nSimilarly to MLMC, the optimal number of samples that minimize the variance of the MFMC estimator can be determined analytically (see [PWGSIAM2016]_) provided the following condition is met\n\n\\begin{align}\\frac{C_{\\alpha-1}}{C_\\alpha} > \\frac{\\rho^2_{0,\\alpha-1}-\\rho^2_{0,\\alpha}}{\\rho^2_{0,\\alpha}-\\rho^2_{0,\\alpha+1}}\\end{align}\n\nWhen this condition is met the optimal number of high fidelity samples is\n\n\\begin{align}N_0 = \\frac{C_\\mathrm{tot}}{\\V{C}^T\\V{r}}\\end{align}\n\nwhere $\\V{C}=[C_0,\\cdots,C_M]^T$ are the costs of each model and $\\V{r}=[r_0,\\ldots,r_M]^T$ are the sample ratios defining the number of samples assigned to each level, i.e.\n\n\\begin{align}N_\\alpha=r_\\alpha N_0.\\end{align}\n\nRecalling that $\\rho_{j,k}$ denotes the correlation between models $j$ and $k$, the optimal sample ratios are\n\n\\begin{align}r_\\alpha=\\left(\\frac{C_0(\\rho^2_{0,\\alpha}-\\rho^2_{0,\\alpha+1})}{C_\\alpha(1-\\rho^2_{0,1})}\\right)^{\\frac{1}{2}}.\\end{align}\n\nNow lets us compare MC with MFMC using optimal sample allocations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\nbenchmark = setup_benchmark(\"polynomial_ensemble\")\npoly_model = benchmark.fun\nmodel_ensemble = interface.ModelEnsemble(poly_model.models)\ncov = poly_model.get_covariance_matrix()\ntarget_costs = np.array([1e1, 1e2, 1e3, 1e4], dtype=int)\ncosts = np.asarray([10**-ii for ii in range(cov.shape[0])])\nmodel_labels = [r'$f_0$', r'$f_1$', r'$f_2$', r'$f_3$', r'$f_4$']\nnpilot_samples = 10\ncov_mc = multifidelity.estimate_model_ensemble_covariance(\n    npilot_samples, benchmark.variable.rvs, model_ensemble,\n    model_ensemble.nmodels)[0]\n\nfrom pyapprox.util.configure_plots import mathrm_labels, mathrm_label\nestimators = [\n    multifidelity.get_estimator(\"mc\", cov, costs, poly_model.variable),\n    multifidelity.get_estimator(\"mfmc\", cov, costs, poly_model.variable)]\nest_labels = mathrm_labels([\"MC\", \"MFMC\"])\noptimized_estimators = multifidelity.compare_estimator_variances(\n    target_costs, estimators)\n\nfig, axs = plt.subplots(1, 2, figsize=(2*8, 6))\nmultifidelity.plot_estimator_variances(\n    optimized_estimators, est_labels, axs[0],\n    ylabel=mathrm_label(\"Relative Estimator Variance\"))\naxs[0].set_xlim(target_costs.min(), target_costs.max())\nmultifidelity.plot_acv_sample_allocation_comparison(\n    optimized_estimators[1], model_labels, axs[1])\nplt.show()\n#fig # necessary for jupyter notebook to reshow plot in new cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n.. [PWGSIAM2016] [Peherstorfer, B., Willcox, K.,  Gunzburger, M., Optimal Model Management for Multifidelity Monte Carlo Estimation, 2016.](https://doi.org/10.1137/15M1046472)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}