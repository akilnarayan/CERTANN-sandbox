{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Add latex macros$$\\newcommand{\\V}[1]{{\\boldsymbol{#1}}}\\newcommand{mean}[1]{{\\mathbb{E}\\left[#1\\right]}}\\newcommand{var}[1]{{\\mathbb{V}\\left[#1\\right]}}\\newcommand{covar}[2]{\\mathbb{C}\\text{ov}\\left[#1,#2\\right]}\\newcommand{corr}[2]{\\mathbb{C}\\text{or}\\left[#1,#2\\right]}\\newcommand{argmin}{\\mathrm{argmin}}\\def\\rv{z}\\def\\reals{\\mathbb{R}}\\def\\pdf{\\rho}\\def\\rvdom{\\Gamma}\\def\\coloneqq{\\colon=}\\newcommand{norm}{\\lVert #1 \\rVert}\\def\\argmax{\\operatorname{argmax}}\\def\\ai{\\alpha}\\def\\bi{\\beta}\\newcommand{\\dx}[1]{\\;\\mathrm{d}#1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# End-to-End Model Analysis\nThis tutorial describes how to use each of the major model analyses in Pyapprox\nfollowing the exposition in [PYAPPROX2022]_.\n\nFirst lets load all the necessary modules and set the random seeds for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport torch\nimport time\nimport warnings\nfrom pyapprox.util.configure_plots import mathrm_label, mathrm_labels\nfrom pyapprox.variables import (\n    IndependentMarginalsVariable, print_statistics, AffineTransform)\nfrom pyapprox.benchmarks import setup_benchmark, list_benchmarks\nfrom __util import pyapprox_fun_1, pyapprox_fun_2\nfrom pyapprox.interface.wrappers import (\n    ModelEnsemble, TimerModel, WorkTrackingModel,\n    evaluate_1darray_function_on_2d_array)\nfrom pyapprox.surrogates import adaptive_approximate\nfrom pyapprox.analysis.sensitivity_analysis import (\n    run_sensitivity_analysis, plot_sensitivity_indices)\nfrom pyapprox.bayes.markov_chain_monte_carlo import (\n    loglike_from_negloglike, MCMCVariable)\nfrom pyapprox.expdesign.bayesian_oed import get_bayesian_oed_optimizer\nfrom pyapprox import multifidelity\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nnp.random.seed(2)\ntorch.manual_seed(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tutorial can save the figures to file if desired. If you do want the plots\nset savefig=True\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "savefig = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code shows how to create and sample from two independent uniform random variables defined on [\u22122, 2]. We use uniform variables here, but any marginal from the scipy.stats module can be used.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nsamples = 30\nunivariate_variables = [stats.uniform(-2, 4), stats.uniform(-2, 4)]\nvariable = IndependentMarginalsVariable(univariate_variables)\nsamples = variable.rvs(nsamples)\nprint_statistics(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyApprox supports various types of variable transformations. The following code\nshows how to use an affinte transformation to map samples from variables\nto samples from the variable's canonical form.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "var_trans = AffineTransform(variable)\ncanonical_samples = var_trans.map_to_canonical(samples)\nprint_statistics(canonical_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pyapprox provides many utilities for interfacing with complex numerical codes\nBelow we take two functions, see `sphx_glr_auto_examples_plot_interface.py`\nfor thier definitions and show how to use the ModelEnsemble and\nWorkTrackingModel to evaluate two models at once and time the wall time\nof each evaluation of each model. The last print statement\nprints the median execution time of each model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_ensemble = ModelEnsemble([pyapprox_fun_1, pyapprox_fun_2])\ntimer_fun_ensemble = TimerModel(model_ensemble)\nworktracking_fun_ensemble = WorkTrackingModel(\n    timer_fun_ensemble, num_config_vars=1)\nfun_ids = np.ones(nsamples)\nfun_ids[:nsamples//2] = 0\nensemble_samples = np.vstack([samples, fun_ids])\nvalues = worktracking_fun_ensemble(ensemble_samples)\nquery_fun_ids = np.atleast_2d([0, 1])\nprint(worktracking_fun_ensemble.work_tracker(query_fun_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pyapprox provide numerous benchmarks for verifying, validating and comparing\nmodel analysis algorithms. The following list the names of all benchmarks and\nthen creates a benchmark that can be used to test the creation of surrogates,\nBayesian inference, and optimal experimental design. This benchmark requires\ndetermining the true coefficients of the Karhunene Loeve expansion (KLE)\nused to characterize the uncertain diffusivity field of an advection\ndiffusion equation. See documentation of the benchmark for more details).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(list_benchmarks())\nnoise_stdev = 1e-1\ninv_benchmark = setup_benchmark(\n    \"advection_diffusion_kle_inversion\", kle_nvars=3,\n    noise_stdev=noise_stdev, nobs=10, kle_length_scale=0.5)\nprint(inv_benchmark.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following plots the modes of the KLE\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(\n    1, inv_benchmark.KLE.nterms, figsize=(8*inv_benchmark.KLE.nterms, 6))\nfor ii in range(inv_benchmark.KLE.nterms):\n    inv_benchmark.mesh.plot(inv_benchmark.KLE.eig_vecs[:, ii:ii+1], 50,\n                            ax=axs[ii])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyApprox provides many popular methods for constructing surrogates\nthat once constructed can be evaluated in place of a computaionally\nexpensive simulation model in model analyses. The following code creates\na Gaussian process (GP) surrogate. The function used to construct the surrogate\ntakes a callback which is evaluated each time the adaptive surrogate is refined.\nHere we use to compute the error of the surrogate as it is constructed using\nvalidation data. Uncomment the code to use a polynomial based surrogate instead\nof a GP. The user does not have to change any subsequent code\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation_samples = inv_benchmark.variable.rvs(100)\nvalidation_values = inv_benchmark.negloglike(validation_samples)\nnsamples, errors = [], []\ndef callback(approx):\n    nsamples.append(approx.num_training_samples())\n    error = np.linalg.norm(\n        approx(validation_samples)-validation_values, axis=0)\n    error /= np.linalg.norm(validation_values, axis=0)\n    errors.append(error)\n\napprox_result = adaptive_approximate(\n    inv_benchmark.negloglike, inv_benchmark.variable, \"gaussian_process\",\n    {\"max_nsamples\": 30, \"ncandidate_samples\": 2e3, \"verbose\": 0,\n     \"callback\": callback, \"kernel_variance\": 400})\n\n# approx_result = adaptive_approximate(\n#     inv_benchmark.negloglike, inv_benchmark.variable, \"polynomial_chaos\",\n#     {\"method\": \"leja\", \"options\": {\n#         \"max_nsamples\": 100, \"ncandidate_samples\": 3e3, \"verbose\": 0,\n#         \"callback\": callback}})\napprox = approx_result.approx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the errors obtained from the callback with\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = plt.subplots(figsize=(8, 6))[1]\nax.loglog(nsamples, errors, \"o-\")\nax.set_xlabel(mathrm_label(\"No. Samples\"))\nax.set_ylabel(mathrm_label(\"Error\"))\nif savefig:\n    plt.savefig(\"gp-error-plot.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will perform a sensitivity analysis. Specifically we compute\nvariance based sensitivity indices that measure the impact of each KLE mode\non the mismatch between the observed data and the model predictions.\nWe use the negative log likelihood to characterize this mismatch.\nHere we have used the surrogate to speed up the computation of the sensitivity\nindices. Uncomment the commented code to use the numerical model. Note\nthe drastic increase in computational cost. Warning: using the numerical model\nwill take many minutes. The plots in the figure, generated from\nleft to right are: main effect, largest Sobol indices and total effect indices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sa_result = run_sensitivity_analysis(\n    \"surrogate_sobol\", approx, inv_benchmark.variable)\n# sa_result = run_sensitivity_analysis(\n#     \"sobol\", benchmark.negloglike, inv_benchmark.variable)\naxs = plot_sensitivity_indices(\n    sa_result)[1]\nif savefig:\n    plt.savefig(\"gp-sa-indices.pdf\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will use the surrogate with Bayesian inference to learn the\ncoefficients of the KL. Specifically we will draw a set of samples from\nthe posterior distribution of the KLE given the observed data provided\nin the benchmark.\n\nBut First we will improve the accuracy of the surrogate\nand print out the error which can be compared to the errors previously plotted.\nThe error of the original surrogate was kept low to demonstrate the ability\nto quantify error in the sensitivity indices from using a surrogate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "approx.refine(100)\nerror = np.linalg.norm(\n    approx(validation_samples)-validation_values, axis=0)\nerror /= np.linalg.norm(validation_values, axis=0)\nprint(\"Surrogate\", error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create a MCMCVariable to sample from the posterior. The benchmark\nhas already formulated the negative log likelihood that is needed. Here\nwe will use the NUTS sampler from PyMC3. This can run many MCMC chains at once\nby setting njobs > 1. However using njobs>1 cannot be used unless the srcipt\nis invoked inside if __name__ == __main__: and so njobs>1\nis not used for this tutorial.\nUncomment the commented code to use the numerical model instead of the surrogate\nwith the MCMC algorithm. Again note the significant increase in computational\ntime\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algorithm, npost_samples, njobs = \"nuts\", 100, 1\n# loglike = partial(loglike_from_negloglike, inv_benchmark.negloglike)\nloglike = partial(loglike_from_negloglike, approx)\nmcmc_variable = MCMCVariable(\n    inv_benchmark.variable, loglike, algorithm, njobs=njobs, loglike_grad=True)\nprint(mcmc_variable)\npost_samples = mcmc_variable.rvs(npost_samples)\nmap_sample = mcmc_variable.maximum_aposteriori_point()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now plot the posterior samples with the 2D Marginals of the posterior. Note\ndo not do this with the numerical model as this would take an eternity due\nto the cost of evaluating the numerical model, which is much higher relative\nto the cost of running the surrogate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mcmc_variable.plot_2d_marginals(\n    nsamples_1d=30,\n    plot_samples=[\n        [post_samples, {}], [map_sample, {\"c\": \"k\", \"marker\": \"X\", \"s\": 100}]])\nif savefig:\n    plt.savefig(\"posterior-samples.pdf\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the Bayesian inference above we used a fixed number of observations\nat randomly chosen spatial locations. However choosing observation locations\nis usually a poor idea. Not all observations can reduce the uncertainty\nin the parameters equally. Here we use Bayesian optimal experimental design\nto choose the 3 best design locations from the previously observed 10 pretending\nthat we do not know the value of the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n#plot a single solution to the PDE before overlaying the designs\ninv_benchmark.mesh.plot(\n    inv_benchmark.obs_fun._fwd_solver.solve()[:, None], 50, ax=ax)\n\ndesign_candidates = inv_benchmark.mesh.mesh_pts[:, inv_benchmark.obs_indices]\noed = get_bayesian_oed_optimizer(\n    \"kl_params\", design_candidates, inv_benchmark.obs_fun, noise_stdev,\n    inv_benchmark.variable)\noed_results = []\nndesign = 3\nfor step in range(ndesign):\n    results_step = oed.update_design()[1]\n    oed_results.append(results_step)\nselected_candidates = design_candidates[:, np.hstack(oed_results)]\nax.plot(design_candidates[0, :], design_candidates[1, :], \"rs\")\nax.plot(selected_candidates[0, :], selected_candidates[1, :], \"ko\")\nif savefig:\n    plt.savefig(\"oed-selected-design.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that typically optimal\nexperimental design (OED) would be used before conducting Bayesian inference.\nHowever, because understanding of Bayesian inference is needed to understand\nBayesian OED we reversed the order. OED is much more expensive than a single\nBayesian calibration because it requires solving many calibration problems.\nSo typically we do not solve the calibration problems in the OED procedure\nto the same degree of accuracy as a final calibration. The accuracy of the\ncalibrations used by OED must only be sufficient to distinguish between designs.\nThis accuracy is typically much lower than the accuracy required in\nestimates of uncertainty in the parameters or predictions needed for decision making tasks such as risk assessment.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will set up a related benchmark to the one we have been using,\nwhich can be used to demonstrate the forward propagation of uncertainty.\nThis benchmark uses the steady state solution of the advection diffusion,\nobtained with a constant addition of a tracer into the domain at a single\nsource model as initial condition. A pump at another locations is then activated\nto extract the tracer from the domain. The benchmark quantity of interest\nmeasures the change of the tracer concentration in a subomain.\nThe benchmark provides models of varying cost\nand accuracy that use different discretizations of the spatial PDE mesh\nand number of time steps which can be used with multi-fideilty methods.\nTo setup the benchmark use the following\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fwd_benchmark = setup_benchmark(\n    \"multi_index_advection_diffusion\",\n    kle_nvars=inv_benchmark.variable.num_vars(), kle_length_scale=0.5,\n    time_scenario=True)\nmodel = WorkTrackingModel(\n    TimerModel(fwd_benchmark.model_ensemble), num_config_vars=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will use Multi-fidelity statistical estimation to compute the\nmean value of the QoI to account for the uncertainty in the KLE cofficients.\nSo first we must compute the covariance between the QoI returned by\neach of our models. We use samples from the posterior. But uncommenting\nthe code below will use samples from the prior.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "npilot_samples = 10\n# generate_samples = inv_benchmark.variable.rvs # for sampling from prior\ngenerate_samples = post_samples \ncov = multifidelity.estimate_model_ensemble_covariance(\n    npilot_samples, generate_samples, model,\n    fwd_benchmark.model_ensemble.nmodels)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By using a WorkTrackingModel we can extract the median costs\nof evaluatin each model which is needed to predict the\nerror of the multi-fidelity estimate of the mean which we can\ncompare to a prediction of the single fidelity estimate that only uses\nthe highest fidelity model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_costs = model.work_tracker(\n    np.asarray([np.arange(fwd_benchmark.model_ensemble.nmodels)]))\n# make costs in terms of fraction of cost of high-fidelity evaluation\nmodel_costs /= model_costs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now visualize the correlation between the models and their computational\ncost relative to the highest-fidelity model cost\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(2*8, 6))\nmultifidelity.plot_correlation_matrix(\n    multifidelity.get_correlation_from_covariance(cov), ax=axs[0])\nmultifidelity.plot_model_costs(model_costs, ax=axs[1])\naxs[0].set_title(mathrm_label(\"Model covariances\"))\naxs[1].set_title(mathrm_label(\"Relative model costs\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now find the best multi-fidelity estimator among all avialable option\nNote, he exact predicted variance will change from run to run even with the\nsame seed because the computational time measured will change slightly\nfor each run\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_est, best_model_indices = (\n    multifidelity.get_best_models_for_acv_estimator(\n        \"acvgmfb\", cov, model_costs, inv_benchmark.variable, 1e2, max_nmodels=3,\n        tree_depth=4))\ntarget_cost = 1000\nbest_est.allocate_samples(target_cost)\nprint(\"Predicted variance\", best_est.optimized_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can plot the relative performance of the single and multi-fidelity\nestimates of the mean before requiring any additional model evaluations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hf_cov, hf_cost = cov[:1, :1], model_costs[:1]\nestimators = [\n    multifidelity.get_estimator(\n        \"mc\", hf_cov, hf_cost, inv_benchmark.variable),\n    best_est]\ntarget_costs = np.array([1e1, 1e2, 1e3, 1e4], dtype=int)\noptimized_estimators = multifidelity.compare_estimator_variances(\n    target_costs, estimators)\nest_labels = mathrm_labels([\"MC\", \"ACV\"])\nfig, axs = plt.subplots(1, 2, figsize=(2*8, 6))\nmultifidelity.plot_estimator_variances(\n    optimized_estimators, est_labels, axs[0],\n    ylabel=mathrm_label(\"Relative Estimator Variance\"))\naxs[0].set_xlim(target_costs.min(), target_costs.max())\nnmodels = cov.shape[0]\nmodel_labels = [\n    r\"$f_{%d}$\" % ii for ii in np.arange(nmodels)[best_model_indices]]\nmultifidelity.plot_acv_sample_allocation_comparison(\n    optimized_estimators[1], model_labels, axs[1])\nif savefig:\n    plt.savefig(\"acv-variance-reduction.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is clear that the multi-fidelity estimator will be more computationally\nefficient for multiple computational budgets (target costs). Once the user\nis ready to actually estimate the mean QoI they can use\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_cost = 10\nbest_est.allocate_samples(target_cost)\nbest_model_ensemble = ModelEnsemble(\n    [fwd_benchmark.model_ensemble.functions[ii] for ii in best_model_indices])\nsamples, values = best_est.generate_data(best_model_ensemble)\nmean = best_est(values)\nprint(\"Mean QoI\", mean)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [PYAPPROX2022] `Jakeman J.D., PyApprox: Enabling efficient model analysis. (2022)`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}